{
  "hash": "b3efe110c89a5026684b60e377a535ce",
  "result": {
    "markdown": "---\ntitle : \"4-german-creditcard-logistics-reg\"\ncode-fold: true\n---\n\n:::{.callout-note,title=\"简介\"}\n - ref :[german-creditcard](https://online.stat.psu.edu/stat857/node/215/)\n - scitype 转换 参考:[autotype(d, :few_to_finite)方法](https://juliaai.github.io/DataScienceTutorials.jl/data/processing/)\n  \n:::\n\n## 1. load package\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\ninclude(\"../utils.jl\")\nimport MLJ:predict,fit!,predict_mode,range\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie\n```\n:::\n\n\n## 2. data procsssing\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nXtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();\n```\n:::\n\n\n## 3. MLJ workflow\n### 3.1 define model\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\nmodel=LogisticClassifier()\nNuSVC = @load NuSVC pkg=LIBSVM\nmodel2 = NuSVC()\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel3 = KNNClassifier(weights = NearestNeighborModels.Inverse())\n\n\"定义 几个 tune 参数的区间 \"\nk1 =range(model, :gamma, lower=0.1, upper=1.2);\nk2 =range(model, :lambda, lower=0.1, upper=1.2);\nk3 =range(model, :penalty, values=([:l2, :l1,:en,:none]));\nk4 =range(model, :fit_intercept, values=([true, false]));\n\ntuning_logistic = TunedModel(model=model,\n\t\t\t\t\t\t\t resampling = CV(nfolds=4, rng=1234),\n\t\t\t\t\t\t\t tuning = Grid(resolution=8),\n\t\t\t\t\t\t\t range = [k1,k2],\n\t\t\t\t\t\t\t measure=accuracy)\nmach = machine(tuning_logistic, Xtrain, ytrain;scitype_check_level=0)|>fit!\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, …), …), …).\n[ Info: Attempting to evaluate 64 models.\n\rEvaluating over 64 metamodels:   0%[>                        ]  ETA: N/A┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) <: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n\rEvaluating over 64 metamodels:   2%[>                        ]  ETA: 0:13:47\rEvaluating over 64 metamodels:   3%[>                        ]  ETA: 0:07:03\rEvaluating over 64 metamodels:   5%[=>                       ]  ETA: 0:04:37\rEvaluating over 64 metamodels:   6%[=>                       ]  ETA: 0:03:25\rEvaluating over 64 metamodels:   8%[=>                       ]  ETA: 0:02:41\rEvaluating over 64 metamodels:   9%[==>                      ]  ETA: 0:02:12\rEvaluating over 64 metamodels:  11%[==>                      ]  ETA: 0:01:51\rEvaluating over 64 metamodels:  12%[===>                     ]  ETA: 0:01:36\rEvaluating over 64 metamodels:  14%[===>                     ]  ETA: 0:01:24\rEvaluating over 64 metamodels:  16%[===>                     ]  ETA: 0:01:14\rEvaluating over 64 metamodels:  17%[====>                    ]  ETA: 0:01:06\rEvaluating over 64 metamodels:  19%[====>                    ]  ETA: 0:00:59\rEvaluating over 64 metamodels:  20%[=====>                   ]  ETA: 0:00:54\rEvaluating over 64 metamodels:  22%[=====>                   ]  ETA: 0:00:49\rEvaluating over 64 metamodels:  23%[=====>                   ]  ETA: 0:00:45\rEvaluating over 64 metamodels:  25%[======>                  ]  ETA: 0:00:41\rEvaluating over 64 metamodels:  27%[======>                  ]  ETA: 0:00:38\rEvaluating over 64 metamodels:  28%[=======>                 ]  ETA: 0:00:35\rEvaluating over 64 metamodels:  30%[=======>                 ]  ETA: 0:00:33\rEvaluating over 64 metamodels:  31%[=======>                 ]  ETA: 0:00:30\rEvaluating over 64 metamodels:  33%[========>                ]  ETA: 0:00:28\rEvaluating over 64 metamodels:  34%[========>                ]  ETA: 0:00:26\rEvaluating over 64 metamodels:  36%[========>                ]  ETA: 0:00:25\rEvaluating over 64 metamodels:  38%[=========>               ]  ETA: 0:00:23\rEvaluating over 64 metamodels:  39%[=========>               ]  ETA: 0:00:22\rEvaluating over 64 metamodels:  41%[==========>              ]  ETA: 0:00:20\rEvaluating over 64 metamodels:  42%[==========>              ]  ETA: 0:00:19\rEvaluating over 64 metamodels:  44%[==========>              ]  ETA: 0:00:18\rEvaluating over 64 metamodels:  45%[===========>             ]  ETA: 0:00:17\rEvaluating over 64 metamodels:  47%[===========>             ]  ETA: 0:00:16\rEvaluating over 64 metamodels:  48%[============>            ]  ETA: 0:00:15\rEvaluating over 64 metamodels:  50%[============>            ]  ETA: 0:00:14\rEvaluating over 64 metamodels:  52%[============>            ]  ETA: 0:00:13\rEvaluating over 64 metamodels:  53%[=============>           ]  ETA: 0:00:12\rEvaluating over 64 metamodels:  55%[=============>           ]  ETA: 0:00:12\rEvaluating over 64 metamodels:  56%[==============>          ]  ETA: 0:00:11\rEvaluating over 64 metamodels:  58%[==============>          ]  ETA: 0:00:10\rEvaluating over 64 metamodels:  59%[==============>          ]  ETA: 0:00:10\rEvaluating over 64 metamodels:  61%[===============>         ]  ETA: 0:00:09\rEvaluating over 64 metamodels:  62%[===============>         ]  ETA: 0:00:08\rEvaluating over 64 metamodels:  64%[================>        ]  ETA: 0:00:08\rEvaluating over 64 metamodels:  66%[================>        ]  ETA: 0:00:07\rEvaluating over 64 metamodels:  67%[================>        ]  ETA: 0:00:07\rEvaluating over 64 metamodels:  69%[=================>       ]  ETA: 0:00:06\rEvaluating over 64 metamodels:  70%[=================>       ]  ETA: 0:00:06\rEvaluating over 64 metamodels:  72%[=================>       ]  ETA: 0:00:05\rEvaluating over 64 metamodels:  73%[==================>      ]  ETA: 0:00:05\rEvaluating over 64 metamodels:  75%[==================>      ]  ETA: 0:00:05\rEvaluating over 64 metamodels:  77%[===================>     ]  ETA: 0:00:04\rEvaluating over 64 metamodels:  78%[===================>     ]  ETA: 0:00:04\rEvaluating over 64 metamodels:  80%[===================>     ]  ETA: 0:00:04\rEvaluating over 64 metamodels:  81%[====================>    ]  ETA: 0:00:03\rEvaluating over 64 metamodels:  83%[====================>    ]  ETA: 0:00:03\rEvaluating over 64 metamodels:  84%[=====================>   ]  ETA: 0:00:03\rEvaluating over 64 metamodels:  86%[=====================>   ]  ETA: 0:00:02\rEvaluating over 64 metamodels:  88%[=====================>   ]  ETA: 0:00:02\rEvaluating over 64 metamodels:  89%[======================>  ]  ETA: 0:00:02\rEvaluating over 64 metamodels:  91%[======================>  ]  ETA: 0:00:01\rEvaluating over 64 metamodels:  92%[=======================> ]  ETA: 0:00:01\rEvaluating over 64 metamodels:  94%[=======================> ]  ETA: 0:00:01\rEvaluating over 64 metamodels:  95%[=======================> ]  ETA: 0:00:01\rEvaluating over 64 metamodels:  97%[========================>]  ETA: 0:00:00\rEvaluating over 64 metamodels:  98%[========================>]  ETA: 0:00:00\rEvaluating over 64 metamodels: 100%[=========================] Time: 0:00:14\n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) <: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nimport MLJLinearModels ✔\nimport MLJLIBSVMInterface ✔\nimport NearestNeighborModels ✔\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\ntrained Machine; does not cache data\n  model: ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, …), …)\n  args: \n    1:\tSource @693 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}\n    2:\tSource @676 ⏎ AbstractVector{OrderedFactor{2}}\n```\n:::\n:::\n\n\n### 3.2 predict test results\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nyhat=predict_mode(mach, Xtest)|>Array\n@info \"german-creditcard 违约预测准确率\"=>accuracy(ytest,yhat)|>d->round(d,digits=3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[ Info: \"german-creditcard 违约预测准确率\" => 0.74\n```\n:::\n:::\n\n\n",
    "supporting": [
      "4-german-creditcard-logistics-reg_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}