[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia MLJ",
    "section": "",
    "text": "Course info & details here"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html",
    "href": "category/classification/2-diabetes-svm-classficiation.html",
    "title": "2-svm-diabetes-classfication",
    "section": "",
    "text": "ä»‹ç»\n\n\n\n\nå‚è€ƒåšå®¢æ–‡ç« :diagnose-diabetes-with-svm\nSVM(æ”¯æŒå‘é‡æœº)é€šè¿‡å¼•å…¥ kernelfunction,ä½¿å¾—æ¨¡å‹çš„åˆ†ç±»çµæ´»æ€§å¤§å¤§å¢å¼º,å¯ä»¥è§£å†³æ›´å¤šé—®é¢˜.åœ¨juliaä¸­å¯ä»¥é€šè¿‡åœ¨LIBSVM.jl å¼•å…¥ kernel function å®ç°, å‚è§ æ–‡æ¡£: Support Vector Machine\nMLJ.jl é€šè¿‡åŒ…è£…æ¥å£ä¹Ÿæä¾›ç›¸ä¼¼åŠŸèƒ½\nå“åº”å˜é‡éœ€è¦è½¬æ¢ç±»å‹ to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "href": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "title": "2-svm-diabetes-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ: fit!, predict\nusing CSV,DataFrames,Random\nusing MLJ\nusing Plots\nusing KernelFunctions"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "href": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "title": "2-svm-diabetes-classfication",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\n df=load_csv(\"diabetes\")\n to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)\n df=to_ScienceType(df)\n first(df,5)|&gt;display\n y, X =  unpack(df, ==(:Outcome), rng=123);\n (Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.7, multi=true,  rng=123)\ndisplay(schema(X))\n\n\n5Ã—9 DataFrame\n\n\n\nRow\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nCatâ€¦\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names                    â”‚ scitypes   â”‚ types   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Pregnancies              â”‚ Count      â”‚ Int64   â”‚\nâ”‚ Glucose                  â”‚ Count      â”‚ Int64   â”‚\nâ”‚ BloodPressure            â”‚ Count      â”‚ Int64   â”‚\nâ”‚ SkinThickness            â”‚ Count      â”‚ Int64   â”‚\nâ”‚ Insulin                  â”‚ Count      â”‚ Int64   â”‚\nâ”‚ BMI                      â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ DiabetesPedigreeFunction â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ Age                      â”‚ Count      â”‚ Int64   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "href": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "title": "2-svm-diabetes-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 defin model\n\n\nCode\nSVC = @load SVC pkg=LIBSVM\n#define kernel function,evaulate  kernelfunctions methods\nkernels=[PolynomialKernel(; degree=2, c=1), \n         SqExponentialKernel(),\n         NeuralNetworkKernel(),\n         LinearKernel(;c=1.0)\n]\n\nsvc_mdls = [SVC(;kernel=k) for  k in kernels]\nsvcs = [machine(model, Xtrain, ytrain;scitype_check_level=0) for model in svc_mdls]\n[fit!(svc) for svc in svcs]\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), â€¦), â€¦).\n\nWARNING: reaching max number of iterations\n[ Info: Training machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = Neural Network Kernel, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = Linear Kernel (c = 1.0), â€¦), â€¦).\n\nWARNING: reaching max number of iterations\n\n\nimport MLJLIBSVMInterface âœ”\n\n\n4-element Vector{Machine{MLJLIBSVMInterface.SVC, true}}:\n machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), â€¦), â€¦)\n machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), â€¦), â€¦)\n machine(SVC(kernel = Neural Network Kernel, â€¦), â€¦)\n machine(SVC(kernel = Linear Kernel (c = 1.0), â€¦), â€¦)\n\n\n\n\n3.2 predict test\n\n\nCode\nfor (idx, str) in enumerate([\"Polynomial \",\"Gaussian\",\"NeuralNetwork\",\"Linear\"])\n    local yhat=predict(svcs[idx],Xtest)\n    local acc=accuracy(yhat,ytest) \n    @info \"$(str) kernel predict accuracy\"=&gt;acc   \nend\n\n\n[ Info: \"Polynomial  kernel predict accuracy\" =&gt; 0.47391304347826085\n[ Info: \"Gaussian kernel predict accuracy\" =&gt; 0.6434782608695652\n[ Info: \"NeuralNetwork kernel predict accuracy\" =&gt; 0.6478260869565218\n[ Info: \"Linear kernel predict accuracy\" =&gt; 0.7782608695652173"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html",
    "href": "category/classification/1-catboost-claffification.html",
    "title": "1-catboost-classfication",
    "section": "",
    "text": "dataset\n\n\n\ndataset å‚è§ clustering-exercises dataset"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-package",
    "href": "category/classification/1-catboost-claffification.html#load-package",
    "title": "1-catboost-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport Plots:scatter!,contourf\nimport MLJ:predict,predict_mode,measures\nusing Plots, MLJ, CSV, DataFrames\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-data",
    "href": "category/classification/1-catboost-claffification.html#load-data",
    "title": "1-catboost-classfication",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n  df=load_csv(\"basic1\")\n  cat=df[:,:color]|&gt;levels|&gt;length # ç±»åˆ«\n  ytrain, Xtrain =  unpack(df, ==(:color), rng=123);\n  first(df,10)\n\n\n10Ã—3 DataFrame\n\n\n\nRow\nx\ny\ncolor\n\n\n\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n79.4083\n152.834\n0\n\n\n2\n98.0463\n186.911\n0\n\n\n3\n240.579\n48.4737\n1\n\n\n4\n109.687\n277.946\n0\n\n\n5\n249.626\n229.753\n1\n\n\n6\n100.785\n281.983\n0\n\n\n7\n235.33\n109.54\n1\n\n\n8\n262.352\n64.5746\n1\n\n\n9\n76.5589\n204.296\n0\n\n\n10\n245.558\n134.502\n1"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "href": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "title": "1-catboost-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    catboost = CatBoostClassifier(iterations=2,learning_rate=0.20)\n    mach = machine(catboost, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n    tx,ty,xtest=boundary_data(df)  # boudary data and xtest \n    ytest = predict_mode(mach, xtest)[:,1]|&gt;Array\n\n\n[ Info: Training machine(CatBoostClassifier(iterations = 2, â€¦), â€¦).\n\n\n40000-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n â‹®\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n3.2 plot results\n\n\nCode\ncontourf(tx,ty,ytest,levels=cat,color=cgrad(:redsblues),alpha=0.7)\np1=scatter!(df[:,:x],df[:,:y],group=df[:,:color],label=false,ms=3,alpha=0.3)"
  },
  {
    "objectID": "category/getting-started.html",
    "href": "category/getting-started.html",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#loading-package-and-data",
    "href": "category/getting-started.html#loading-package-and-data",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#build-decisiontree-model",
    "href": "category/getting-started.html#build-decisiontree-model",
    "title": "getting started with MLJ",
    "section": "2. build DecisionTree model",
    "text": "2. build DecisionTree model\n\n    y, X = unpack(iris, ==(:target); rng=123);\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n                 measures=[log_loss, accuracy],\n                 verbosity=0)\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJDecisionTreeInterface âœ”\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 3.12        â”‚ 1.48    â”‚ [2.88, 1.44, 5 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.913       â”‚ 0.041   â”‚ [0.92, 0.96, 0 â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html",
    "href": "category/regression/4-german-creditcard-logistics-reg.html",
    "title": "4-german-creditcard-logistics-reg",
    "section": "",
    "text": "ref :german-creditcard\nscitype è½¬æ¢ å‚è€ƒ:autotype(d, :few_to_finite)æ–¹æ³•"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "title": "4-german-creditcard-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,fit!,predict_mode,range\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "title": "4-german-creditcard-logistics-reg",
    "section": "2. data procsssing",
    "text": "2. data procsssing\n\n\nCode\nto_ScienceType(d)=coerce(d,autotype(d, :few_to_finite))\ndf=load_csv(\"german_creditcard\")|&gt;to_ScienceType\nfirst(df,5)|&gt;display\ny, X=  unpack(df, ==(:Creditability));\ncat=levels(y)\nX=X[:,1:end-1]  # å»é™¤æœ€åä¸€åˆ—å˜é‡\n(Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true);\n\n\n5Ã—21 DataFrame\n\n\n\nRow\nCreditability\nAccount Balance\nDuration of Credit (month)\nPayment Status of Previous Credit\nPurpose\nCredit Amount\nValue Savings/Stocks\nLength of current employment\nInstalment per cent\nSex & Marital Status\nGuarantors\nDuration in Current address\nMost valuable available asset\nAge (years)\nConcurrent Credits\nType of apartment\nNo of Credits at this Bank\nOccupation\nNo of dependents\nTelephone\nForeign Worker\n\n\n\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nInt64\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\nCatâ€¦\n\n\n\n\n1\n1\n1\n18\n4\n2\n1049\n1\n2\n4\n2\n1\n4\n2\n21\n3\n1\n1\n3\n1\n1\n1\n\n\n2\n1\n1\n9\n4\n0\n2799\n1\n3\n2\n3\n1\n2\n1\n36\n3\n1\n2\n3\n2\n1\n1\n\n\n3\n1\n2\n12\n2\n9\n841\n2\n4\n2\n2\n1\n4\n1\n23\n3\n1\n1\n2\n1\n1\n1\n\n\n4\n1\n1\n12\n4\n0\n2122\n1\n3\n3\n3\n1\n2\n1\n39\n3\n1\n2\n2\n2\n1\n2\n\n\n5\n1\n1\n12\n4\n0\n2171\n1\n3\n4\n3\n1\n4\n2\n38\n1\n2\n2\n2\n1\n1\n2"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "title": "4-german-creditcard-logistics-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\nmodel=LogisticClassifier()\nNuSVC = @load NuSVC pkg=LIBSVM\nmodel2 = NuSVC()\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel3 = KNNClassifier(weights = NearestNeighborModels.Inverse())\n\n\"å®šä¹‰ å‡ ä¸ª tune å‚æ•°çš„åŒºé—´ \"\nk1 =range(model, :gamma, lower=0.1, upper=1.2);\nk2 =range(model, :lambda, lower=0.1, upper=1.2);\nk3 =range(model, :penalty, values=([:l2, :l1,:en,:none]));\nk4 =range(model, :fit_intercept, values=([true, false]));\n\ntuning_logistic = TunedModel(model=model,\n                             resampling = CV(nfolds=4, rng=1234),\n                             tuning = Grid(resolution=8),\n                             range = [k1,k2],\n                             measure=accuracy)\nmach = machine(tuning_logistic, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦), â€¦).\n[ Info: Attempting to evaluate 64 models.\nEvaluating over 64 metamodels:   0%[&gt;                        ]  ETA: N/Aâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{Union{AbstractVector{Count}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\nEvaluating over 64 metamodels:   2%[&gt;                        ]  ETA: 0:00:01Evaluating over 64 metamodels:   3%[&gt;                        ]  ETA: 0:00:01Evaluating over 64 metamodels:   5%[=&gt;                       ]  ETA: 0:00:01Evaluating over 64 metamodels:   6%[=&gt;                       ]  ETA: 0:00:01Evaluating over 64 metamodels:   8%[=&gt;                       ]  ETA: 0:00:01Evaluating over 64 metamodels:   9%[==&gt;                      ]  ETA: 0:00:01Evaluating over 64 metamodels:  11%[==&gt;                      ]  ETA: 0:00:01Evaluating over 64 metamodels:  12%[===&gt;                     ]  ETA: 0:00:01Evaluating over 64 metamodels:  14%[===&gt;                     ]  ETA: 0:00:01Evaluating over 64 metamodels:  16%[===&gt;                     ]  ETA: 0:00:01Evaluating over 64 metamodels:  17%[====&gt;                    ]  ETA: 0:00:01Evaluating over 64 metamodels:  19%[====&gt;                    ]  ETA: 0:00:01Evaluating over 64 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 64 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 64 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 64 metamodels:  25%[======&gt;                  ]  ETA: 0:00:01Evaluating over 64 metamodels:  27%[======&gt;                  ]  ETA: 0:00:01Evaluating over 64 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:01Evaluating over 64 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:01Evaluating over 64 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:01Evaluating over 64 metamodels:  33%[========&gt;                ]  ETA: 0:00:00Evaluating over 64 metamodels:  34%[========&gt;                ]  ETA: 0:00:00Evaluating over 64 metamodels:  36%[========&gt;                ]  ETA: 0:00:00Evaluating over 64 metamodels:  38%[=========&gt;               ]  ETA: 0:00:00Evaluating over 64 metamodels:  39%[=========&gt;               ]  ETA: 0:00:00Evaluating over 64 metamodels:  41%[==========&gt;              ]  ETA: 0:00:00Evaluating over 64 metamodels:  42%[==========&gt;              ]  ETA: 0:00:00Evaluating over 64 metamodels:  44%[==========&gt;              ]  ETA: 0:00:00Evaluating over 64 metamodels:  45%[===========&gt;             ]  ETA: 0:00:00Evaluating over 64 metamodels:  47%[===========&gt;             ]  ETA: 0:00:00Evaluating over 64 metamodels:  48%[============&gt;            ]  ETA: 0:00:00Evaluating over 64 metamodels:  50%[============&gt;            ]  ETA: 0:00:00Evaluating over 64 metamodels:  52%[============&gt;            ]  ETA: 0:00:00Evaluating over 64 metamodels:  53%[=============&gt;           ]  ETA: 0:00:00Evaluating over 64 metamodels:  55%[=============&gt;           ]  ETA: 0:00:00Evaluating over 64 metamodels:  56%[==============&gt;          ]  ETA: 0:00:00Evaluating over 64 metamodels:  58%[==============&gt;          ]  ETA: 0:00:00Evaluating over 64 metamodels:  59%[==============&gt;          ]  ETA: 0:00:00Evaluating over 64 metamodels:  61%[===============&gt;         ]  ETA: 0:00:00Evaluating over 64 metamodels:  62%[===============&gt;         ]  ETA: 0:00:00Evaluating over 64 metamodels:  64%[================&gt;        ]  ETA: 0:00:00Evaluating over 64 metamodels:  66%[================&gt;        ]  ETA: 0:00:00Evaluating over 64 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 64 metamodels:  69%[=================&gt;       ]  ETA: 0:00:00Evaluating over 64 metamodels:  70%[=================&gt;       ]  ETA: 0:00:00Evaluating over 64 metamodels:  72%[=================&gt;       ]  ETA: 0:00:00Evaluating over 64 metamodels:  73%[==================&gt;      ]  ETA: 0:00:00Evaluating over 64 metamodels:  75%[==================&gt;      ]  ETA: 0:00:00Evaluating over 64 metamodels:  77%[===================&gt;     ]  ETA: 0:00:00Evaluating over 64 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 64 metamodels:  80%[===================&gt;     ]  ETA: 0:00:00Evaluating over 64 metamodels:  81%[====================&gt;    ]  ETA: 0:00:00Evaluating over 64 metamodels:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 64 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 64 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 64 metamodels:  88%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 64 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 64 metamodels:  91%[======================&gt;  ]  ETA: 0:00:00Evaluating over 64 metamodels:  92%[=======================&gt; ]  ETA: 0:00:00Evaluating over 64 metamodels:  94%[=======================&gt; ]  ETA: 0:00:00Evaluating over 64 metamodels:  95%[=======================&gt; ]  ETA: 0:00:00Evaluating over 64 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels: 100%[=========================] Time: 0:00:00\nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{Union{AbstractVector{Count}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n\n\nimport MLJLinearModels âœ”\nimport MLJLIBSVMInterface âœ”\nimport NearestNeighborModels âœ”\n\n\ntrained Machine; does not cache data\n  model: ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦)\n  args: \n    1:  Source @581 â Table{Union{AbstractVector{Count}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}\n    2:  Source @414 â AbstractVector{OrderedFactor{2}}\n\n\n\n\n3.2 predict test results\n\n\nCode\nyhat=predict_mode(mach, Xtest)|&gt;Array\n@info \"german-creditcard è¿çº¦é¢„æµ‹å‡†ç¡®ç‡\"=&gt;accuracy(ytest,yhat)|&gt;d-&gt;round(d,digits=3)\n\n\n[ Info: \"german-creditcard è¿çº¦é¢„æµ‹å‡†ç¡®ç‡\" =&gt; 0.74"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html",
    "href": "category/regression/5-boston-housing-mixture-regression.html",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nåˆ©ç”¨ Boston houseing å±æ€§é¢„æµ‹æˆ¿ä»·, å˜é‡å¯èƒ½ä¼šå­˜åœ¨äº¤äº’ä½œç”¨\næ‰€ä»¥è€ƒè™‘ä½¿ç”¨æ··åˆæ¨¡å‹"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing MLJ"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nX, y= @load_boston;"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "href": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nmodelType= @load GaussianMixtureRegressor pkg = \"BetaML\"\ngmr= modelType()\n\n(fitResults, cache, report) = MLJ.fit(gmr, 1, X, y);\n\n\nimport BetaML âœ”\nIter. 1:    Var. of the post  21.74887448784977       Log-likelihood -21687.09917379566\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n3.2 results\n\n\nCode\ny_res= predict(gmr, fitResults, X)\nrmse(y_res,y)\n\n\n7.9566567641159605"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html",
    "href": "category/regression/1-salary-linear-reg.html",
    "title": "1-salary-linear-reg",
    "section": "",
    "text": "explore YearsExperience and Salary relationship\n\n\ndataset: kaggle salary dataset\næ•°æ®ç±»å‹éœ€è¦åšè½¬æ¢: to_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nusing MLJLinearModels.jl ğŸ”—"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#load-package",
    "href": "category/regression/1-salary-linear-reg.html#load-package",
    "title": "1-salary-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#process-data",
    "href": "category/regression/1-salary-linear-reg.html#process-data",
    "title": "1-salary-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\nload(csv)-&gt;dataframe ==&gt;sciencetype ==&gt;MLJ table\n\n\n\n\nCode\ndf=CSV.File(\"./data/salary_dataset.csv\") |&gt; DataFrame |&gt; dropmissing;\nfirst(df,5)\n\n\n5Ã—3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\nto_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nnew_df=to_ScienceType(df)\nfirst(new_df,5)\n\n\n5Ã—3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\n X=MLJ.table(reshape(new_df[:,2],30,1))\n y=Vector(new_df[:,3])\n show(y)\n\n\n[39344.0, 46206.0, 37732.0, 43526.0, 39892.0, 56643.0, 60151.0, 54446.0, 64446.0, 57190.0, 63219.0, 55795.0, 56958.0, 57082.0, 61112.0, 67939.0, 66030.0, 83089.0, 81364.0, 93941.0, 91739.0, 98274.0, 101303.0, 113813.0, 109432.0, 105583.0, 116970.0, 112636.0, 122392.0, 121873.0]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "href": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "title": "1-salary-linear-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 load model\n\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fp=MLJ.fitted_params(mach)  #å­¦ä¹ çš„æ¨¡å‹å‚æ•°\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\nimport MLJLinearModels âœ”\n\n\n(coefs = [:x1 =&gt; 9449.962321455077],\n intercept = 24848.203966523164,)\n\n\n\n\n3.2 build linear function\n\n\nCode\n    a=fp.coefs[1,1][2]\n    b=fp.intercept\n    line_func(t)=a*t+b\n\n\nline_func (generic function with 1 method)"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#plot-results",
    "href": "category/regression/1-salary-linear-reg.html#plot-results",
    "title": "1-salary-linear-reg",
    "section": "4. plot results",
    "text": "4. plot results\n\n\nCode\nxs=range(extrema(new_df[:,2])...,200)\nfig=Figure()\nax=Axis(fig[1,1];xlabel=\"YearsExperience\",ylabel=\"Salary\")\nlines!(ax,xs,line_func.(xs);label=\"fit-line\",linewidth=3)\nscatter!(ax,new_df[:,2],new_df[:,3];label=\"data\",marker_style...)\naxislegend(ax)\nfig"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html",
    "href": "category/regression/2-ecommerce-linear-reg.html",
    "title": "2-ecommerce-linear-reg",
    "section": "",
    "text": "é€šè¿‡ä¸Šç½‘æµè§ˆæ—¶é—´é¢„æµ‹å¹´èŠ±è´¹\n\n\ndataset: kaggle ecommerce dataset\nmodel\nusing MLJLinearModels.jl ğŸ”—"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "href": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "title": "2-ecommerce-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing GLMakie, MLJ,CSV,DataFrames,StatsBase"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "href": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "title": "2-ecommerce-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\nstr=\"Ecommerce-Customers\"   \ndf=CSV.File(\"./data/Ecommerce-Customers.csv\") |&gt; DataFrame |&gt; dropmissing;\nselect!(df,4:8)\nX=df[:,1:4]|&gt;Matrix|&gt;MLJ.table\ny=Vector(df[:,5])\nfirst(df,5)\n\n\n5Ã—5 DataFrame\n\n\n\nRow\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n34.4973\n12.6557\n39.5777\n4.08262\n587.951\n\n\n2\n31.9263\n11.1095\n37.269\n2.66403\n392.205\n\n\n3\n33.0009\n11.3303\n37.1106\n4.10454\n487.548\n\n\n4\n34.3056\n13.7175\n36.7213\n3.12018\n581.852\n\n\n5\n33.3307\n12.7952\n37.5367\n4.44631\n599.406"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "title": "2-ecommerce-linear-reg",
    "section": "3. plot corrleation of variables",
    "text": "3. plot corrleation of variables\n\n\nCode\naxs = []\nlabel=names(df)|&gt;Array\ncolors = [:orange, :lightgreen, :purple,:lightblue,:red,:green]\n\nfig = Figure(resolution=(1400, 1400))\nax=Axis(fig[1,1])\n\nfunction plot_diag(i)\n\n    ax = Axis(fig[i, i])\n    push!(axs, ax)\n    density!(ax, df[:, i]; color=(colors[i], 0.5),\n            strokewidth=1.25, strokecolor=colors[i])\nend\n\n\nfunction plot_cor(i, j)\n    ax = Axis(fig[i, j])\n    scatter!(ax, df[:, i], df[:, j]; color=colors[j])\nend\n\n\nfunction plot_pair()\n    [(i == j ? plot_diag(i) : plot_cor(i, j)) for i in 1:5, j in 1:5]\nend\n\nfunction add_xy_label()\n    for i in 1:5\n        Axis(fig[5, i], xlabel=label[i],)\n        Axis(fig[i, 1], ylabel=label[i],)\n    end\nend\n\nfunction main()\n\n    plot_pair()\n    add_xy_label()\n    return fig\nend\n\nmain()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "title": "2-ecommerce-linear-reg",
    "section": "4. plot pair variablesâ€™s cov and cor matrix",
    "text": "4. plot pair variablesâ€™s cov and cor matrix\n\n\nCode\ndf_cov = df|&gt;Matrix|&gt;cov.|&gt; d -&gt; round(d, digits=3)\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=3)\n\nfunction plot_cov_cor()\n    fig = Figure(resolution=(2200, 800))\n    ax1 = Axis(fig[1, 1]; xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cov matrix\",yreversed=true)\n    ax3 = Axis(fig[1, 3], xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cor matrix\",yreversed=true)\n\n    hm = heatmap!(ax1, df_cov)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cov[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    hm2 = heatmap!(ax3, df_cor)\n    Colorbar(fig[1, 4], hm2)\n    [text!(ax3, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    fig\nend\n\nplot_cov_cor()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "href": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "title": "2-ecommerce-linear-reg",
    "section": "5. MLJ workflow",
    "text": "5. MLJ workflow\n\n5.1 load model\n\n\nCode\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fitted_params(mach)\n\n\nimport MLJLinearModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\n(coefs = [:x1 =&gt; 25.734271084705085, :x2 =&gt; 38.709153810834366, :x3 =&gt; 0.43673883559434407, :x4 =&gt; 61.57732375487839],\n intercept = -1051.5942553006273,)\n\n\n\n\n5.2 predict\n\n\nCode\n  y_hat =predict(mach, X)\n  \"rmsd\"=&gt;rmsd(y,y_hat)\n\n\n\"rmsd\" =&gt; 9.923256785022247\n\n\n\n\n5.3 plot residuals\n\n\nCode\nresid=y_hat.=y\nstem(resid)"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html",
    "href": "category/regression/3-cricket-chirp-rate.html",
    "title": "3-cricket-chirp-rate",
    "section": "",
    "text": "source1\nsource2\nsourc3\n\n\né›ªæ ‘èŸ‹èŸ€çš„é¸£å«å®é™…æ˜¯å¤§è…¿æ‘©æ“¦å‘å‡ºçš„å£°éŸ³, ç»è¿‡æ•°æ®æ”¶é›†,å‘ç°é¸£å«çš„é¢‘ç‡å’Œç¯å¢ƒæ¸©åº¦æ­£ç›¸å…³.\n\nç»è¿‡çº¿æ€§æ‹Ÿåˆå¾—åˆ°çš„å‡½æ•°ä¸º:C(t)=4.25t-157.8"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "href": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "title": "3-cricket-chirp-rate",
    "section": "1. load pacakge",
    "text": "1. load pacakge\n\n\nCode\nimport FileIO:load\nimport MLJ:fit!,match,predict,table,fitted_params\nusing GLMakie, CSV,DataFrames,MLJ,FileIO\nimg=load(\"./data/snowy-cricket.jpg\");"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#process-data",
    "href": "category/regression/3-cricket-chirp-rate.html#process-data",
    "title": "3-cricket-chirp-rate",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\ndf=CSV.File(\"./data/CricketChirps.csv\") |&gt; DataFrame |&gt; dropmissing;\nX=MLJ.table(reshape(df[:,1],7,1))\ny=Vector(df[:,2])\n\ntest_X=range(extrema(df[:,1])...,50)\ntest_X=MLJ.table(reshape(test_X,50,1))\ncols=names(df)\n\n\n2-element Vector{String}:\n \"Temperature\"\n \"Chirps\""
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "href": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "title": "3-cricket-chirp-rate",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n    mach = fit!(machine(LinearRegressor(), X, y))\n    report(mach)\n\n\nimport MLJLinearModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LinearRegressor` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{AbstractVector{Continuous}}, AbstractVector{Count}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Continuous}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\n\n\n3.2 plot fitting curve\n\n\nCode\nyhat=predict(mach,test_X).|&gt;(d-&gt;round(d,digits=2))\nfunction plot_fitting_curve(df,yhat)\n    X=df[:,1]\n    test_X=range(extrema(df[:,1])...,50)\n    cols=names(df)\n    fig=Figure()\n    ax=Axis(fig[1:3,1:3];xlabel=\"$(cols[1])\",ylabel=\"$(cols[2])\",title=\"cricket-chirp\")\n    ax2 = Axis(fig[2,4],title=\"snowy-tree-cricket\")\n    scatter!(ax, X,y,markersize=16,color=(:red,0.8))\n    lines!(ax, test_X,yhat,color=:blue)\n    image!(ax2,img)\n    hidespines!(ax2)\n    hidedecorations!(ax2)\n    fig\nend\nplot_fitting_curve(df,yhat)"
  },
  {
    "objectID": "category/materials.html",
    "href": "category/materials.html",
    "title": "dataset list",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html",
    "href": "category/classification/24-classfication-comparison.html",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#load-package",
    "href": "category/classification/24-classfication-comparison.html#load-package",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#make-data",
    "href": "category/classification/24-classfication-comparison.html#make-data",
    "title": "several classfication model comparison",
    "section": "2. make data",
    "text": "2. make data\n\n\nCode\n    function circle_data()\n    X, y = make_circles(400; noise=0.1, factor=0.3)\n    df = DataFrame(X)\n    df.y = y\n    return df\n    end\n    function moons_data()\n        X, y = make_moons(400; noise=0.1)\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    function blob_data()\n        X, y = make_blobs(400, 2; centers=2, cluster_std=[1.0, 2.0])\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    #cat=df1.y|&gt;levels|&gt;unique\n    colors=[:green, :purple]\n\n\n2-element Vector{Symbol}:\n :green\n :purple"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-function",
    "href": "category/classification/24-classfication-comparison.html#define-function",
    "title": "several classfication model comparison",
    "section": "3. define function",
    "text": "3. define function\n\n\nCode\nfunction plot_origin_data(df)\n    fig=Figure()\n    ax=Axis(fig[1,1])\n    local cat=df.y|&gt;levels|&gt;unique\n    \n    local colors=[:green, :purple]\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        #@show d\n    end\n    fig\nend\n\nnums=100\nfunction boundary_data(df,;n=nums)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = LinRange(xlow,xhigh,n1)\n    ty = LinRange(ylow,yhigh,n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    x_test=MLJ.table(x_test')\n    return tx,ty,x_test\nend\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n        tx,ty,xs,ys, xtest=boundary_data(df)\n        local ax=Axis(fig[row,i],title=\"$(names[i])\")\n\n        contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:phase)\n\n        for (i,c) in enumerate(cat)\n            d=df[y.==c,:]\n            scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        end\n        hidedecorations!(ax)\nend\n\n\nplot_desc_boudary (generic function with 1 method)"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "href": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "title": "several classfication model comparison",
    "section": "4. define machine learning models",
    "text": "4. define machine learning models\n\n\nCode\n    using CatBoost.MLJCatBoostInterface\n    SVC = @load SVC pkg=LIBSVM   \n    KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n    DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n    RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n    CatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n    BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n    Booster = @load AdaBoostStumpClassifier pkg=DecisionTree\n    \n    models=[KNNClassifier,DecisionTreeClassifier,RandomForestClassifier,CatBoostClassifier,BayesianLDA,SVC]\n    names=[\"KNN\",\"DecisionTree\",\"RandomForest\",\"CatBoost\",\"BayesianLDA\",\"SVC\"]\n   function _fit(df::DataFrame,m)\n    X,y=df[:,1:2],df[:,3]\n    _,_,xtest=boundary_data(df;n=nums)\n    local predict= m==MLJLIBSVMInterface.SVC  ? MLJ.predict : MLJ.predict_mode \n    model=m()\n   mach = machine(model, X, y)|&gt;fit!\n   yhat=predict(mach, xtest)\n   ytest=yhat|&gt;Array|&gt;d-&gt;reshape(d,nums,nums)\n   return  ytest\nend\n\n\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n    tx,ty,_=boundary_data(df)\n    local y=df.y\n    local ax=Axis(fig[row,i],title=\"$(names[i])\")\n    cat=y|&gt;levels|&gt;unique\n    contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:redsblues)\n\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n    end\n    hidedecorations!(ax)\n    \n\nend\n\nfunction plot_comparsion(testdata,df;row=1)\n    \n    for (i,data) in enumerate(testdata)\n        plot_desc_boudary(fig,data,i;df=df,row=row)\n    end\n    fig\nend\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJLIBSVMInterface âœ”\nimport NearestNeighborModels âœ”\nimport MLJDecisionTreeInterface âœ”\nimport MLJDecisionTreeInterface âœ”\nimport CatBoost âœ”\nimport MLJMultivariateStatsInterface âœ”\nimport MLJDecisionTreeInterface âœ”\n\n\nplot_comparsion (generic function with 1 method)\n\n\n\n\nCode\nfig=Figure(resolution=(2100,1000))\nfunction plot_comparsion(testdata,df,row=1)\n    \n    for i in eachindex(testdata)\n        plot_desc_boudary(fig,testdata[i],i;df=df,row=row)\n    end\n    fig\nend\n\n\n\ndf1=circle_data()\n\nytest1=[_fit(df1,m) for (i,m) in enumerate(models)]\n\ndf2=moons_data()\nytest2=[_fit(df2,m) for (i,m) in enumerate(models)]\n\ndf3=blob_data()\nytest3=[_fit(df3,m) for (i,m) in enumerate(models)]\n\ndfs=[df2,df1,df3]\nytests=[ytest2,ytest1,ytest3]\n\nfig=Figure(resolution=(2100,1000))\n\nfor (df, data,i)  in zip(dfs,ytests,[1,2,3])\n    plot_comparsion(data,df;row=i)\nend\n\nfig\n\n\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦)."
  },
  {
    "objectID": "category/schedule.html",
    "href": "category/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "category/dataset/index.html",
    "href": "category/dataset/index.html",
    "title": "dataset index list",
    "section": "",
    "text": "â€œintor of datasetâ€\n1"
  }
]