[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia MLJ",
    "section": "",
    "text": "Course info & details here"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html",
    "href": "category/classification/2-diabetes-svm-classficiation.html",
    "title": "2-svm-diabetes-classfication",
    "section": "",
    "text": "介绍\n\n\n\n\n参考博客文章:diagnose-diabetes-with-svm\nSVM(支持向量机)通过引入 kernelfunction,使得模型的分类灵活性大大增强,可以解决更多问题.在julia中可以通过在LIBSVM.jl 引入 kernel function 实现, 参见 文档: Support Vector Machine\nMLJ.jl 通过包装接口也提供相似功能\n响应变量需要转换类型 to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "href": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "title": "2-svm-diabetes-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ: fit!, predict\nusing CSV,DataFrames,Random\nusing MLJ\nusing Plots\nusing KernelFunctions"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "href": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "title": "2-svm-diabetes-classfication",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\n df=load_csv(\"diabetes\")\n to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)\n df=to_ScienceType(df)\n first(df,5)|&gt;display\n y, X =  unpack(df, ==(:Outcome), rng=123);\n (Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.7, multi=true,  rng=123)\ndisplay(schema(X))\n\n\n5×9 DataFrame\n\n\n\nRow\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nCat…\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n┌──────────────────────────┬────────────┬─────────┐\n│ names                    │ scitypes   │ types   │\n├──────────────────────────┼────────────┼─────────┤\n│ Pregnancies              │ Count      │ Int64   │\n│ Glucose                  │ Count      │ Int64   │\n│ BloodPressure            │ Count      │ Int64   │\n│ SkinThickness            │ Count      │ Int64   │\n│ Insulin                  │ Count      │ Int64   │\n│ BMI                      │ Continuous │ Float64 │\n│ DiabetesPedigreeFunction │ Continuous │ Float64 │\n│ Age                      │ Count      │ Int64   │\n└──────────────────────────┴────────────┴─────────┘"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "href": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "title": "2-svm-diabetes-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 defin model\n\n\nCode\nSVC = @load SVC pkg=LIBSVM\n#define kernel function,evaulate  kernelfunctions methods\nkernels=[PolynomialKernel(; degree=2, c=1), \n         SqExponentialKernel(),\n         NeuralNetworkKernel(),\n         LinearKernel(;c=1.0)\n]\n\nsvc_mdls = [SVC(;kernel=k) for  k in kernels]\nsvcs = [machine(model, Xtrain, ytrain;scitype_check_level=0) for model in svc_mdls]\n[fit!(svc) for svc in svcs]\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), …), …).\n\nWARNING: reaching max number of iterations\n[ Info: Training machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), …), …).\n[ Info: Training machine(SVC(kernel = Neural Network Kernel, …), …).\n[ Info: Training machine(SVC(kernel = Linear Kernel (c = 1.0), …), …).\n\nWARNING: reaching max number of iterations\n\n\nimport MLJLIBSVMInterface ✔\n\n\n4-element Vector{Machine{MLJLIBSVMInterface.SVC, true}}:\n machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), …), …)\n machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), …), …)\n machine(SVC(kernel = Neural Network Kernel, …), …)\n machine(SVC(kernel = Linear Kernel (c = 1.0), …), …)\n\n\n\n\n3.2 predict test\n\n\nCode\nfor (idx, str) in enumerate([\"Polynomial \",\"Gaussian\",\"NeuralNetwork\",\"Linear\"])\n    local yhat=predict(svcs[idx],Xtest)\n    local acc=accuracy(yhat,ytest) \n    @info \"$(str) kernel predict accuracy\"=&gt;acc   \nend\n\n\n[ Info: \"Polynomial  kernel predict accuracy\" =&gt; 0.47391304347826085\n[ Info: \"Gaussian kernel predict accuracy\" =&gt; 0.6434782608695652\n[ Info: \"NeuralNetwork kernel predict accuracy\" =&gt; 0.6478260869565218\n[ Info: \"Linear kernel predict accuracy\" =&gt; 0.7782608695652173"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html",
    "href": "category/classification/1-catboost-claffification.html",
    "title": "1-catboost-classfication",
    "section": "",
    "text": "dataset\n\n\n\ndataset 参见 clustering-exercises dataset"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-package",
    "href": "category/classification/1-catboost-claffification.html#load-package",
    "title": "1-catboost-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport Plots:scatter!,contourf\nimport MLJ:predict,predict_mode,measures\nusing Plots, MLJ, CSV, DataFrames\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-data",
    "href": "category/classification/1-catboost-claffification.html#load-data",
    "title": "1-catboost-classfication",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n  df=load_csv(\"basic1\")\n  cat=df[:,:color]|&gt;levels|&gt;length # 类别\n  ytrain, Xtrain =  unpack(df, ==(:color), rng=123);\n  first(df,10)\n\n\n10×3 DataFrame\n\n\n\nRow\nx\ny\ncolor\n\n\n\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n79.4083\n152.834\n0\n\n\n2\n98.0463\n186.911\n0\n\n\n3\n240.579\n48.4737\n1\n\n\n4\n109.687\n277.946\n0\n\n\n5\n249.626\n229.753\n1\n\n\n6\n100.785\n281.983\n0\n\n\n7\n235.33\n109.54\n1\n\n\n8\n262.352\n64.5746\n1\n\n\n9\n76.5589\n204.296\n0\n\n\n10\n245.558\n134.502\n1"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "href": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "title": "1-catboost-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    catboost = CatBoostClassifier(iterations=2,learning_rate=0.20)\n    mach = machine(catboost, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n    tx,ty,xtest=boundary_data(df)  # boudary data and xtest \n    ytest = predict_mode(mach, xtest)[:,1]|&gt;Array\n\n\n[ Info: Training machine(CatBoostClassifier(iterations = 2, …), …).\n\n\n40000-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n ⋮\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n3.2 plot results\n\n\nCode\ncontourf(tx,ty,ytest,levels=cat,color=cgrad(:redsblues),alpha=0.7)\np1=scatter!(df[:,:x],df[:,:y],group=df[:,:color],label=false,ms=3,alpha=0.3)"
  },
  {
    "objectID": "category/getting-started.html",
    "href": "category/getting-started.html",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#loading-package-and-data",
    "href": "category/getting-started.html#loading-package-and-data",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#build-decisiontree-model",
    "href": "category/getting-started.html#build-decisiontree-model",
    "title": "getting started with MLJ",
    "section": "2. build DecisionTree model",
    "text": "2. build DecisionTree model\n\n    y, X = unpack(iris, ==(:target); rng=123);\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n                 measures=[log_loss, accuracy],\n                 verbosity=0)\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJDecisionTreeInterface ✔\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 3.12        │ 1.48    │ [2.88, 1.44, 5 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.913       │ 0.041   │ [0.92, 0.96, 0 ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html",
    "href": "category/regression/4-german-creditcard-logistics-reg.html",
    "title": "4-german-creditcard-logistics-reg",
    "section": "",
    "text": "ref :german-creditcard\nscitype 转换 参考:autotype(d, :few_to_finite)方法"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "title": "4-german-creditcard-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,fit!,predict_mode,range\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "title": "4-german-creditcard-logistics-reg",
    "section": "2. data procsssing",
    "text": "2. data procsssing\n\n\nCode\nXtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "title": "4-german-creditcard-logistics-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\nmodel=LogisticClassifier()\nNuSVC = @load NuSVC pkg=LIBSVM\nmodel2 = NuSVC()\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel3 = KNNClassifier(weights = NearestNeighborModels.Inverse())\n\n\"定义 几个 tune 参数的区间 \"\nk1 =range(model, :gamma, lower=0.1, upper=1.2);\nk2 =range(model, :lambda, lower=0.1, upper=1.2);\nk3 =range(model, :penalty, values=([:l2, :l1,:en,:none]));\nk4 =range(model, :fit_intercept, values=([true, false]));\n\ntuning_logistic = TunedModel(model=model,\n                             resampling = CV(nfolds=4, rng=1234),\n                             tuning = Grid(resolution=8),\n                             range = [k1,k2],\n                             measure=accuracy)\nmach = machine(tuning_logistic, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, …), …), …).\n[ Info: Attempting to evaluate 64 models.\nEvaluating over 64 metamodels:   0%[&gt;                        ]  ETA: N/A┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\nEvaluating over 64 metamodels:   2%[&gt;                        ]  ETA: 0:13:47Evaluating over 64 metamodels:   3%[&gt;                        ]  ETA: 0:07:03Evaluating over 64 metamodels:   5%[=&gt;                       ]  ETA: 0:04:37Evaluating over 64 metamodels:   6%[=&gt;                       ]  ETA: 0:03:25Evaluating over 64 metamodels:   8%[=&gt;                       ]  ETA: 0:02:41Evaluating over 64 metamodels:   9%[==&gt;                      ]  ETA: 0:02:12Evaluating over 64 metamodels:  11%[==&gt;                      ]  ETA: 0:01:51Evaluating over 64 metamodels:  12%[===&gt;                     ]  ETA: 0:01:36Evaluating over 64 metamodels:  14%[===&gt;                     ]  ETA: 0:01:24Evaluating over 64 metamodels:  16%[===&gt;                     ]  ETA: 0:01:14Evaluating over 64 metamodels:  17%[====&gt;                    ]  ETA: 0:01:06Evaluating over 64 metamodels:  19%[====&gt;                    ]  ETA: 0:00:59Evaluating over 64 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:54Evaluating over 64 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:49Evaluating over 64 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:45Evaluating over 64 metamodels:  25%[======&gt;                  ]  ETA: 0:00:41Evaluating over 64 metamodels:  27%[======&gt;                  ]  ETA: 0:00:38Evaluating over 64 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:35Evaluating over 64 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:33Evaluating over 64 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:30Evaluating over 64 metamodels:  33%[========&gt;                ]  ETA: 0:00:28Evaluating over 64 metamodels:  34%[========&gt;                ]  ETA: 0:00:26Evaluating over 64 metamodels:  36%[========&gt;                ]  ETA: 0:00:25Evaluating over 64 metamodels:  38%[=========&gt;               ]  ETA: 0:00:23Evaluating over 64 metamodels:  39%[=========&gt;               ]  ETA: 0:00:22Evaluating over 64 metamodels:  41%[==========&gt;              ]  ETA: 0:00:20Evaluating over 64 metamodels:  42%[==========&gt;              ]  ETA: 0:00:19Evaluating over 64 metamodels:  44%[==========&gt;              ]  ETA: 0:00:18Evaluating over 64 metamodels:  45%[===========&gt;             ]  ETA: 0:00:17Evaluating over 64 metamodels:  47%[===========&gt;             ]  ETA: 0:00:16Evaluating over 64 metamodels:  48%[============&gt;            ]  ETA: 0:00:15Evaluating over 64 metamodels:  50%[============&gt;            ]  ETA: 0:00:14Evaluating over 64 metamodels:  52%[============&gt;            ]  ETA: 0:00:13Evaluating over 64 metamodels:  53%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  55%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  56%[==============&gt;          ]  ETA: 0:00:11Evaluating over 64 metamodels:  58%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  59%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  61%[===============&gt;         ]  ETA: 0:00:09Evaluating over 64 metamodels:  62%[===============&gt;         ]  ETA: 0:00:08Evaluating over 64 metamodels:  64%[================&gt;        ]  ETA: 0:00:08Evaluating over 64 metamodels:  66%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  67%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  69%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  70%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  72%[=================&gt;       ]  ETA: 0:00:05Evaluating over 64 metamodels:  73%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  75%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  77%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  78%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  80%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  81%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  83%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:03Evaluating over 64 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  88%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  89%[======================&gt;  ]  ETA: 0:00:02Evaluating over 64 metamodels:  91%[======================&gt;  ]  ETA: 0:00:01Evaluating over 64 metamodels:  92%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  94%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  95%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels: 100%[=========================] Time: 0:00:14\n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n\n\nimport MLJLinearModels ✔\nimport MLJLIBSVMInterface ✔\nimport NearestNeighborModels ✔\n\n\ntrained Machine; does not cache data\n  model: ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, …), …)\n  args: \n    1:  Source @693 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}\n    2:  Source @676 ⏎ AbstractVector{OrderedFactor{2}}\n\n\n\n\n3.2 predict test results\n\n\nCode\nyhat=predict_mode(mach, Xtest)|&gt;Array\n@info \"german-creditcard 违约预测准确率\"=&gt;accuracy(ytest,yhat)|&gt;d-&gt;round(d,digits=3)\n\n\n[ Info: \"german-creditcard 违约预测准确率\" =&gt; 0.74"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html",
    "href": "category/regression/2-ecommerce-linear-reg.html",
    "title": "2-ecommerce-linear-reg",
    "section": "",
    "text": "通过上网浏览时间预测年花费\n\n\ndataset: kaggle ecommerce dataset\nmodel\nusing MLJLinearModels.jl 🔗"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "href": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "title": "2-ecommerce-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing GLMakie, MLJ,CSV,DataFrames,StatsBase"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "href": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "title": "2-ecommerce-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\nstr=\"Ecommerce-Customers\"   \ndf=CSV.File(\"./data/Ecommerce-Customers.csv\") |&gt; DataFrame |&gt; dropmissing;\nselect!(df,4:8)\nX=df[:,1:4]|&gt;Matrix|&gt;MLJ.table\ny=Vector(df[:,5])\nfirst(df,5)\n\n\n5×5 DataFrame\n\n\n\nRow\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n34.4973\n12.6557\n39.5777\n4.08262\n587.951\n\n\n2\n31.9263\n11.1095\n37.269\n2.66403\n392.205\n\n\n3\n33.0009\n11.3303\n37.1106\n4.10454\n487.548\n\n\n4\n34.3056\n13.7175\n36.7213\n3.12018\n581.852\n\n\n5\n33.3307\n12.7952\n37.5367\n4.44631\n599.406"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "title": "2-ecommerce-linear-reg",
    "section": "3. plot corrleation of variables",
    "text": "3. plot corrleation of variables\n\n\nCode\naxs = []\nlabel=names(df)|&gt;Array\ncolors = [:orange, :lightgreen, :purple,:lightblue,:red,:green]\n\nfig = Figure(resolution=(1400, 1400))\nax=Axis(fig[1,1])\n\nfunction plot_diag(i)\n\n    ax = Axis(fig[i, i])\n    push!(axs, ax)\n    density!(ax, df[:, i]; color=(colors[i], 0.5),\n            strokewidth=1.25, strokecolor=colors[i])\nend\n\n\nfunction plot_cor(i, j)\n    ax = Axis(fig[i, j])\n    scatter!(ax, df[:, i], df[:, j]; color=colors[j])\nend\n\n\nfunction plot_pair()\n    [(i == j ? plot_diag(i) : plot_cor(i, j)) for i in 1:5, j in 1:5]\nend\n\nfunction add_xy_label()\n    for i in 1:5\n        Axis(fig[5, i], xlabel=label[i],)\n        Axis(fig[i, 1], ylabel=label[i],)\n    end\nend\n\nfunction main()\n\n    plot_pair()\n    add_xy_label()\n    return fig\nend\n\nmain()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "title": "2-ecommerce-linear-reg",
    "section": "4. plot pair variables’s cov and cor matrix",
    "text": "4. plot pair variables’s cov and cor matrix\n\n\nCode\ndf_cov = df|&gt;Matrix|&gt;cov.|&gt; d -&gt; round(d, digits=3)\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=3)\n\nfunction plot_cov_cor()\n    fig = Figure(resolution=(2200, 800))\n    ax1 = Axis(fig[1, 1]; xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cov matrix\",yreversed=true)\n    ax3 = Axis(fig[1, 3], xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cor matrix\",yreversed=true)\n\n    hm = heatmap!(ax1, df_cov)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cov[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    hm2 = heatmap!(ax3, df_cor)\n    Colorbar(fig[1, 4], hm2)\n    [text!(ax3, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    fig\nend\n\nplot_cov_cor()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "href": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "title": "2-ecommerce-linear-reg",
    "section": "5. MLJ workflow",
    "text": "5. MLJ workflow\n\n5.1 load model\n\n\nCode\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fitted_params(mach)\n\n\nimport MLJLinearModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n┌ Info: Solver: MLJLinearModels.Analytical\n│   iterative: Bool false\n└   max_inner: Int64 200\n\n\n(coefs = [:x1 =&gt; 25.734271084705085, :x2 =&gt; 38.709153810834366, :x3 =&gt; 0.43673883559434407, :x4 =&gt; 61.57732375487839],\n intercept = -1051.5942553006273,)\n\n\n\n\n5.2 predict\n\n\nCode\n  y_hat =predict(mach, X)\n  \"rmsd\"=&gt;rmsd(y,y_hat)\n\n\n\"rmsd\" =&gt; 9.923256785022247\n\n\n\n\n5.3 plot residuals\n\n\nCode\nresid=y_hat.=y\nstem(resid)"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html",
    "href": "category/regression/5-boston-housing-mixture-regression.html",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "",
    "text": "简介\n\n\n\n\n利用 Boston houseing 属性预测房价, 变量可能会存在交互作用\n所以考虑使用混合模型"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing MLJ"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nX, y= @load_boston;"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "href": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nmodelType= @load GaussianMixtureRegressor pkg = \"BetaML\"\ngmr= modelType()\n\n(fitResults, cache, report) = MLJ.fit(gmr, 1, X, y);\n\n\nimport BetaML ✔\nIter. 1:    Var. of the post  21.74887448784977       Log-likelihood -21687.09917379566\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n3.2 results\n\n\nCode\ny_res= predict(gmr, fitResults, X)\nrmse(y_res,y)\n\n\n7.9566567641159605"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html",
    "href": "category/regression/1-salary-linear-reg.html",
    "title": "1-salary-linear-reg",
    "section": "",
    "text": "explore YearsExperience and Salary relationship\n\n\ndataset: kaggle salary dataset\n数据类型需要做转换: to_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nusing MLJLinearModels.jl 🔗"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#load-package",
    "href": "category/regression/1-salary-linear-reg.html#load-package",
    "title": "1-salary-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#process-data",
    "href": "category/regression/1-salary-linear-reg.html#process-data",
    "title": "1-salary-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\nload(csv)-&gt;dataframe ==&gt;sciencetype ==&gt;MLJ table\n\n\n\n\nCode\ndf=CSV.File(\"./data/salary_dataset.csv\") |&gt; DataFrame |&gt; dropmissing;\nfirst(df,5)\n\n\n5×3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\nto_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nnew_df=to_ScienceType(df)\nfirst(new_df,5)\n\n\n5×3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\n X=MLJ.table(reshape(new_df[:,2],30,1))\n y=Vector(new_df[:,3])\n show(y)\n\n\n[39344.0, 46206.0, 37732.0, 43526.0, 39892.0, 56643.0, 60151.0, 54446.0, 64446.0, 57190.0, 63219.0, 55795.0, 56958.0, 57082.0, 61112.0, 67939.0, 66030.0, 83089.0, 81364.0, 93941.0, 91739.0, 98274.0, 101303.0, 113813.0, 109432.0, 105583.0, 116970.0, 112636.0, 122392.0, 121873.0]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "href": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "title": "1-salary-linear-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 load model\n\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fp=MLJ.fitted_params(mach)  #学习的模型参数\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n┌ Info: Solver: MLJLinearModels.Analytical\n│   iterative: Bool false\n└   max_inner: Int64 200\n\n\nimport MLJLinearModels ✔\n\n\n(coefs = [:x1 =&gt; 9449.962321455077],\n intercept = 24848.203966523164,)\n\n\n\n\n3.2 build linear function\n\n\nCode\n    a=fp.coefs[1,1][2]\n    b=fp.intercept\n    line_func(t)=a*t+b\n\n\nline_func (generic function with 1 method)"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#plot-results",
    "href": "category/regression/1-salary-linear-reg.html#plot-results",
    "title": "1-salary-linear-reg",
    "section": "4. plot results",
    "text": "4. plot results\n\n\nCode\nxs=range(extrema(new_df[:,2])...,200)\nfig=Figure()\nax=Axis(fig[1,1];xlabel=\"YearsExperience\",ylabel=\"Salary\")\nlines!(ax,xs,line_func.(xs);label=\"fit-line\",linewidth=3)\nscatter!(ax,new_df[:,2],new_df[:,3];label=\"data\",marker_style...)\naxislegend(ax)\nfig"
  },
  {
    "objectID": "category/regression/7-test-latexify.html",
    "href": "category/regression/7-test-latexify.html",
    "title": "7-latexify-test",
    "section": "",
    "text": "Code\nusing Latexify,Markdown,Symbolics\n\n@variables x\nexpr=latexify(3x^3 + 2x - 5)\n\nMarkdown.parse(\"\"\"$(expr)\"\"\")\n\n\n\\[\\begin{equation} -5 + 2 x + 3 x^{3} \\end{equation}\\]"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html",
    "href": "category/regression/6-compare of BetalML method.html",
    "title": "6-compare of BetalML models",
    "section": "",
    "text": "使用 BetaMLjl 库 on german-creditcard dataset\n\nref :german-creditcard\n类型转换:coerce(d,autotype(d, (:few_to_finite, :discrete_to_continuous)))\nBetaML是julia中另一个大型的机器学习库,参考文档:BetaML Doc"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-package",
    "href": "category/regression/6-compare of BetalML method.html#load-package",
    "title": "6-compare of BetalML models",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,predict_mode\nimport BetaML\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-data",
    "href": "category/regression/6-compare of BetalML method.html#load-data",
    "title": "6-compare of BetalML models",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n   Xtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#define-models",
    "href": "category/regression/6-compare of BetalML method.html#define-models",
    "title": "6-compare of BetalML models",
    "section": "3. define models",
    "text": "3. define models\n\n\nCode\nfunction define_models()\n\n        modelType1= @load NeuralNetworkClassifier pkg = \"BetaML\"\n\n        layers= [BetaML.DenseLayer(19,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,2,f=BetaML.relu),BetaML.VectorFunctionLayer(2,f=BetaML.softmax)];\n        nn_model= modelType1(layers=layers,opt_alg=BetaML.ADAM())\n\n        modelType2= @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\n        dt_model= modelType2()\n\n        modelType3= @load KernelPerceptron pkg = \"BetaML\"\n        kp_model= modelType3()\n\n\n        modelType4= @load LinearPerceptron pkg = \"BetaML\"\n        lp_model= modelType4()\n\n        modelType5= @load Pegasos pkg = \"BetaML\" verbosity=0\n        peg_model=modelType5()\n\n\n        modelType6= @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\n        rf_model=modelType6()\n\n        \n        cat_model=CatBoostClassifier(iterations=5)\n\n        models=[nn_model,dt_model,kp_model,lp_model,peg_model,rf_model,cat_model]\n        models_name=[\"nn\",\"dt\",\"kp\",\"lp\",\"peg\",\"rf\",\"cat\"]\n        return models,models_name\n    end\n\n    models,models_name=define_models()\n\n\nimport BetaML ✔\nimport BetaML ✔\nimport BetaML ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\n(Probabilistic[NeuralNetworkClassifier(layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([0.05820738438709466 0.15359974309514773 -0.2831571785409146 0.16676850370879254 0.04929165267757313 -0.38285625364631787 0.15943637609587863 -0.27963202262072295 0.4132068837304728 0.23994588656424914 0.3331154888122921 -0.14479877282526987 0.45349354202038267 -0.36374167213500097 0.38082093079144613 0.37683827444194523 -0.46572298136617724 -0.23696388317085407 -0.08100593088111102; 0.06901683915827367 0.10885118809668531 -0.4562971657881886 0.14338110290778844 -0.11795018859442519 0.28052259638617466 -0.39114674196409016 -0.04831482621792332 0.35273693227920394 0.10480838584360691 -0.3882485252335783 -0.4056213461754389 0.017690421658710653 -0.1955787553364121 0.34792091768031325 0.09347562692019068 0.009391921693455096 -0.212095472683146 -0.20048430029910436; -0.06941467294954756 0.16129590429684365 -0.3945640354462488 0.15750470788914933 0.3885122964456477 0.3332096512447256 0.09856154384626498 -0.05614878195860046 0.33348964914921825 0.3707940793066557 -0.4479100528048172 -0.46347117014718187 -0.007039753144278238 -0.011376074492971178 -0.07072402501612518 0.08988149206751922 -0.19302224723166356 0.07734011936937807 -0.2438275089701145; 0.30256089311740614 -0.09932629957089412 -0.0077149806013037114 0.2654651769987186 0.3982010752255802 0.1289689169965758 0.297683799044506 -0.3235707608961984 -0.13947296518093505 -0.3866613481102852 0.4612340110485032 0.09589044406863684 -0.34681211749141494 0.3912379724306973 -0.274696646744903 -0.396760579894517 -0.20437013714880814 0.3495768800225088 0.28755435414271086; 0.0919437046719424 0.16931759505456506 -0.16060918296751214 0.08009622213393358 -0.3610249335742296 -0.3636441188511364 -0.0747093794790551 0.02281957204525198 0.271534490785404 -0.010953880667109894 -0.46554915248123163 -0.013605598378954087 -0.26005124669666824 -0.35861740319732727 0.30156562967838657 -0.14506943028251668 0.37773622090065445 0.0043235812838134935 -0.28832307709763183; -0.3806236147103368 -0.41421792839709537 -0.05618041769254023 0.16700437692063458 0.27102999676282086 0.35732365366629854 -0.05554105450928831 -0.2940072650901307 -0.34858976308809075 -0.08781933261412772 0.07928849524951381 -0.2921509924901116 0.18309806737871553 -0.11845842252775374 -0.06605444214841183 -0.0754080395983614 0.02466617817400185 -0.02664319143567151 0.15425330169287638; 0.13965560253588044 -0.09589353646704613 0.20626211811746914 -0.02921942808900324 0.3573230040979693 0.42935726597655327 -0.23408604902535424 -0.4215264566882938 0.4272593128320675 -0.16466109292776998 -0.33591805893963517 -0.036667714700584164 -0.34113360310055485 -0.3182231815230606 -0.11030028776196782 0.3256404048487927 0.14568923379312165 -0.019597433857755753 0.077261277931228; 0.38597940043614193 0.4319364470182649 -0.28449397939645227 0.07973192193347306 0.2694921479971733 -0.18858938084715898 -0.40054720769577173 -0.35243943303754405 0.2698207629784914 -0.04530609440807121 0.19307566106689628 -0.42992875049556945 -0.4287847726205011 -0.27969985088909566 -0.31641882142491573 -0.12743914848036442 0.31095121929552044 -0.456382107919568 -0.1648328690974179], [-0.2666745908870727, 0.13335549524629603, 0.4625666178783718, 0.3847289564398341, 0.09889588201385607, 0.2458351640238045, 0.2109170440803862, 0.27406286915212036], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([0.005075366742773646 -0.48097438646814705 0.24202419659313423 0.6039221547858566 -0.11976624202124153 -0.26134915538668896 -0.1520800429322422 -0.6079732437148756; -0.15902388506436266 0.4101717997035834 -0.5271613841794951 0.2690671662689458 -0.4230502037386945 0.11813231578901096 0.09325917845478093 0.4018585055010937; -0.2865075182395346 0.43471923470729346 0.2682249356075499 -0.5804239225619936 -0.017040839642027406 -0.3158258548173093 -0.477498223140287 0.13153104269917792; -0.5603066570243207 0.5952577897306224 0.4279333630418678 -0.35588446441781557 0.01209911108282935 -0.29111475402071807 -0.0174045265860725 0.04347310148045058; 0.5361749410563643 -0.3710729959358052 0.3529941097864132 -0.6063062419946463 -0.34004410443094263 0.4490957104375721 0.5123489900427278 0.4783393295378502; -0.11215598338594046 -0.28622835800662383 -0.5851596590693836 0.3360290209415717 -0.4882731373846543 0.3718133861167885 0.40579943105041694 -0.2718409279216593; -0.49954399310383746 0.3947207073626293 -0.5158613400091111 0.49616248887375614 0.20709696475507455 0.021484345248235504 -0.06475831562880818 -0.30896275033352183; -0.05886588471216203 0.2781006271207779 -0.3955067757966984 -0.49202779529733237 -0.49270737466597236 -0.5144344557540311 -0.5739435312843488 -0.005598791115059343], [0.12486624424729331, 0.017621262906071578, -0.5119634127517022, -0.43616584920553747, -0.5048865910191135, 0.020664280050312178, 0.5074801901222517, 0.36057442264095185], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.3362431477665603 0.5672463100472439 0.6291035495812666 0.3352432455473987 0.113728772083315 0.3165847703233825 0.403355366417247 -0.4407581299244073; -0.0742656560025654 -0.5025245602904989 0.6085454291677123 0.30955389410722545 0.40102639527542594 -0.3606842071176862 0.5068886580090403 -0.6273414555421792], [0.0529497761563511, -0.5677199029810545], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 2, 2, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], …), DecisionTreeClassifier(max_depth = 0, …), KernelPerceptron(kernel = radial_kernel, …), LinearPerceptron(initial_coefficients = nothing, …), Pegasos(initial_coefficients = nothing, …), RandomForestClassifier(n_trees = 30, …), CatBoostClassifier(iterations = 5, …)], [\"nn\", \"dt\", \"kp\", \"lp\", \"peg\", \"rf\", \"cat\"])"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#train-model",
    "href": "category/regression/6-compare of BetalML method.html#train-model",
    "title": "6-compare of BetalML models",
    "section": "4. train model",
    "text": "4. train model\n\n\nCode\nfunction train_model()\n    for (idx,model) in enumerate(models[1:6])\n        local (fitResults, cache, report) = MLJ.fit(model, 0, Xtrain,ytrain);\n        local est_classes= predict_mode(model, fitResults, Xtest)\n        local acc=accuracy(ytest,est_classes)|&gt;d-&gt;round(d, digits=3)\n        @info \"$(models_name[idx])===&gt;$(acc)\"\n    end\nend\n\ntrain_model()\n\n\n[ Info: nn===&gt;0.695\n[ Info: dt===&gt;0.705\n[ Info: kp===&gt;0.325\n[ Info: lp===&gt;0.695\n[ Info: peg===&gt;0.695\n[ Info: rf===&gt;0.735"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html",
    "href": "category/regression/8-iris-logistics-reg.html",
    "title": "8-iris-logistics-reg",
    "section": "",
    "text": "简介\n\n\n\n\nref: probml page84 figure 2.13\ndataset:iris\nplots:使用 GLMakie:contourf 方法"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#load-package",
    "href": "category/regression/8-iris-logistics-reg.html#load-package",
    "title": "8-iris-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#process-data",
    "href": "category/regression/8-iris-logistics-reg.html#process-data",
    "title": "8-iris-logistics-reg",
    "section": "2 process data",
    "text": "2 process data\n\n2.1 import iris datset\n\n\nCode\niris = load_iris();\n\n#selectrows(iris, 1:3)  |&gt; pretty\n\niris = DataFrames.DataFrame(iris);\nfirst(iris,5)|&gt;display\ny, X = unpack(iris, ==(:target); rng=123);\n\nX=select!(X,3:4)\n\nbyCat = iris.target\ncateg = unique(byCat)\ncolors1 = [:orange,:lightgreen,:purple];\n\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n2.2 make desc boundary data\n\n生成决策边界实际是利用训练模型对区间内的每个点都做出预测,利用两个属性的最大值和最小值 生成 grid 数据,这是 test数据\n\n\n\nCode\n# grid data\n   n1 = n2 = 200\n   tx = LinRange(0, 8, 200)\n   ty = LinRange(-1, 4, 200)\n   X_test = mapreduce(collect, hcat, Iterators.product(tx, ty))\n   X_test = MLJ.table(X_test')\n\n\nTables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "href": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "title": "8-iris-logistics-reg",
    "section": "3. Logisitcs model",
    "text": "3. Logisitcs model\n\n3.1 training model\n\n\nCode\n     LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n      \n     model = machine(LogisticClassifier(), X,y )\n     fit!(model)\n\n\nimport MLJLinearModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n┌ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, NamedTuple{(), Tuple{}}}\n│   optim_options: Optim.Options{Float64, Nothing}\n└   lbfgs_options: NamedTuple{(), Tuple{}} NamedTuple()\n\n\ntrained Machine; caches model-specific representations of data\n  model: LogisticClassifier(lambda = 2.220446049250313e-16, …)\n  args: \n    1:  Source @794 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @579 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n3.2 predict\n\n\nCode\nŷ = MLJ.predict(model, X_test)\n\nres=mode.(ŷ)|&gt;d-&gt;reshape(d,200,200)\nfunction trans(i)\n     \n    if i==\"setosa\"\n       res=1\n    elseif  i==\"versicolor\"\n       res=2\n       \n    else\n       res=3\n    end\nend\nypred=[trans(res[i,j]) for i in 1:200, j in 1:200]\n\n\n200×200 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  …  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  …  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#plot-res",
    "href": "category/regression/8-iris-logistics-reg.html#plot-res",
    "title": "8-iris-logistics-reg",
    "section": "4 plot res",
    "text": "4 plot res\n\n\nCode\n   function  add_legend(axs)\n      Legend(fig[1,2], axs,\"Label\";width=100,height=200)\n   end\n\n   function desision_boundary(ax)\n      axs=[]\n      for (k, c) in enumerate(categ)\n         indc = findall(x -&gt; x == c, byCat)\n         #@show indc\n         x=scatter!(iris[:,3][indc],iris[:,4][indc];color=colors1[k],markersize=14)\n         push!(axs,x)\n      end\n      return axs\n   end\n\n   fig = Figure(resolution=(800,600))\n   ax=Axis(fig[1,1],xlabel=\"Petal length\",ylabel=\"Petal width\",title=L\"Iris Logistics classfication\")\n   contourf!(ax,tx, ty, ypred, levels=length(categ))\n   axs=desision_boundary(ax)\n   Legend(fig[1,2],[axs...],categ)\n   fig"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html",
    "href": "category/regression/3-cricket-chirp-rate.html",
    "title": "3-cricket-chirp-rate",
    "section": "",
    "text": "source1\nsource2\nsourc3\n\n\n雪树蟋蟀的鸣叫实际是大腿摩擦发出的声音, 经过数据收集,发现鸣叫的频率和环境温度正相关.\n\n经过线性拟合得到的函数为:C(t)=4.25t-157.8"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "href": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "title": "3-cricket-chirp-rate",
    "section": "1. load pacakge",
    "text": "1. load pacakge\n\n\nCode\nimport FileIO:load\nimport MLJ:fit!,match,predict,table,fitted_params\nusing GLMakie, CSV,DataFrames,MLJ,FileIO\nimg=load(\"./data/snowy-cricket.jpg\");"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#process-data",
    "href": "category/regression/3-cricket-chirp-rate.html#process-data",
    "title": "3-cricket-chirp-rate",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\ndf=CSV.File(\"./data/CricketChirps.csv\") |&gt; DataFrame |&gt; dropmissing;\nX=MLJ.table(reshape(df[:,1],7,1))\ny=Vector(df[:,2])\n\ntest_X=range(extrema(df[:,1])...,50)\ntest_X=MLJ.table(reshape(test_X,50,1))\ncols=names(df)\n\n\n2-element Vector{String}:\n \"Temperature\"\n \"Chirps\""
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "href": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "title": "3-cricket-chirp-rate",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n    mach = fit!(machine(LinearRegressor(), X, y))\n    report(mach)\n\n\nimport MLJLinearModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LinearRegressor` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{AbstractVector{Continuous}}, AbstractVector{Count}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Continuous}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n┌ Info: Solver: MLJLinearModels.Analytical\n│   iterative: Bool false\n└   max_inner: Int64 200\n\n\n\n\n3.2 plot fitting curve\n\n\nCode\nyhat=predict(mach,test_X).|&gt;(d-&gt;round(d,digits=2))\nfunction plot_fitting_curve(df,yhat)\n    X=df[:,1]\n    test_X=range(extrema(df[:,1])...,50)\n    cols=names(df)\n    fig=Figure()\n    ax=Axis(fig[1:3,1:3];xlabel=\"$(cols[1])\",ylabel=\"$(cols[2])\",title=\"cricket-chirp\")\n    ax2 = Axis(fig[2,4],title=\"snowy-tree-cricket\")\n    scatter!(ax, X,y,markersize=16,color=(:red,0.8))\n    lines!(ax, test_X,yhat,color=:blue)\n    image!(ax2,img)\n    hidespines!(ax2)\n    hidedecorations!(ax2)\n    fig\nend\nplot_fitting_curve(df,yhat)"
  },
  {
    "objectID": "category/materials.html",
    "href": "category/materials.html",
    "title": "dataset list",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html",
    "href": "category/classification/24-classfication-comparison.html",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#load-package",
    "href": "category/classification/24-classfication-comparison.html#load-package",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#make-data",
    "href": "category/classification/24-classfication-comparison.html#make-data",
    "title": "several classfication model comparison",
    "section": "2. make data",
    "text": "2. make data\n\n\nCode\n    function circle_data()\n    X, y = make_circles(400; noise=0.1, factor=0.3)\n    df = DataFrame(X)\n    df.y = y\n    return df\n    end\n    function moons_data()\n        X, y = make_moons(400; noise=0.1)\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    function blob_data()\n        X, y = make_blobs(400, 2; centers=2, cluster_std=[1.0, 2.0])\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    #cat=df1.y|&gt;levels|&gt;unique\n    colors=[:green, :purple]\n\n\n2-element Vector{Symbol}:\n :green\n :purple"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-function",
    "href": "category/classification/24-classfication-comparison.html#define-function",
    "title": "several classfication model comparison",
    "section": "3. define function",
    "text": "3. define function\n\n\nCode\nfunction plot_origin_data(df)\n    fig=Figure()\n    ax=Axis(fig[1,1])\n    local cat=df.y|&gt;levels|&gt;unique\n    \n    local colors=[:green, :purple]\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        #@show d\n    end\n    fig\nend\n\nnums=100\nfunction boundary_data(df,;n=nums)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = LinRange(xlow,xhigh,n1)\n    ty = LinRange(ylow,yhigh,n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    x_test=MLJ.table(x_test')\n    return tx,ty,x_test\nend\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n        tx,ty,xs,ys, xtest=boundary_data(df)\n        local ax=Axis(fig[row,i],title=\"$(names[i])\")\n\n        contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:phase)\n\n        for (i,c) in enumerate(cat)\n            d=df[y.==c,:]\n            scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        end\n        hidedecorations!(ax)\nend\n\n\nplot_desc_boudary (generic function with 1 method)"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "href": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "title": "several classfication model comparison",
    "section": "4. define machine learning models",
    "text": "4. define machine learning models\n\n\nCode\n    using CatBoost.MLJCatBoostInterface\n    SVC = @load SVC pkg=LIBSVM   \n    KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n    DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n    RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n    CatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n    BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n    Booster = @load AdaBoostStumpClassifier pkg=DecisionTree\n    \n    models=[KNNClassifier,DecisionTreeClassifier,RandomForestClassifier,CatBoostClassifier,BayesianLDA,SVC]\n    names=[\"KNN\",\"DecisionTree\",\"RandomForest\",\"CatBoost\",\"BayesianLDA\",\"SVC\"]\n   function _fit(df::DataFrame,m)\n    X,y=df[:,1:2],df[:,3]\n    _,_,xtest=boundary_data(df;n=nums)\n    local predict= m==MLJLIBSVMInterface.SVC  ? MLJ.predict : MLJ.predict_mode \n    model=m()\n   mach = machine(model, X, y)|&gt;fit!\n   yhat=predict(mach, xtest)\n   ytest=yhat|&gt;Array|&gt;d-&gt;reshape(d,nums,nums)\n   return  ytest\nend\n\n\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n    tx,ty,_=boundary_data(df)\n    local y=df.y\n    local ax=Axis(fig[row,i],title=\"$(names[i])\")\n    cat=y|&gt;levels|&gt;unique\n    contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:redsblues)\n\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n    end\n    hidedecorations!(ax)\n    \n\nend\n\nfunction plot_comparsion(testdata,df;row=1)\n    \n    for (i,data) in enumerate(testdata)\n        plot_desc_boudary(fig,data,i;df=df,row=row)\n    end\n    fig\nend\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJLIBSVMInterface ✔\nimport NearestNeighborModels ✔\nimport MLJDecisionTreeInterface ✔\nimport MLJDecisionTreeInterface ✔\nimport CatBoost ✔\nimport MLJMultivariateStatsInterface ✔\nimport MLJDecisionTreeInterface ✔\n\n\nplot_comparsion (generic function with 1 method)\n\n\n\n\nCode\nfig=Figure(resolution=(2100,1000))\nfunction plot_comparsion(testdata,df,row=1)\n    \n    for i in eachindex(testdata)\n        plot_desc_boudary(fig,testdata[i],i;df=df,row=row)\n    end\n    fig\nend\n\n\n\ndf1=circle_data()\n\nytest1=[_fit(df1,m) for (i,m) in enumerate(models)]\n\ndf2=moons_data()\nytest2=[_fit(df2,m) for (i,m) in enumerate(models)]\n\ndf3=blob_data()\nytest3=[_fit(df3,m) for (i,m) in enumerate(models)]\n\ndfs=[df2,df1,df3]\nytests=[ytest2,ytest1,ytest3]\n\nfig=Figure(resolution=(2100,1000))\n\nfor (df, data,i)  in zip(dfs,ytests,[1,2,3])\n    plot_comparsion(data,df;row=i)\nend\n\nfig\n\n\n[ Info: Training machine(KNNClassifier(K = 5, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, …), …).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, …), …).\n[ Info: Training machine(BayesianLDA(method = gevd, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n[ Info: Training machine(KNNClassifier(K = 5, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, …), …).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, …), …).\n[ Info: Training machine(BayesianLDA(method = gevd, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n[ Info: Training machine(KNNClassifier(K = 5, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, …), …).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, …), …).\n[ Info: Training machine(BayesianLDA(method = gevd, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …)."
  },
  {
    "objectID": "category/schedule.html",
    "href": "category/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "category/dataset/index.html",
    "href": "category/dataset/index.html",
    "title": "dataset index list",
    "section": "",
    "text": "“intor of dataset”\n1"
  }
]