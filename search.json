[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About MLJ",
    "section": "",
    "text": "MLJ(Machine Learning in Julia) å‚è§ Blaom et al. (2020), æ˜¯ä½¿ç”¨Juliaè¯­è¨€çš„æœºå™¨å­¦ä¹ é›†æˆå·¥å…·ç®±,ç±»ä¼¼Python çš„ scikit-learn,å…¶ä¸­åŒ…æ‹¬ 180å¤šç§åŸç”ŸJulia è¯­è¨€æ¨¡å‹, åŒæ—¶ä¹ŸåŒ…æ‹¬äº†å¤§é‡çš„scikit-learn æ¨¡å‹"
  },
  {
    "objectID": "index.html#æµè§ˆmlj-model",
    "href": "index.html#æµè§ˆmlj-model",
    "title": "About MLJ",
    "section": "1. æµè§ˆMLJ Model",
    "text": "1. æµè§ˆMLJ Model\nç”±äºå¯¼å…¥çš„æ¨¡å‹éå¸¸å¤š, æ‰€ä»¥devç‰ˆæ–‡æ¡£ æä¾›äº†å¿«é€Ÿæµè§ˆæ¨¡å‹å†…å®¹,æŒ‰ç…§æœºå™¨å­¦ä¹ çš„ç±»å‹è¿›è¡Œåˆ’åˆ†"
  },
  {
    "objectID": "index.html#mlj-workflow",
    "href": "index.html#mlj-workflow",
    "title": "About MLJ",
    "section": "2. MLJ workflow",
    "text": "2. MLJ workflow\n\nimport data\nmodel search\nInstantiating a model\nEvaluating a model\nperformance evaluation\n\nåŸºæœ¬ä¸Šæ‰€æœ‰çš„ quarto note éƒ½éµå¾ªè¿™ä¸ªæµç¨‹"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html",
    "href": "category/classification/2-diabetes-svm-classficiation.html",
    "title": "2-svm-diabetes-classfication",
    "section": "",
    "text": "ä»‹ç»\n\n\n\n\nå‚è€ƒåšå®¢æ–‡ç« :diagnose-diabetes-with-svm\nSVM(æ”¯æŒå‘é‡æœº)é€šè¿‡å¼•å…¥ kernelfunction,ä½¿å¾—æ¨¡å‹çš„åˆ†ç±»çµæ´»æ€§å¤§å¤§å¢å¼º,å¯ä»¥è§£å†³æ›´å¤šé—®é¢˜.åœ¨juliaä¸­å¯ä»¥é€šè¿‡åœ¨LIBSVM.jl å¼•å…¥ kernel function å®ç°, å‚è§ æ–‡æ¡£: Support Vector Machine\nMLJ.jl é€šè¿‡åŒ…è£…æ¥å£ä¹Ÿæä¾›ç›¸ä¼¼åŠŸèƒ½\nå“åº”å˜é‡éœ€è¦è½¬æ¢ç±»å‹ to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "href": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "title": "2-svm-diabetes-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ: fit!, predict\nusing CSV,DataFrames,Random\nusing MLJ\nusing Plots\nusing KernelFunctions"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "href": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "title": "2-svm-diabetes-classfication",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\n df=load_csv(\"diabetes\")\n to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)\n df=to_ScienceType(df)\n first(df,5)|&gt;display\n y, X =  unpack(df, ==(:Outcome), rng=123);\n (Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.7, multi=true,  rng=123)\ndisplay(schema(X))\n\n\n5Ã—9 DataFrame\n\n\n\nRow\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nCatâ€¦\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names                    â”‚ scitypes   â”‚ types   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Pregnancies              â”‚ Count      â”‚ Int64   â”‚\nâ”‚ Glucose                  â”‚ Count      â”‚ Int64   â”‚\nâ”‚ BloodPressure            â”‚ Count      â”‚ Int64   â”‚\nâ”‚ SkinThickness            â”‚ Count      â”‚ Int64   â”‚\nâ”‚ Insulin                  â”‚ Count      â”‚ Int64   â”‚\nâ”‚ BMI                      â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ DiabetesPedigreeFunction â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ Age                      â”‚ Count      â”‚ Int64   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "href": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "title": "2-svm-diabetes-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 defin model\n\n\nCode\nSVC = @load SVC pkg=LIBSVM\n#define kernel function,evaulate  kernelfunctions methods\nkernels=[PolynomialKernel(; degree=2, c=1), \n         SqExponentialKernel(),\n         NeuralNetworkKernel(),\n         LinearKernel(;c=1.0)\n]\n\nsvc_mdls = [SVC(;kernel=k) for  k in kernels]\nsvcs = [machine(model, Xtrain, ytrain;scitype_check_level=0) for model in svc_mdls]\n[fit!(svc) for svc in svcs]\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), â€¦), â€¦).\n\nWARNING: reaching max number of iterations\n[ Info: Training machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = Neural Network Kernel, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = Linear Kernel (c = 1.0), â€¦), â€¦).\n\nWARNING: reaching max number of iterations\n\n\nimport MLJLIBSVMInterface âœ”\n\n\n4-element Vector{Machine{MLJLIBSVMInterface.SVC, true}}:\n machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), â€¦), â€¦)\n machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), â€¦), â€¦)\n machine(SVC(kernel = Neural Network Kernel, â€¦), â€¦)\n machine(SVC(kernel = Linear Kernel (c = 1.0), â€¦), â€¦)\n\n\n\n\n3.2 predict test\n\n\nCode\nfor (idx, str) in enumerate([\"Polynomial \",\"Gaussian\",\"NeuralNetwork\",\"Linear\"])\n    local yhat=predict(svcs[idx],Xtest)\n    local acc=accuracy(yhat,ytest) \n    @info \"$(str) kernel predict accuracy\"=&gt;acc   \nend\n\n\n[ Info: \"Polynomial  kernel predict accuracy\" =&gt; 0.47391304347826085\n[ Info: \"Gaussian kernel predict accuracy\" =&gt; 0.6434782608695652\n[ Info: \"NeuralNetwork kernel predict accuracy\" =&gt; 0.6478260869565218\n[ Info: \"Linear kernel predict accuracy\" =&gt; 0.7782608695652173"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html",
    "href": "category/classification/1-catboost-claffification.html",
    "title": "1-catboost-classfication",
    "section": "",
    "text": "dataset\n\n\n\ndataset å‚è§ clustering-exercises dataset"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-package",
    "href": "category/classification/1-catboost-claffification.html#load-package",
    "title": "1-catboost-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport Plots:scatter!,contourf\nimport MLJ:predict,predict_mode,measures\nusing Plots, MLJ, CSV, DataFrames\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-data",
    "href": "category/classification/1-catboost-claffification.html#load-data",
    "title": "1-catboost-classfication",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n  df=load_csv(\"basic1\")\n  cat=df[:,:color]|&gt;levels|&gt;length # ç±»åˆ«\n  ytrain, Xtrain =  unpack(df, ==(:color), rng=123);\n  first(df,10)\n\n\n10Ã—3 DataFrame\n\n\n\nRow\nx\ny\ncolor\n\n\n\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n79.4083\n152.834\n0\n\n\n2\n98.0463\n186.911\n0\n\n\n3\n240.579\n48.4737\n1\n\n\n4\n109.687\n277.946\n0\n\n\n5\n249.626\n229.753\n1\n\n\n6\n100.785\n281.983\n0\n\n\n7\n235.33\n109.54\n1\n\n\n8\n262.352\n64.5746\n1\n\n\n9\n76.5589\n204.296\n0\n\n\n10\n245.558\n134.502\n1"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "href": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "title": "1-catboost-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    catboost = CatBoostClassifier(iterations=2,learning_rate=0.20)\n    mach = machine(catboost, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n    tx,ty,xtest=boundary_data(df)  # boudary data and xtest \n    ytest = predict_mode(mach, xtest)[:,1]|&gt;Array\n\n\n[ Info: Training machine(CatBoostClassifier(iterations = 2, â€¦), â€¦).\n\n\n40000-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n â‹®\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n3.2 plot results\n\n\nCode\ncontourf(tx,ty,ytest,levels=cat,color=cgrad(:redsblues),alpha=0.7)\np1=scatter!(df[:,:x],df[:,:y],group=df[:,:color],label=false,ms=3,alpha=0.3)"
  },
  {
    "objectID": "category/getting-started.html",
    "href": "category/getting-started.html",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#loading-package-and-data",
    "href": "category/getting-started.html#loading-package-and-data",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#build-decisiontree-model",
    "href": "category/getting-started.html#build-decisiontree-model",
    "title": "getting started with MLJ",
    "section": "2. build DecisionTree model",
    "text": "2. build DecisionTree model\n\n    y, X = unpack(iris, ==(:target); rng=123);\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n                 measures=[log_loss, accuracy],\n                 verbosity=0)\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJDecisionTreeInterface âœ”\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 3.12        â”‚ 1.48    â”‚ [2.88, 1.44, 5 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.913       â”‚ 0.041   â”‚ [0.92, 0.96, 0 â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html",
    "href": "category/regression/4-german-creditcard-logistics-reg.html",
    "title": "4-german-creditcard-logistics-reg",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nref :german-creditcard\nscitype è½¬æ¢ å‚è€ƒ:autotype(d, :few_to_finite)æ–¹æ³•"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "title": "4-german-creditcard-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,fit!,predict_mode,range\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "title": "4-german-creditcard-logistics-reg",
    "section": "2. data procsssing",
    "text": "2. data procsssing\n\n\nCode\nXtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "title": "4-german-creditcard-logistics-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\nmodel=LogisticClassifier()\nNuSVC = @load NuSVC pkg=LIBSVM\nmodel2 = NuSVC()\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel3 = KNNClassifier(weights = NearestNeighborModels.Inverse())\n\n\"å®šä¹‰ å‡ ä¸ª tune å‚æ•°çš„åŒºé—´ \"\nk1 =range(model, :gamma, lower=0.1, upper=1.2);\nk2 =range(model, :lambda, lower=0.1, upper=1.2);\nk3 =range(model, :penalty, values=([:l2, :l1,:en,:none]));\nk4 =range(model, :fit_intercept, values=([true, false]));\n\ntuning_logistic = TunedModel(model=model,\n                             resampling = CV(nfolds=4, rng=1234),\n                             tuning = Grid(resolution=8),\n                             range = [k1,k2],\n                             measure=accuracy)\nmach = machine(tuning_logistic, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦), â€¦).\n[ Info: Attempting to evaluate 64 models.\nEvaluating over 64 metamodels:   0%[&gt;                        ]  ETA: N/Aâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\nEvaluating over 64 metamodels:   2%[&gt;                        ]  ETA: 0:13:55Evaluating over 64 metamodels:   3%[&gt;                        ]  ETA: 0:07:07Evaluating over 64 metamodels:   5%[=&gt;                       ]  ETA: 0:04:40Evaluating over 64 metamodels:   6%[=&gt;                       ]  ETA: 0:03:27Evaluating over 64 metamodels:   8%[=&gt;                       ]  ETA: 0:02:43Evaluating over 64 metamodels:   9%[==&gt;                      ]  ETA: 0:02:13Evaluating over 64 metamodels:  11%[==&gt;                      ]  ETA: 0:01:52Evaluating over 64 metamodels:  12%[===&gt;                     ]  ETA: 0:01:37Evaluating over 64 metamodels:  14%[===&gt;                     ]  ETA: 0:01:25Evaluating over 64 metamodels:  16%[===&gt;                     ]  ETA: 0:01:15Evaluating over 64 metamodels:  17%[====&gt;                    ]  ETA: 0:01:07Evaluating over 64 metamodels:  19%[====&gt;                    ]  ETA: 0:01:00Evaluating over 64 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:54Evaluating over 64 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:50Evaluating over 64 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:45Evaluating over 64 metamodels:  25%[======&gt;                  ]  ETA: 0:00:42Evaluating over 64 metamodels:  27%[======&gt;                  ]  ETA: 0:00:38Evaluating over 64 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:36Evaluating over 64 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:33Evaluating over 64 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:31Evaluating over 64 metamodels:  33%[========&gt;                ]  ETA: 0:00:29Evaluating over 64 metamodels:  34%[========&gt;                ]  ETA: 0:00:27Evaluating over 64 metamodels:  36%[========&gt;                ]  ETA: 0:00:25Evaluating over 64 metamodels:  38%[=========&gt;               ]  ETA: 0:00:23Evaluating over 64 metamodels:  39%[=========&gt;               ]  ETA: 0:00:22Evaluating over 64 metamodels:  41%[==========&gt;              ]  ETA: 0:00:20Evaluating over 64 metamodels:  42%[==========&gt;              ]  ETA: 0:00:19Evaluating over 64 metamodels:  44%[==========&gt;              ]  ETA: 0:00:18Evaluating over 64 metamodels:  45%[===========&gt;             ]  ETA: 0:00:17Evaluating over 64 metamodels:  47%[===========&gt;             ]  ETA: 0:00:16Evaluating over 64 metamodels:  48%[============&gt;            ]  ETA: 0:00:15Evaluating over 64 metamodels:  50%[============&gt;            ]  ETA: 0:00:14Evaluating over 64 metamodels:  52%[============&gt;            ]  ETA: 0:00:13Evaluating over 64 metamodels:  53%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  55%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  56%[==============&gt;          ]  ETA: 0:00:11Evaluating over 64 metamodels:  58%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  59%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  61%[===============&gt;         ]  ETA: 0:00:09Evaluating over 64 metamodels:  62%[===============&gt;         ]  ETA: 0:00:08Evaluating over 64 metamodels:  64%[================&gt;        ]  ETA: 0:00:08Evaluating over 64 metamodels:  66%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  67%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  69%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  70%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  72%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  73%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  75%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  77%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  78%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  80%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  81%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  83%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:03Evaluating over 64 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  88%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  89%[======================&gt;  ]  ETA: 0:00:02Evaluating over 64 metamodels:  91%[======================&gt;  ]  ETA: 0:00:01Evaluating over 64 metamodels:  92%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  94%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  95%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels: 100%[=========================] Time: 0:00:14\nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n\n\nimport MLJLinearModels âœ”\nimport MLJLIBSVMInterface âœ”\nimport NearestNeighborModels âœ”\n\n\ntrained Machine; does not cache data\n  model: ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦)\n  args: \n    1:  Source @007 â Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}\n    2:  Source @757 â AbstractVector{OrderedFactor{2}}\n\n\n\n\n3.2 predict test results\n\n\nCode\nyhat=predict_mode(mach, Xtest)|&gt;Array\n@info \"german-creditcard è¿çº¦é¢„æµ‹å‡†ç¡®ç‡\"=&gt;accuracy(ytest,yhat)|&gt;d-&gt;round(d,digits=3)\n\n\n[ Info: \"german-creditcard è¿çº¦é¢„æµ‹å‡†ç¡®ç‡\" =&gt; 0.74"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html",
    "href": "category/regression/8-iris-logistics-reg.html",
    "title": "8-iris-logistics-reg",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nref: probml page84 figure 2.13\ndataset:iris\nplots:ä½¿ç”¨ GLMakie:contourf æ–¹æ³•"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#load-package",
    "href": "category/regression/8-iris-logistics-reg.html#load-package",
    "title": "8-iris-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#process-data",
    "href": "category/regression/8-iris-logistics-reg.html#process-data",
    "title": "8-iris-logistics-reg",
    "section": "2 process data",
    "text": "2 process data\n\n2.1 import iris datset\n\n\nCode\niris = load_iris();\n\n#selectrows(iris, 1:3)  |&gt; pretty\n\niris = DataFrames.DataFrame(iris);\nfirst(iris,5)|&gt;display\ny, X = unpack(iris, ==(:target); rng=123);\n\nX=select!(X,3:4)\n\nbyCat = iris.target\ncateg = unique(byCat)\ncolors1 = [:orange,:lightgreen,:purple];\n\n\n5Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n2.2 make desc boundary data\n\nç”Ÿæˆå†³ç­–è¾¹ç•Œå®é™…æ˜¯åˆ©ç”¨è®­ç»ƒæ¨¡å‹å¯¹åŒºé—´å†…çš„æ¯ä¸ªç‚¹éƒ½åšå‡ºé¢„æµ‹,åˆ©ç”¨ä¸¤ä¸ªå±æ€§çš„æœ€å¤§å€¼å’Œæœ€å°å€¼ ç”Ÿæˆ grid æ•°æ®,è¿™æ˜¯ testæ•°æ®\n\n\n\nCode\n# grid data\n   n1 = n2 = 200\n   tx = LinRange(0, 8, 200)\n   ty = LinRange(-1, 4, 200)\n   X_test = mapreduce(collect, hcat, Iterators.product(tx, ty))\n   X_test = MLJ.table(X_test')\n\n\nTables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "href": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "title": "8-iris-logistics-reg",
    "section": "3. Logisitcs model",
    "text": "3. Logisitcs model\n\n3.1 training model\n\n\nCode\n     LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n      \n     model = machine(LogisticClassifier(), X,y )\n     fit!(model)\n\n\nimport MLJLinearModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, NamedTuple{(), Tuple{}}}\nâ”‚   optim_options: Optim.Options{Float64, Nothing}\nâ””   lbfgs_options: NamedTuple{(), Tuple{}} NamedTuple()\n\n\ntrained Machine; caches model-specific representations of data\n  model: LogisticClassifier(lambda = 2.220446049250313e-16, â€¦)\n  args: \n    1:  Source @794 â Table{AbstractVector{Continuous}}\n    2:  Source @579 â AbstractVector{Multiclass{3}}\n\n\n\n\n3.2 predict\n\n\nCode\nyÌ‚ = MLJ.predict(model, X_test)\n\nres=mode.(yÌ‚)|&gt;d-&gt;reshape(d,200,200)\nfunction trans(i)\n     \n    if i==\"setosa\"\n       res=1\n    elseif  i==\"versicolor\"\n       res=2\n       \n    else\n       res=3\n    end\nend\nypred=[trans(res[i,j]) for i in 1:200, j in 1:200]\n\n\n200Ã—200 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n â‹®              â‹®              â‹®        â‹±        â‹®              â‹®           \n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  â€¦  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  â€¦  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#plot-res",
    "href": "category/regression/8-iris-logistics-reg.html#plot-res",
    "title": "8-iris-logistics-reg",
    "section": "4 plot res",
    "text": "4 plot res\n\n\nCode\n   function  add_legend(axs)\n      Legend(fig[1,2], axs,\"Label\";width=100,height=200)\n   end\n\n   function desision_boundary(ax)\n      axs=[]\n      for (k, c) in enumerate(categ)\n         indc = findall(x -&gt; x == c, byCat)\n         #@show indc\n         x=scatter!(iris[:,3][indc],iris[:,4][indc];color=colors1[k],markersize=14)\n         push!(axs,x)\n      end\n      return axs\n   end\n\n   fig = Figure(resolution=(800,600))\n   ax=Axis(fig[1,1],xlabel=\"Petal length\",ylabel=\"Petal width\",title=L\"Iris Logistics classfication\")\n   contourf!(ax,tx, ty, ypred, levels=length(categ))\n   axs=desision_boundary(ax)\n   Legend(fig[1,2],[axs...],categ)\n   fig"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html",
    "href": "category/regression/6-compare of BetalML method.html",
    "title": "6-compare of BetalML models",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\nä½¿ç”¨ BetaMLjl åº“ on german-creditcard dataset\n\nref :german-creditcard\nç±»å‹è½¬æ¢:coerce(d,autotype(d, (:few_to_finite, :discrete_to_continuous)))\nBetaMLæ˜¯juliaä¸­å¦ä¸€ä¸ªå¤§å‹çš„æœºå™¨å­¦ä¹ åº“,å‚è€ƒæ–‡æ¡£:BetaML Doc"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-package",
    "href": "category/regression/6-compare of BetalML method.html#load-package",
    "title": "6-compare of BetalML models",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,predict_mode\nimport BetaML\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-data",
    "href": "category/regression/6-compare of BetalML method.html#load-data",
    "title": "6-compare of BetalML models",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n   Xtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#define-models",
    "href": "category/regression/6-compare of BetalML method.html#define-models",
    "title": "6-compare of BetalML models",
    "section": "3. define models",
    "text": "3. define models\n\n\nCode\nfunction define_models()\n\n        modelType1= @load NeuralNetworkClassifier pkg = \"BetaML\"\n\n        layers= [BetaML.DenseLayer(19,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,2,f=BetaML.relu),BetaML.VectorFunctionLayer(2,f=BetaML.softmax)];\n        nn_model= modelType1(layers=layers,opt_alg=BetaML.ADAM())\n\n        modelType2= @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\n        dt_model= modelType2()\n\n        modelType3= @load KernelPerceptron pkg = \"BetaML\"\n        kp_model= modelType3()\n\n\n        modelType4= @load LinearPerceptron pkg = \"BetaML\"\n        lp_model= modelType4()\n\n        modelType5= @load Pegasos pkg = \"BetaML\" verbosity=0\n        peg_model=modelType5()\n\n\n        modelType6= @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\n        rf_model=modelType6()\n\n        \n        cat_model=CatBoostClassifier(iterations=5)\n\n        models=[nn_model,dt_model,kp_model,lp_model,peg_model,rf_model,cat_model]\n        models_name=[\"nn\",\"dt\",\"kp\",\"lp\",\"peg\",\"rf\",\"cat\"]\n        return models,models_name\n    end\n\n    models,models_name=define_models()\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport BetaML âœ”\nimport BetaML âœ”\nimport BetaML âœ”\n\n\n(Probabilistic[NeuralNetworkClassifier(layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.36074636511061065 -0.39582846276007433 -0.19367369489258768 0.2680078206831555 0.23415797915589948 -0.35426717723464507 0.41873972936820797 0.03695708843884965 -0.1262241154740001 -0.4535341279511609 0.27083756550881694 -0.26285190070170183 -0.29113824789040094 0.3272732026253102 0.3120181348536681 0.03189504313141717 0.17859507646360423 0.24100348772661923 0.014525967665540818; 0.02585857761506677 -0.1786289777618676 0.27390992608661585 -0.00882009262935829 -0.05597939255352807 -0.45185469645137644 0.43448083345893546 -0.07933636613135336 -0.40808183079083227 -0.1512087063152568 -0.22346202440402502 0.24282973121527757 -0.4611082088959598 -0.22314239815908504 -0.2040062087828935 -0.23716874176485098 -0.4007849436405437 -0.029299013620295244 -0.2240163921875015; 0.3586660049153875 0.0990446439566613 -0.30304464083062305 -0.35711866133831294 -0.18581141830140957 0.10300116526719799 -0.3547092905827105 0.17358444569480663 0.46816569231449073 0.36725354636566204 -0.09950624589607898 -0.18348423586113166 -0.05591644911594651 0.21540326133872428 0.4397024519627983 -0.019152022051641848 0.015081037200303848 -0.09338789491179 0.2698572766549305; -0.14737750284903278 0.15390901525950657 0.11733088463889524 -0.18553420478116528 0.2773342867980156 -0.44187683747564216 -0.07128222618035901 -0.3975783832585831 -0.1797548030782582 0.45414987217334984 0.11774352278251005 0.4318637917472961 0.34114366748895236 0.20543094312431615 0.45879918119036506 0.19966031284458824 -0.4243977564030008 0.3528498049246546 -0.26449251745539387; -0.46603077560973166 0.363177679644008 0.15387610207235164 0.16878070443105436 0.22469025601685005 0.43163009427259763 0.3433671636225248 -0.04331120409999056 -0.3450286254110857 0.27013134708859105 -0.1246971621378602 0.18019183676577127 -0.22935636935801848 -0.38713683379338093 -0.36324994527472265 0.12175941762041481 0.37182252248090214 -0.2946228229043437 0.36728385779700773; -0.35965444222821236 -0.2513459765142385 -0.12760789988466997 -0.3651832232912821 0.2275393345538726 0.19344000211089812 -0.06541914124742532 0.45592078923421525 -0.3588317610866417 0.37053197119661924 -0.16897336019104503 0.006708286081282766 0.22294499850839905 0.3064633029426777 0.449684771230571 0.29878199021450175 -0.19723891228337898 0.03398494223628351 0.195930874969185; 0.16768892675786945 0.20178877728541117 -0.01920955698773008 -0.07346464258077734 -0.22641700608346715 -0.3972909747656714 -0.3926505747242142 -0.3211783657517294 -0.42299054770886885 0.4394603691078505 -0.020340987431912816 0.31152821754851673 0.275435690977708 -0.453777188734648 0.38653852340376965 0.2257564654780106 -0.39273527136599073 -0.3588904629697215 0.04673991700334451; -0.3922339208951156 0.4104731956148045 0.01849632586826505 -0.2311936494942672 0.05614872575245905 -0.10645646149582344 -0.4578158813458524 0.17582732885781888 0.007762622179870338 0.1398437234045315 -0.2026471964256374 0.3674505083332545 -0.26909992476264555 -0.20785693976235886 0.07734529876859592 0.08917618938717092 -0.22274107976966068 -0.3823399200961872 -0.45505603896994484], [-0.157465439622054, -0.21995520319116818, -0.3384789069755978, -0.15006788522938158, -0.08068739583553858, 0.17488394613124297, -0.04748199633913597, -0.3250052328690887], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.3852959591608028 -0.553059892716297 -0.015028124586988656 -0.011201024404014936 0.005100673463055649 -0.18746389838773353 -0.4465071177773833 -0.24705649867006285; 0.6026146990300689 -0.33870198929549894 0.2718518742522277 -0.3384705743525871 -0.06814271449805787 0.606002135337379 -0.10336455886078566 0.26238572386113823; 0.45018281052830456 0.18679055621241314 -0.37362480422350963 0.006078042783034587 -0.45064052026604706 -0.1425599823506864 -0.3585911486603981 -0.6063601118282695; -0.05738177001216915 -0.3564277181599287 -0.36781688568508414 -0.14252074674595333 0.2571075127666419 0.19183099681926707 -0.018662422915370347 -0.3510293358769623; -0.08061251239900558 0.1179048173797197 -0.25520661838834113 -0.02273079835591796 0.38222026437329126 0.40094107662179646 -0.5807559333068457 0.36662125095206255; 0.33297001370651136 0.33740265915614154 0.11892808620953843 -0.461833943645412 -0.44731582380216783 -0.1810395086037453 0.2447640455667498 0.18200413571170693; -0.48967054357747297 -0.4865518207276439 -0.24790297957249452 -0.45395785625464546 -0.21218635943332892 -0.11260342698590892 -0.38292860160231035 -0.5607840330610061; 0.49172091443161037 0.348025736136519 0.04600440542299833 -0.5245141418067015 -0.16209562928447985 -0.16401386904196824 -0.08199421251258743 0.043141435301436104], [0.5320229402035459, 0.08848618862963409, 0.49077131013907294, 0.40163369167512397, -0.3241193921715959, -0.4400009135782846, 0.07456092888227783, 0.36052644514730536], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.3367271772548713 0.6877326959270641 0.1575891073545942 -0.4441500273567661 0.20426279244526513 -0.08785955388176736 0.09549279041837222 -0.25434472335021696; -0.21366575215568595 0.01925661356946584 0.654401524198115 0.07698019071966455 0.006405545613967667 -0.4771138208850888 -0.7483462850040817 0.09549653596307062], [-0.604516114936473, -0.11748999487867229], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 2, 2, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], â€¦), DecisionTreeClassifier(max_depth = 0, â€¦), KernelPerceptron(kernel = radial_kernel, â€¦), LinearPerceptron(initial_coefficients = nothing, â€¦), Pegasos(initial_coefficients = nothing, â€¦), RandomForestClassifier(n_trees = 30, â€¦), CatBoostClassifier(iterations = 5, â€¦)], [\"nn\", \"dt\", \"kp\", \"lp\", \"peg\", \"rf\", \"cat\"])"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#train-model",
    "href": "category/regression/6-compare of BetalML method.html#train-model",
    "title": "6-compare of BetalML models",
    "section": "4. train model",
    "text": "4. train model\n\n\nCode\nfunction train_model()\n    for (idx,model) in enumerate(models[1:6])\n        local (fitResults, cache, report) = MLJ.fit(model, 0, Xtrain,ytrain);\n        local est_classes= predict_mode(model, fitResults, Xtest)\n        local acc=accuracy(ytest,est_classes)|&gt;d-&gt;round(d, digits=3)\n        @info \"$(models_name[idx])===&gt;$(acc)\"\n    end\nend\n\ntrain_model()\n\n\n[ Info: nn===&gt;0.305\n[ Info: dt===&gt;0.705\n[ Info: kp===&gt;0.335\n[ Info: lp===&gt;0.45\n[ Info: peg===&gt;0.695\n[ Info: rf===&gt;0.735"
  },
  {
    "objectID": "category/regression/7-test-latexify.html",
    "href": "category/regression/7-test-latexify.html",
    "title": "7-latexify-test",
    "section": "",
    "text": "Code\nusing Latexify,Markdown,Symbolics\n\n@variables x\nexpr=latexify(3x^3 + 2x - 5)\n\nMarkdown.parse(\"\"\"$(expr)\"\"\")\n\n\n\\[\\begin{equation} -5 + 2 x + 3 x^{3} \\end{equation}\\]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html",
    "href": "category/regression/1-salary-linear-reg.html",
    "title": "1-salary-linear-reg",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nexplore YearsExperience and Salary relationship\n\n\ndataset: kaggle salary dataset\næ•°æ®ç±»å‹éœ€è¦åšè½¬æ¢: to_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nusing MLJLinearModels.jl ğŸ”—"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#load-package",
    "href": "category/regression/1-salary-linear-reg.html#load-package",
    "title": "1-salary-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#process-data",
    "href": "category/regression/1-salary-linear-reg.html#process-data",
    "title": "1-salary-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\nload(csv)-&gt;dataframe ==&gt;sciencetype ==&gt;MLJ table\n\n\n\n\nCode\ndf=CSV.File(\"./data/salary_dataset.csv\") |&gt; DataFrame |&gt; dropmissing;\nfirst(df,5)\n\n\n5Ã—3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\nto_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nnew_df=to_ScienceType(df)\nfirst(new_df,5)\n\n\n5Ã—3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\n X=MLJ.table(reshape(new_df[:,2],30,1))\n y=Vector(new_df[:,3])\n show(y)\n\n\n[39344.0, 46206.0, 37732.0, 43526.0, 39892.0, 56643.0, 60151.0, 54446.0, 64446.0, 57190.0, 63219.0, 55795.0, 56958.0, 57082.0, 61112.0, 67939.0, 66030.0, 83089.0, 81364.0, 93941.0, 91739.0, 98274.0, 101303.0, 113813.0, 109432.0, 105583.0, 116970.0, 112636.0, 122392.0, 121873.0]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "href": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "title": "1-salary-linear-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 load model\n\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fp=MLJ.fitted_params(mach)  #å­¦ä¹ çš„æ¨¡å‹å‚æ•°\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\nimport MLJLinearModels âœ”\n\n\n(coefs = [:x1 =&gt; 9449.962321455077],\n intercept = 24848.203966523164,)\n\n\n\n\n3.2 build linear function\n\n\nCode\n    a=fp.coefs[1,1][2]\n    b=fp.intercept\n    line_func(t)=a*t+b\n\n\nline_func (generic function with 1 method)"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#plot-results",
    "href": "category/regression/1-salary-linear-reg.html#plot-results",
    "title": "1-salary-linear-reg",
    "section": "4. plot results",
    "text": "4. plot results\n\n\nCode\nxs=range(extrema(new_df[:,2])...,200)\nfig=Figure()\nax=Axis(fig[1,1];xlabel=\"YearsExperience\",ylabel=\"Salary\")\nlines!(ax,xs,line_func.(xs);label=\"fit-line\",linewidth=3)\nscatter!(ax,new_df[:,2],new_df[:,3];label=\"data\",marker_style...)\naxislegend(ax)\nfig"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html",
    "href": "category/regression/5-boston-housing-mixture-regression.html",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nåˆ©ç”¨ Boston houseing å±æ€§é¢„æµ‹æˆ¿ä»·, å˜é‡å¯èƒ½ä¼šå­˜åœ¨äº¤äº’ä½œç”¨\næ‰€ä»¥è€ƒè™‘ä½¿ç”¨æ··åˆæ¨¡å‹"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing MLJ"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nX, y= @load_boston;"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "href": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nmodelType= @load GaussianMixtureRegressor pkg = \"BetaML\"\ngmr= modelType()\n\n(fitResults, cache, report) = MLJ.fit(gmr, 1, X, y);\n\n\nimport BetaML âœ”\nIter. 1:    Var. of the post  21.74887448784977       Log-likelihood -21687.09917379566\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n3.2 results\n\n\nCode\ny_res= predict(gmr, fitResults, X)\nrmse(y_res,y)\n\n\n7.9566567641159605"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html",
    "href": "category/regression/2-ecommerce-linear-reg.html",
    "title": "2-ecommerce-linear-reg",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\né€šè¿‡ä¸Šç½‘æµè§ˆæ—¶é—´é¢„æµ‹å¹´èŠ±è´¹\n\n\ndataset: kaggle ecommerce dataset\nmodel\nusing MLJLinearModels.jl ğŸ”—"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "href": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "title": "2-ecommerce-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing GLMakie, MLJ,CSV,DataFrames,StatsBase\n\n\nWARNING: using StatsBase.predict in module Main conflicts with an existing identifier."
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "href": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "title": "2-ecommerce-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\nstr=\"Ecommerce-Customers\"   \ndf=CSV.File(\"./data/Ecommerce-Customers.csv\") |&gt; DataFrame |&gt; dropmissing;\nselect!(df,4:8)\nX=df[:,1:4]|&gt;Matrix|&gt;MLJ.table\ny=Vector(df[:,5])\nfirst(df,5)\n\n\n5Ã—5 DataFrame\n\n\n\nRow\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n34.4973\n12.6557\n39.5777\n4.08262\n587.951\n\n\n2\n31.9263\n11.1095\n37.269\n2.66403\n392.205\n\n\n3\n33.0009\n11.3303\n37.1106\n4.10454\n487.548\n\n\n4\n34.3056\n13.7175\n36.7213\n3.12018\n581.852\n\n\n5\n33.3307\n12.7952\n37.5367\n4.44631\n599.406"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "title": "2-ecommerce-linear-reg",
    "section": "3. plot corrleation of variables",
    "text": "3. plot corrleation of variables\n\n\nCode\naxs = []\nlabel=names(df)|&gt;Array\ncolors = [:orange, :lightgreen, :purple,:lightblue,:red,:green]\n\nfig = Figure(resolution=(1400, 1400))\nax=Axis(fig[1,1])\n\nfunction plot_diag(i)\n\n    ax = Axis(fig[i, i])\n    push!(axs, ax)\n    density!(ax, df[:, i]; color=(colors[i], 0.5),\n            strokewidth=1.25, strokecolor=colors[i])\nend\n\n\nfunction plot_cor(i, j)\n    ax = Axis(fig[i, j])\n    scatter!(ax, df[:, i], df[:, j]; color=colors[j])\nend\n\n\nfunction plot_pair()\n    [(i == j ? plot_diag(i) : plot_cor(i, j)) for i in 1:5, j in 1:5]\nend\n\nfunction add_xy_label()\n    for i in 1:5\n        Axis(fig[5, i], xlabel=label[i],)\n        Axis(fig[i, 1], ylabel=label[i],)\n    end\nend\n\nfunction main()\n\n    plot_pair()\n    add_xy_label()\n    return fig\nend\n\nmain()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "title": "2-ecommerce-linear-reg",
    "section": "4. plot pair variablesâ€™s cov and cor matrix",
    "text": "4. plot pair variablesâ€™s cov and cor matrix\n\n\nCode\ndf_cov = df|&gt;Matrix|&gt;cov.|&gt; d -&gt; round(d, digits=3)\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=3)\n\nfunction plot_cov_cor()\n    fig = Figure(resolution=(2200, 800))\n    ax1 = Axis(fig[1, 1]; xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cov matrix\",yreversed=true)\n    ax3 = Axis(fig[1, 3], xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cor matrix\",yreversed=true)\n\n    hm = heatmap!(ax1, df_cov)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cov[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    hm2 = heatmap!(ax3, df_cor)\n    Colorbar(fig[1, 4], hm2)\n    [text!(ax3, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    fig\nend\n\nplot_cov_cor()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "href": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "title": "2-ecommerce-linear-reg",
    "section": "5. MLJ workflow",
    "text": "5. MLJ workflow\n\n5.1 load model\n\n\nCode\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fitted_params(mach)\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\nimport MLJLinearModels âœ”\n\n\n(coefs = [:x1 =&gt; 25.734271084705085, :x2 =&gt; 38.709153810834366, :x3 =&gt; 0.43673883559434407, :x4 =&gt; 61.57732375487839],\n intercept = -1051.5942553006273,)\n\n\n\n\n5.2 predict\n\n\nCode\n  y_hat =predict(mach, X)\n  \"rmsd\"=&gt;rmsd(y,y_hat)\n\n\n\"rmsd\" =&gt; 9.923256785022247\n\n\n\n\n5.3 plot residuals\n\n\nCode\nresid=y_hat.=y\nstem(resid)"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html",
    "href": "category/regression/9-poisson-reg.html",
    "title": "9-poisson-reg",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\næ³Šæ¾å›å½’(Poisson Regression) æ˜¯ä¸€ç±»ç‰¹æ®Šçš„å›å½’æ¨¡å‹,ç›¸åº”å˜é‡æ˜¯è®¡æ•°æ•°æ®(ç¦»æ•£æ­£æ•´æ•°) å“åº”å˜é‡çš„åˆ†å¸ƒéµå¾ªæ³Šæ¾åˆ†å¸ƒ\ndataset data\næ•°æ®é›†æœ‰ä¸¤ä¸ªå˜é‡, é¢„æµ‹å˜é‡ä¸ºæ•°å­¦æˆç»©(Math Score),å“åº”å˜é‡ä¸ºå¥–å­¦é‡‘ç­‰çº§(0-6)"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html#load-package",
    "href": "category/regression/9-poisson-reg.html#load-package",
    "title": "9-poisson-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params,coerce\n    using GLMakie,MLJ,CSV,DataFrames,ScientificTypes"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html#load-data",
    "href": "category/regression/9-poisson-reg.html#load-data",
    "title": "9-poisson-reg",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nto_ScienceType(d)=coerce(d,:Awards=&gt; Multiclass,:MathScore=&gt;Continuous)\ndf=CSV.File(\"./data/competition_awards_data.csv\") |&gt; DataFrame|&gt;dropmissing\n \n X=MLJ.table(reshape(df[:,2],200,1))\n y=Vector(df[:,1])\n (Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\n first(df,10)\n\n\n10Ã—2 DataFrame\n\n\n\nRow\nAwards\nMathScore\n\n\n\nInt64\nInt64\n\n\n\n\n1\n0\n43\n\n\n2\n0\n38\n\n\n3\n0\n41\n\n\n4\n0\n33\n\n\n5\n0\n39\n\n\n6\n0\n43\n\n\n7\n0\n35\n\n\n8\n0\n41\n\n\n9\n0\n36\n\n\n10\n0\n38"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html#mlj-workflow",
    "href": "category/regression/9-poisson-reg.html#mlj-workflow",
    "title": "9-poisson-reg",
    "section": "3. MLJ Workflow",
    "text": "3. MLJ Workflow\n\n3.1 load model\n\n\nCode\n    CountRegressor = @load LinearCountRegressor pkg=GLM\n    model = CountRegressor(fit_intercept=false)\n    mach = machine(model, Xtrain, ytrain)\n    fit!(mach)\n\n\nimport MLJGLMInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc GLM.LinearCountRegressor` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{AbstractVector{Count}}, AbstractVector{Count}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Union{Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Count}}, Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Count}, AbstractVector{&lt;:Union{Continuous, Count}}}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearCountRegressor(fit_intercept = false, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: LinearCountRegressor(fit_intercept = false, â€¦)\n  args: \n    1:  Source @955 â Table{AbstractVector{Count}}\n    2:  Source @555 â AbstractVector{Count}\n\n\n\n\n3.2 predict model results\n\n\nCode\n yhat=predict_mode(mach, Xtest)|&gt;Array\n @info \"rms\"=&gt;rms(yhat,ytest)\n\n report(mach)\n\n\n[ Info: \"rms\" =&gt; 0.9486832980505138\n\n\n(stderror = [0.0013746169531615926],\n dof_residual = 160.0,\n vcov = [1.88957176791926e-6;;],\n deviance = 254.53389416397937,\n coef_table = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n         Coef.  Std. Error     z  Pr(&gt;|z|)    Lower 95%   Upper 95%\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nx1  0.00104856  0.00137462  0.76    0.4456  -0.00164564  0.00374276\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€,)"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html",
    "href": "category/regression/3-cricket-chirp-rate.html",
    "title": "3-cricket-chirp-rate",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nsource1\nsource2\nsourc3\n\n\né›ªæ ‘èŸ‹èŸ€çš„é¸£å«å®é™…æ˜¯å¤§è…¿æ‘©æ“¦å‘å‡ºçš„å£°éŸ³, ç»è¿‡æ•°æ®æ”¶é›†,å‘ç°é¸£å«çš„é¢‘ç‡å’Œç¯å¢ƒæ¸©åº¦æ­£ç›¸å…³.\n\nç»è¿‡çº¿æ€§æ‹Ÿåˆå¾—åˆ°çš„å‡½æ•°ä¸º:C(t)=4.25t-157.8"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "href": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "title": "3-cricket-chirp-rate",
    "section": "1. load pacakge",
    "text": "1. load pacakge\n\n\nCode\nimport FileIO:load\nimport MLJ:fit!,match,predict,table,fitted_params\nusing GLMakie, CSV,DataFrames,MLJ,FileIO\nimg=load(\"./data/snowy-cricket.jpg\");"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#process-data",
    "href": "category/regression/3-cricket-chirp-rate.html#process-data",
    "title": "3-cricket-chirp-rate",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\ndf=CSV.File(\"./data/CricketChirps.csv\") |&gt; DataFrame |&gt; dropmissing;\nX=MLJ.table(reshape(df[:,1],7,1))\ny=Vector(df[:,2])\n\ntest_X=range(extrema(df[:,1])...,50)\ntest_X=MLJ.table(reshape(test_X,50,1))\ncols=names(df)\n\n\n2-element Vector{String}:\n \"Temperature\"\n \"Chirps\""
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "href": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "title": "3-cricket-chirp-rate",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n    mach = fit!(machine(LinearRegressor(), X, y))\n    report(mach)\n\n\n[ Info: For silent loading, specify `verbosity=0`. \nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LinearRegressor` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{AbstractVector{Continuous}}, AbstractVector{Count}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Continuous}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\nimport MLJLinearModels âœ”\n\n\n\n\n3.2 plot fitting curve\n\n\nCode\nyhat=predict(mach,test_X).|&gt;(d-&gt;round(d,digits=2))\nfunction plot_fitting_curve(df,yhat)\n    X=df[:,1]\n    test_X=range(extrema(df[:,1])...,50)\n    cols=names(df)\n    fig=Figure()\n    ax=Axis(fig[1:3,1:3];xlabel=\"$(cols[1])\",ylabel=\"$(cols[2])\",title=\"cricket-chirp\")\n    ax2 = Axis(fig[2,4],title=\"snowy-tree-cricket\")\n    scatter!(ax, X,y,markersize=16,color=(:red,0.8))\n    lines!(ax, test_X,yhat,color=:blue)\n    image!(ax2,img)\n    hidespines!(ax2)\n    hidedecorations!(ax2)\n    fig\nend\nplot_fitting_curve(df,yhat)"
  },
  {
    "objectID": "category/materials.html",
    "href": "category/materials.html",
    "title": "dataset list",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html",
    "href": "category/classification/24-classfication-comparison.html",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#load-package",
    "href": "category/classification/24-classfication-comparison.html#load-package",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#make-data",
    "href": "category/classification/24-classfication-comparison.html#make-data",
    "title": "several classfication model comparison",
    "section": "2. make data",
    "text": "2. make data\n\n\nCode\n    function circle_data()\n    X, y = make_circles(400; noise=0.1, factor=0.3)\n    df = DataFrame(X)\n    df.y = y\n    return df\n    end\n    function moons_data()\n        X, y = make_moons(400; noise=0.1)\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    function blob_data()\n        X, y = make_blobs(400, 2; centers=2, cluster_std=[1.0, 2.0])\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    #cat=df1.y|&gt;levels|&gt;unique\n    colors=[:green, :purple]\n\n\n2-element Vector{Symbol}:\n :green\n :purple"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-function",
    "href": "category/classification/24-classfication-comparison.html#define-function",
    "title": "several classfication model comparison",
    "section": "3. define function",
    "text": "3. define function\n\n\nCode\nfunction plot_origin_data(df)\n    fig=Figure()\n    ax=Axis(fig[1,1])\n    local cat=df.y|&gt;levels|&gt;unique\n    \n    local colors=[:green, :purple]\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        #@show d\n    end\n    fig\nend\n\nnums=100\nfunction boundary_data(df,;n=nums)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = LinRange(xlow,xhigh,n1)\n    ty = LinRange(ylow,yhigh,n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    x_test=MLJ.table(x_test')\n    return tx,ty,x_test\nend\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n        tx,ty,xs,ys, xtest=boundary_data(df)\n        local ax=Axis(fig[row,i],title=\"$(names[i])\")\n\n        contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:phase)\n\n        for (i,c) in enumerate(cat)\n            d=df[y.==c,:]\n            scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        end\n        hidedecorations!(ax)\nend\n\n\nplot_desc_boudary (generic function with 1 method)"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "href": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "title": "several classfication model comparison",
    "section": "4. define machine learning models",
    "text": "4. define machine learning models\n\n\nCode\n    using CatBoost.MLJCatBoostInterface\n    SVC = @load SVC pkg=LIBSVM   \n    KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n    DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n    RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n    CatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n    BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n    Booster = @load AdaBoostStumpClassifier pkg=DecisionTree\n    \n    models=[KNNClassifier,DecisionTreeClassifier,RandomForestClassifier,CatBoostClassifier,BayesianLDA,SVC]\n    names=[\"KNN\",\"DecisionTree\",\"RandomForest\",\"CatBoost\",\"BayesianLDA\",\"SVC\"]\n   function _fit(df::DataFrame,m)\n    X,y=df[:,1:2],df[:,3]\n    _,_,xtest=boundary_data(df;n=nums)\n    local predict= m==MLJLIBSVMInterface.SVC  ? MLJ.predict : MLJ.predict_mode \n    model=m()\n   mach = machine(model, X, y)|&gt;fit!\n   yhat=predict(mach, xtest)\n   ytest=yhat|&gt;Array|&gt;d-&gt;reshape(d,nums,nums)\n   return  ytest\nend\n\n\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n    tx,ty,_=boundary_data(df)\n    local y=df.y\n    local ax=Axis(fig[row,i],title=\"$(names[i])\")\n    cat=y|&gt;levels|&gt;unique\n    contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:redsblues)\n\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n    end\n    hidedecorations!(ax)\n    \n\nend\n\nfunction plot_comparsion(testdata,df;row=1)\n    \n    for (i,data) in enumerate(testdata)\n        plot_desc_boudary(fig,data,i;df=df,row=row)\n    end\n    fig\nend\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJLIBSVMInterface âœ”\nimport NearestNeighborModels âœ”\nimport MLJDecisionTreeInterface âœ”\nimport MLJDecisionTreeInterface âœ”\nimport CatBoost âœ”\nimport MLJMultivariateStatsInterface âœ”\nimport MLJDecisionTreeInterface âœ”\n\n\nplot_comparsion (generic function with 1 method)\n\n\n\n\nCode\nfig=Figure(resolution=(2100,1000))\nfunction plot_comparsion(testdata,df,row=1)\n    \n    for i in eachindex(testdata)\n        plot_desc_boudary(fig,testdata[i],i;df=df,row=row)\n    end\n    fig\nend\n\n\n\ndf1=circle_data()\n\nytest1=[_fit(df1,m) for (i,m) in enumerate(models)]\n\ndf2=moons_data()\nytest2=[_fit(df2,m) for (i,m) in enumerate(models)]\n\ndf3=blob_data()\nytest3=[_fit(df3,m) for (i,m) in enumerate(models)]\n\ndfs=[df2,df1,df3]\nytests=[ytest2,ytest1,ytest3]\n\nfig=Figure(resolution=(2100,1000))\n\nfor (df, data,i)  in zip(dfs,ytests,[1,2,3])\n    plot_comparsion(data,df;row=i)\nend\n\nfig\n\n\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦)."
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html",
    "title": "1-nci60-pca-clustering-svm",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\nå‚è§ [ISLR-nci60]:An Introduction to Statistical Learning.pdf page 18\næµç¨‹ä¸º:pca-&gt;clustering-&gt;svm åŠç›‘ç£å­¦ä¹ æ–¹æ³•,é¦–å…ˆå¯¹æ•°æ®é™ç»´, ç„¶åèšç±», æœ€åä½¿ç”¨ SVM è¿›è¡Œåˆ†ç±»å­¦ä¹ "
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html#load-package",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html#load-package",
    "title": "1-nci60-pca-clustering-svm",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:transform,predict\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie,Random\nRandom.seed!(45454)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html#import-data",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html#import-data",
    "title": "1-nci60-pca-clustering-svm",
    "section": "2. import data",
    "text": "2. import data\n\n\nCode\n    df= CSV.File(\"./data/NCI60.csv\") |&gt; DataFrame |&gt; dropmissing\n    Xtr = df[:,2:end]\n    Xtr_labels = Vector(df[:,1])\n    # # split other half to testing set\n    Xte=df[1:3:end,2:end]\n    Xte_labels = Vector(df[1:3:end,1])\n    first(df,10)\n\n\n10Ã—6831 DataFrame6731 columns omitted\n\n\n\nRow\nColumn1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\nâ‹¯\n\n\n\nString3\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nâ‹¯\n\n\n\n\n1\nV1\n0.3\n1.18\n0.55\n1.14\n-0.265\n-0.07\n0.35\n-0.315\n-0.45\n-0.65498\n-0.65\n-0.94\n0.31\n0.0150098\n-0.08\n-2.37\n-0.54\n-0.615\n0.0\n-0.51999\n-0.37\n-0.29\n-0.17499\n0.07\n-0.04\n0.025\n-0.74\n-0.47999\n-0.45\n-0.93\n0.16\n-0.55\n-0.55001\n0.055\n-0.37\n-0.165\n0.21\n0.47\n0.0\n-2.60208e-18\n0.139981\n-0.215\n-0.065\n-0.225\n-0.35\n-1.335\n0.0\n0.2175\n0.25\n0.13\n-0.48\n-0.42\n-0.7\n-0.275\n-0.34499\n-0.16\n-0.35\n0.555\n0.29\n-0.27\n-0.339981\n0.305\n-0.005\n0.7\n0.45002\n0.21\n0.29\n0.0849902\n-0.45501\n0.12\n-0.66\n0.1\n0.1\n-0.099961\n-0.399981\n-0.195\n0.28\n2.36\n0.47\n0.18\n-0.64499\n1.3\n0.0\n-0.48\n0.595\n-0.0599805\n0.055\n0.0975\n0.4\n0.28\n0.76\n1.425\n-0.51\n0.94\n0.94\n0.68\n-0.21\n-1.19\n0.0\nâ‹¯\n\n\n2\nV2\n0.679961\n1.28996\n0.169961\n0.379961\n0.464961\n0.579961\n0.699961\n0.724961\n-0.040039\n-0.285019\n-0.310039\n-0.720039\n-0.010039\n0.0\n-0.570039\n0.0\n-0.470039\n-0.355039\n0.00498051\n-0.480029\n-0.140039\n-0.090039\n0.00497074\n-0.220039\n-0.370039\n0.0\n-0.320039\n0.159971\n0.179961\n-0.320039\n-0.440039\n0.349961\n0.449951\n0.104961\n0.489961\n0.204961\n-0.050039\n-0.010039\n0.269961\n0.019961\n0.0499415\n-0.315039\n-0.325039\n-0.055039\n-0.280039\n-0.255039\n0.229961\n-0.342539\n-0.560039\n-0.900039\n-0.060039\n-0.200039\n-0.670039\n0.324961\n0.134971\n0.539961\n0.229961\n0.084961\n-0.080039\n0.949961\n0.93998\n0.194961\n1.90496\n0.499961\n0.349981\n0.899961\n1.21996\n0.0\n0.374951\n0.279961\n-3.89862e-5\n-0.090039\n-0.050039\n0.0\n0.90998\n0.274961\n-0.040039\n0.869961\n-0.100039\n1.40996\n1.00497\n0.779961\n-0.110039\n-0.350039\n-0.215039\n-0.0600195\n0.324961\n0.267461\n0.129961\n0.229961\n0.079961\n0.514961\n-0.420039\n-0.350039\n-0.790039\n-0.290039\n-0.010039\n-1.05004\n-2.04004\nâ‹¯\n\n\n3\nV3\n0.94\n-0.04\n-0.17\n-0.04\n-0.605\n0.0\n0.09\n0.645\n0.43\n0.475019\n0.41\n0.13\n-0.35\n0.0\n0.0\n0.0\n-0.8\n0.0\n-0.00498051\n0.0\n-0.14\n0.05\n-0.0649903\n-0.06\n0.29\n0.715\n-0.07\n-0.0899903\n-0.31\n0.58\n-0.48\n0.23\n-0.0400098\n-0.935\n-0.75\n-0.385\n-0.34\n0.12\n-0.47\n0.17\n-0.86002\n-0.175\n-0.715\n-0.965\n-0.54\n-0.005\n-0.06\n-0.7225\n-0.92\n0.47\n0.7\n0.67\n-0.9\n-0.265\n-0.42499\n-0.24\n-0.03\n0.215\n0.29\n0.07\n0.12002\n0.515\n0.545\n-0.03\n0.19002\n-0.07\n0.4\n0.0\n-0.0950098\n0.27\n0.08\n0.0\n0.0\n-0.319961\n0.0900195\n0.105\n-0.28\n1.99\n0.0\n0.87\n0.0\n0.74\n0.0\n-1.2\n-0.335\n0.630019\n0.345\n0.6975\n0.27\n-0.02\n0.0\n-0.115\n-0.44\n0.54\n0.49\n0.64\n0.66\n0.0\n0.0\nâ‹¯\n\n\n4\nV4\n0.28\n-0.31\n0.68\n-0.81\n0.625\n-1.38778e-17\n0.17\n0.245\n0.02\n0.0950195\n-0.01\n-0.12\n-0.21\n0.0\n0.61\n-1.02\n-0.47\n0.0\n-0.76498\n0.0\n-0.31\n-0.62\n-0.28499\n-0.54\n-0.52\n-0.135\n-0.89\n-0.26999\n-0.84\n-0.23\n0.32\n0.0\n0.10999\n0.455\n-0.34\n-0.895\n-1.08\n-0.43\n-0.03\n-0.13\n-0.540019\n-1.225\n-1.265\n-1.415\n-0.27\n-0.705\n-0.22\n-0.5025\n-0.04\n-0.15\n-0.16\n-0.29\n-0.18\n-0.665\n-0.77499\n0.21\n-0.77\n-0.605\n-0.19\n0.17\n-0.20998\n-0.615\n0.165\n-0.19\n0.0\n-0.06\n-0.01\n-0.50501\n-0.77501\n-0.29\n-0.71\n-0.45\n-0.61\n-0.809961\n-1.53998\n-1.035\n0.0\n3.6\n0.0\n0.85\n-0.19499\n-1.21\n1.02\n0.66\n0.775\n-0.279981\n-0.245\n-0.5025\n-0.8\n-0.75\n0.06\n-1.075\n0.54\n0.16\n0.0\n0.23\n-0.74\n0.0\n-2.5\nâ‹¯\n\n\n5\nV5\n0.485\n-0.465\n0.395\n0.905\n0.2\n-0.005\n0.085\n0.11\n0.235\n1.49002\n0.685\n0.605\n0.355\n1.22001\n2.425\n0.0\n-0.315\n0.31\n-0.519981\n-0.0749902\n-0.865\n-0.455\n-0.49999\n-0.245\n-0.235\n-0.33\n0.0\n0.0150097\n-0.105\n-0.225\n-0.105\n-0.275\n-0.57501\n-0.45\n-0.465\n-0.39\n-0.995\n-0.355\n0.0\n-0.475\n-0.38502\n-0.77\n-0.96\n-0.97\n-0.895\n-0.63\n-0.535\n-0.8875\n-0.945\n-0.535\n0.005\n0.185\n-0.105\n-0.34\n-0.19999\n-0.665\n-0.225\n-0.41\n-0.215\n-0.175\n-0.784981\n-0.33\n-0.39\n-0.075\n-0.20498\n-0.325\n-0.485\n-0.35001\n-0.23001\n-0.155\n-0.575\n-0.615\n-0.695\n-0.634961\n-1.09498\n-1.0\n-0.335\n-1.385\n0.345\n0.815\n0.56001\n-0.155\n0.0\n-1.195\n-0.16\n-0.10498\n0.0\n-0.0075\n-0.945\n-0.965\n-0.225\n-0.46\n0.045\n0.795\n1.305\n0.705\n0.055\n0.715\n0.925\nâ‹¯\n\n\n6\nV6\n0.31\n-0.03\n-0.1\n-0.46\n-0.205\n-0.54\n-0.64\n-0.585\n-0.77\n-0.24498\n-0.12\n0.0\n-0.69\n-0.73499\n-0.67\n-0.05\n0.09\n-0.805\n0.59502\n-0.42999\n-0.85\n-0.09\n-0.0149903\n0.0\n0.15\n0.805\n-0.7\n0.36001\n-0.16\n0.04\n-0.17\n0.09\n0.0599902\n-0.635\n-0.51\n-0.585\n0.72\n-0.17\n-0.55\n0.21\n-0.13002\n0.125\n-0.415\n-0.475\n-0.02\n-0.405\n-0.4\n0.0475\n0.22\n-1.4\n-0.65\n-0.65\n-0.15\n-0.475\n-0.51499\n0.0\n-0.35\n-0.755\n-0.54\n-1.16\n-1.70998\n-0.415\n-1.275\n-0.89\n-0.269981\n-1.18\n-1.89\n-1.93501\n-2.56501\n-2.5\n-1.01\n-0.75\n-0.85\n-0.589961\n0.0\n-0.085\n0.18\n1.37\n-0.15\n-2.84\n0.0\n-1.74\n-1.38\n-2.62\n-0.715\n-0.819981\n-0.545\n-0.5525\n-0.76\n-1.02\n-1.71\n-0.495\n-0.76\n-0.28\n-0.39\n-0.5\n-0.53\n-0.85\n-0.38\nâ‹¯\n\n\n7\nV7\n-0.83\n0.0\n0.13\n-1.63\n0.075\n-0.36\n0.1\n0.155\n-0.29\n-0.0849805\n-0.01\n-0.24\n-0.44\n0.0\n0.0\n0.0\n-0.38\n0.495\n0.0\n0.43001\n0.31\n0.09\n-0.30499\n-0.24\n-0.58\n0.445\n-0.28\n0.37001\n0.0\n-0.27\n-0.12\n0.05\n0.00999023\n-0.195\n-0.86\n-0.405\n-0.6\n-0.27\n-0.54\n0.19\n-0.0100195\n-0.465\n-1.375\n-0.945\n-0.31\n-0.435\n-0.1\n-0.8725\n-0.74\n-2.66\n-2.02\n-1.52\n-1.33\n-0.555\n-0.84499\n-0.76\n-0.84\n-0.735\n-0.37\n-0.95\n-1.14998\n-1.185\n-1.485\n-0.58\n-0.0299805\n-1.26\n-1.25\n-0.0450098\n-1.08501\n-0.08\n-0.44\n-0.1\n-0.28\n-0.269961\n0.0900195\n-0.335\n-0.17\n-0.48\n0.05\n-1.12\n-0.58499\n-1.93\n0.0\n-1.29\n-0.875\n-1.62998\n-0.755\n-0.8325\n-0.73\n-0.47\n-0.69\n-1.145\n-1.6\n-0.58\n-1.39\n-0.62\n-0.15\n-0.79\n-0.83\nâ‹¯\n\n\n8\nV8\n-0.19\n-0.87\n-0.45\n0.08\n0.005\n0.35\n-0.04\n-0.265\n-0.31\n-0.24498\n-0.91\n-1.23\n-0.52\n-0.26499\n-0.48\n-1.4\n-0.19\n0.125\n-0.51498\n-0.83999\n-0.72\n-0.32\n-0.59499\n-0.34\n-0.82\n0.0\n-1.18\n-0.32999\n-0.69\n-1.67\n1.3\n0.0\n-0.43001\n-0.685\n-1.55\n-0.955\n-0.74\n-0.2\n-0.58\n0.01\n-0.44002\n-0.055\n-0.565\n-0.795\n-0.8\n-0.345\n-0.37\n-0.2725\n-0.71\n-0.21\n-0.32\n-0.3\n-0.92\n-0.115\n0.0150098\n0.13\n0.13\n0.475\n-0.57\n-0.07\n-0.389981\n-0.185\n-1.055\n0.15\n-0.18998\n0.84\n0.84\n0.73499\n0.18499\n0.21\n-0.48\n-0.28\n0.12\n0.100039\n-0.389981\n-0.715\n0.14\n-0.82\n-0.47\n-1.64\n0.0\n0.0\n0.0\n-0.4\n-0.335\n0.11002\n-0.125\n0.0675\n-0.77\n-0.65\n-0.43\n1.215\n0.4\n-0.7\n-1.79\n-1.13\n-0.07\n0.53\n0.21\nâ‹¯\n\n\n9\nV9\n0.46\n0.0\n1.15\n-1.4\n-0.005\n-0.7\n-0.92\n-0.515\n-0.28\n-0.11498\n-0.17\n-0.4\n-0.52\n-0.23499\n2.21\n-2.44\n0.26\n0.155\n0.135019\n-0.90999\n-0.13\n-0.4\n-0.29499\n0.16\n-0.23\n-0.315\n0.51\n0.20001\n-0.19\n0.09\n-0.85\n-0.78\n-1.05001\n0.005\n-1.06\n-0.425\n-0.76\n-0.27\n-0.96\n-1.26\n-1.05002\n-0.835\n-1.705\n-1.195\n-1.24\n-1.065\n0.04\n-0.0225\n-0.48\n-0.46\n0.22\n0.46\n0.56\n0.335\n0.35501\n0.39\n0.18\n-0.055\n-0.11\n0.46\n0.46002\n-0.005\n0.445\n-0.52\n-0.339981\n-0.01\n-0.55\n0.17499\n-0.41501\n-0.18\n0.05\n3.44234e-18\n0.0\n-0.249961\n0.360019\n0.195\n-0.3\n-0.77\n-0.37\n-0.34\n-0.45499\n-0.82\n0.0\n-0.7\n0.145\n0.0200195\n-0.155\n-0.1825\n0.74\n0.52\n0.05\n-0.105\n-0.54\n0.0\n0.53\n0.21\n0.0\n0.0\n1.36\nâ‹¯\n\n\n10\nV10\n0.76\n1.49\n0.28\n0.1\n-0.525\n0.36\n0.6\n0.175\n0.58\n1.14502\n1.75\n1.71\n0.51\n0.66501\n-0.1\n-0.24\n-0.69\n-0.115\n0.235019\n0.89001\n0.3\n0.08\n0.12501\n0.32\n1.28\n0.895\n1.35\n0.42001\n0.37\n0.45\n-0.25\n-0.01\n-0.27001\n-0.455\n0.61\n0.615\n0.41\n0.2\n0.13\n0.7\n-0.12002\n-0.135\n-0.235\n0.475\n0.04\n-0.295\n1.05\n1.1275\n0.74\n-0.26\n-0.09\n0.15\n-0.09\n-0.265\n0.15501\n-0.23\n0.25\n0.095\n0.36\n-0.08\n-0.12998\n0.205\n0.985\n0.42\n0.32002\n0.25\n0.19\n0.21499\n0.0449902\n-0.2\n-0.31\n0.0\n-0.41\n-0.00996101\n0.19002\n0.095\n0.0\n1.94\n0.14\n-0.18\n-0.58499\n-0.42\n-0.38\n-1.52\n-0.745\n-0.55998\n-0.285\n-0.2025\n-0.05\n0.35\n0.03\n0.755\n0.8\n1.52\n2.11\n0.91\n-0.25\n-0.56\n-0.77\nâ‹¯"
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html#mlj-workflow",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html#mlj-workflow",
    "title": "1-nci60-pca-clustering-svm",
    "section": "2. MLJ WorkFlow",
    "text": "2. MLJ WorkFlow\n\n2.1 define models\néœ€è¦å®šä¹‰ä¸‰ä¸ªæ¨¡å‹:\n\npca model\nclustering model\nclassficiation model\n\n\n\nCode\n PCA = @load PCA pkg=MultivariateStats\n KMeans = @load KMeans pkg=Clustering\n SVC = @load SVC pkg=LIBSVM\n\n model=PCA(maxoutdim=2) # pca model\n model2 = KMeans(k=3)   # clustering model\n model3 = SVC()        # svm dodel\n\n\nimport MLJMultivariateStatsInterface âœ”\nimport MLJClusteringInterface âœ”\nimport MLJLIBSVMInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nSVC(\n  kernel = LIBSVM.Kernel.RadialBasis, \n  gamma = 0.0, \n  cost = 1.0, \n  cachesize = 200.0, \n  degree = 3, \n  coef0 = 0.0, \n  tolerance = 0.001, \n  shrinking = true)\n\n\n\n\n2.2 PCA\nåœ¨ PCA æµç¨‹ä¸­è¦å®Œæˆä¸¤æ­¥:\n\nPCA æ¨¡å‹è®­ç»ƒ(å¦‚æœä¸ºäº†ä¾¿äºå¯è§†åŒ–, ç»´åº¦ä¸º 2æˆ–è€… 3)\nå°†åŸå§‹æ•°æ®æ˜ å°„åˆ°é™ç»´çš„ç©ºé—´ä¸Š\n\n\n\nCode\nmach = machine(model, Xtr) |&gt; fit!\nXproj =transform(mach, Xtr)\nfirst(Xproj,10)\n\n\n[ Info: Training machine(PCA(maxoutdim = 2, â€¦), â€¦).\n\n\n10Ã—2 DataFrame\n\n\n\nRow\nx1\nx2\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n-19.7958\n0.115269\n\n\n2\n-21.5461\n-1.45735\n\n\n3\n-25.0566\n1.52609\n\n\n4\n-37.4095\n-11.3895\n\n\n5\n-50.2186\n-1.34617\n\n\n6\n-26.4352\n0.462982\n\n\n7\n-27.3393\n2.65031\n\n\n8\n-21.4897\n4.95414\n\n\n9\n-20.8525\n10.1631\n\n\n10\n-26.9529\n21.4733\n\n\n\n\n\n\n\n\n2.3 ç”Ÿæˆå†³ç­–è¾¹ç•Œæµ‹è¯•æ•°æ®é›†\n\n\nCode\nfunction boundary_data(df,;n=200)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = range(xlow,xhigh; length=n1)\n    ty = range(ylow,yhigh; length=n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    xtest=MLJ.table(x_test')\n    return tx,ty, xtest\nend\ntx,ty, xtest=boundary_data(Xproj)  #xtest  ç”Ÿæˆå†³ç­–è¾¹ç•Œçš„æ•°æ®\n\n\n(-50.21864152783379:0.5383986285981281:56.9226855631937, -51.362711708066826:0.3660101806028475:21.473314231899835, Tables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64)\n\n\n\n\n2.5 Clustering and SVM training\n\n\nCode\n mach2= machine(model2, Xproj) |&gt; fit!\n yhat = predict(mach2, Xproj)  # èšç±»ç»“æœ\n cat=yhat|&gt;Array|&gt;levels\n\n mach3 = machine(model3, Xproj, yhat)|&gt;fit!\n ypred=predict(mach3, xtest)|&gt;Array|&gt;d-&gt;reshape(d,200,200) #SVM ç»“æœ\n\n\n[ Info: Training machine(KMeans(k = 3, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n\n\n200Ã—200 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n â‹®              â‹®              â‹®        â‹±        â‹®              â‹®           \n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3  â€¦  3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3  â€¦  3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n\n\n\n\n2.6 plot results\n\n\nCode\n    function plot_model()\n        fig = Figure()\n        ax = Axis(fig[1, 1],title=\"NCI60 Machine Learning\",subtitle=\"pca-&gt;clustering-&gt;svm\")\n\n        colors = [:red, :orange, :blue]\n        contourf!(ax, tx,ty,ypred)\n        for (i, c) in enumerate(Array(yhat))\n            data = Xproj[i, :]\n            \n            scatter!(ax, data.x1, data.x2;marker=:circle,markersize=12,color=(colors[c],0.3),strokewidth=1,strokecolor=:black)\n            text!(ax,data.x1, data.x2;text=\"v$(i)\")\n        end\n\n        fig\n        #save(\"NCI60 Machine Learning:pca-&gt;clustering-&gt;svm-with-tag.png\",fig)\n    end\n\n    plot_model()"
  },
  {
    "objectID": "category/schedule.html",
    "href": "category/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "category/dataset/index.html",
    "href": "category/dataset/index.html",
    "title": "dataset index list",
    "section": "",
    "text": "â€œintor of datasetâ€\n1"
  }
]