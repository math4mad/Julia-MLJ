[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia MLJ",
    "section": "",
    "text": "Course info & details here"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html",
    "href": "category/classification/2-diabetes-svm-classficiation.html",
    "title": "2-svm-diabetes-classfication",
    "section": "",
    "text": "ä»‹ç»\n\n\n\n\nå‚è€ƒåšå®¢æ–‡ç« :diagnose-diabetes-with-svm\nSVM(æ”¯æŒå‘é‡æœº)é€šè¿‡å¼•å…¥ kernelfunction,ä½¿å¾—æ¨¡å‹çš„åˆ†ç±»çµæ´»æ€§å¤§å¤§å¢å¼º,å¯ä»¥è§£å†³æ›´å¤šé—®é¢˜.åœ¨juliaä¸­å¯ä»¥é€šè¿‡åœ¨LIBSVM.jl å¼•å…¥ kernel function å®ç°, å‚è§ æ–‡æ¡£: Support Vector Machine\nMLJ.jl é€šè¿‡åŒ…è£…æ¥å£ä¹Ÿæä¾›ç›¸ä¼¼åŠŸèƒ½\nå“åº”å˜é‡éœ€è¦è½¬æ¢ç±»å‹ to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "href": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "title": "2-svm-diabetes-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ: fit!, predict\nusing CSV,DataFrames,Random\nusing MLJ\nusing Plots\nusing KernelFunctions"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "href": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "title": "2-svm-diabetes-classfication",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\n df=load_csv(\"diabetes\")\n to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)\n df=to_ScienceType(df)\n first(df,5)|&gt;display\n y, X =  unpack(df, ==(:Outcome), rng=123);\n (Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.7, multi=true,  rng=123)\ndisplay(schema(X))\n\n\n5Ã—9 DataFrame\n\n\n\nRow\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nCatâ€¦\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names                    â”‚ scitypes   â”‚ types   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Pregnancies              â”‚ Count      â”‚ Int64   â”‚\nâ”‚ Glucose                  â”‚ Count      â”‚ Int64   â”‚\nâ”‚ BloodPressure            â”‚ Count      â”‚ Int64   â”‚\nâ”‚ SkinThickness            â”‚ Count      â”‚ Int64   â”‚\nâ”‚ Insulin                  â”‚ Count      â”‚ Int64   â”‚\nâ”‚ BMI                      â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ DiabetesPedigreeFunction â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ Age                      â”‚ Count      â”‚ Int64   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "href": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "title": "2-svm-diabetes-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 defin model\n\n\nCode\nSVC = @load SVC pkg=LIBSVM\n#define kernel function,evaulate  kernelfunctions methods\nkernels=[PolynomialKernel(; degree=2, c=1), \n         SqExponentialKernel(),\n         NeuralNetworkKernel(),\n         LinearKernel(;c=1.0)\n]\n\nsvc_mdls = [SVC(;kernel=k) for  k in kernels]\nsvcs = [machine(model, Xtrain, ytrain;scitype_check_level=0) for model in svc_mdls]\n[fit!(svc) for svc in svcs]\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), â€¦), â€¦).\n\nWARNING: reaching max number of iterations\n[ Info: Training machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = Neural Network Kernel, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = Linear Kernel (c = 1.0), â€¦), â€¦).\n\nWARNING: reaching max number of iterations\n\n\nimport MLJLIBSVMInterface âœ”\n\n\n4-element Vector{Machine{MLJLIBSVMInterface.SVC, true}}:\n machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), â€¦), â€¦)\n machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), â€¦), â€¦)\n machine(SVC(kernel = Neural Network Kernel, â€¦), â€¦)\n machine(SVC(kernel = Linear Kernel (c = 1.0), â€¦), â€¦)\n\n\n\n\n3.2 predict test\n\n\nCode\nfor (idx, str) in enumerate([\"Polynomial \",\"Gaussian\",\"NeuralNetwork\",\"Linear\"])\n    local yhat=predict(svcs[idx],Xtest)\n    local acc=accuracy(yhat,ytest) \n    @info \"$(str) kernel predict accuracy\"=&gt;acc   \nend\n\n\n[ Info: \"Polynomial  kernel predict accuracy\" =&gt; 0.47391304347826085\n[ Info: \"Gaussian kernel predict accuracy\" =&gt; 0.6434782608695652\n[ Info: \"NeuralNetwork kernel predict accuracy\" =&gt; 0.6478260869565218\n[ Info: \"Linear kernel predict accuracy\" =&gt; 0.7782608695652173"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html",
    "href": "category/classification/1-catboost-claffification.html",
    "title": "1-catboost-classfication",
    "section": "",
    "text": "dataset\n\n\n\ndataset å‚è§ clustering-exercises dataset"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-package",
    "href": "category/classification/1-catboost-claffification.html#load-package",
    "title": "1-catboost-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport Plots:scatter!,contourf\nimport MLJ:predict,predict_mode,measures\nusing Plots, MLJ, CSV, DataFrames\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-data",
    "href": "category/classification/1-catboost-claffification.html#load-data",
    "title": "1-catboost-classfication",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n  df=load_csv(\"basic1\")\n  cat=df[:,:color]|&gt;levels|&gt;length # ç±»åˆ«\n  ytrain, Xtrain =  unpack(df, ==(:color), rng=123);\n  first(df,10)\n\n\n10Ã—3 DataFrame\n\n\n\nRow\nx\ny\ncolor\n\n\n\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n79.4083\n152.834\n0\n\n\n2\n98.0463\n186.911\n0\n\n\n3\n240.579\n48.4737\n1\n\n\n4\n109.687\n277.946\n0\n\n\n5\n249.626\n229.753\n1\n\n\n6\n100.785\n281.983\n0\n\n\n7\n235.33\n109.54\n1\n\n\n8\n262.352\n64.5746\n1\n\n\n9\n76.5589\n204.296\n0\n\n\n10\n245.558\n134.502\n1"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "href": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "title": "1-catboost-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    catboost = CatBoostClassifier(iterations=2,learning_rate=0.20)\n    mach = machine(catboost, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n    tx,ty,xtest=boundary_data(df)  # boudary data and xtest \n    ytest = predict_mode(mach, xtest)[:,1]|&gt;Array\n\n\n[ Info: Training machine(CatBoostClassifier(iterations = 2, â€¦), â€¦).\n\n\n40000-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n â‹®\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n3.2 plot results\n\n\nCode\ncontourf(tx,ty,ytest,levels=cat,color=cgrad(:redsblues),alpha=0.7)\np1=scatter!(df[:,:x],df[:,:y],group=df[:,:color],label=false,ms=3,alpha=0.3)"
  },
  {
    "objectID": "category/getting-started.html",
    "href": "category/getting-started.html",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#loading-package-and-data",
    "href": "category/getting-started.html#loading-package-and-data",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#build-decisiontree-model",
    "href": "category/getting-started.html#build-decisiontree-model",
    "title": "getting started with MLJ",
    "section": "2. build DecisionTree model",
    "text": "2. build DecisionTree model\n\n    y, X = unpack(iris, ==(:target); rng=123);\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n                 measures=[log_loss, accuracy],\n                 verbosity=0)\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJDecisionTreeInterface âœ”\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 3.12        â”‚ 1.48    â”‚ [2.88, 1.44, 5 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.913       â”‚ 0.041   â”‚ [0.92, 0.96, 0 â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html",
    "href": "category/regression/4-german-creditcard-logistics-reg.html",
    "title": "4-german-creditcard-logistics-reg",
    "section": "",
    "text": "ref :german-creditcard\nscitype è½¬æ¢ å‚è€ƒ:autotype(d, :few_to_finite)æ–¹æ³•"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "title": "4-german-creditcard-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,fit!,predict_mode,range\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "title": "4-german-creditcard-logistics-reg",
    "section": "2. data procsssing",
    "text": "2. data procsssing\n\n\nCode\nXtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "title": "4-german-creditcard-logistics-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\nmodel=LogisticClassifier()\nNuSVC = @load NuSVC pkg=LIBSVM\nmodel2 = NuSVC()\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel3 = KNNClassifier(weights = NearestNeighborModels.Inverse())\n\n\"å®šä¹‰ å‡ ä¸ª tune å‚æ•°çš„åŒºé—´ \"\nk1 =range(model, :gamma, lower=0.1, upper=1.2);\nk2 =range(model, :lambda, lower=0.1, upper=1.2);\nk3 =range(model, :penalty, values=([:l2, :l1,:en,:none]));\nk4 =range(model, :fit_intercept, values=([true, false]));\n\ntuning_logistic = TunedModel(model=model,\n                             resampling = CV(nfolds=4, rng=1234),\n                             tuning = Grid(resolution=8),\n                             range = [k1,k2],\n                             measure=accuracy)\nmach = machine(tuning_logistic, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦), â€¦).\n[ Info: Attempting to evaluate 64 models.\nEvaluating over 64 metamodels:   0%[&gt;                        ]  ETA: N/Aâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\nEvaluating over 64 metamodels:   2%[&gt;                        ]  ETA: 0:13:47Evaluating over 64 metamodels:   3%[&gt;                        ]  ETA: 0:07:03Evaluating over 64 metamodels:   5%[=&gt;                       ]  ETA: 0:04:37Evaluating over 64 metamodels:   6%[=&gt;                       ]  ETA: 0:03:25Evaluating over 64 metamodels:   8%[=&gt;                       ]  ETA: 0:02:41Evaluating over 64 metamodels:   9%[==&gt;                      ]  ETA: 0:02:12Evaluating over 64 metamodels:  11%[==&gt;                      ]  ETA: 0:01:51Evaluating over 64 metamodels:  12%[===&gt;                     ]  ETA: 0:01:36Evaluating over 64 metamodels:  14%[===&gt;                     ]  ETA: 0:01:24Evaluating over 64 metamodels:  16%[===&gt;                     ]  ETA: 0:01:14Evaluating over 64 metamodels:  17%[====&gt;                    ]  ETA: 0:01:06Evaluating over 64 metamodels:  19%[====&gt;                    ]  ETA: 0:00:59Evaluating over 64 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:54Evaluating over 64 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:49Evaluating over 64 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:45Evaluating over 64 metamodels:  25%[======&gt;                  ]  ETA: 0:00:41Evaluating over 64 metamodels:  27%[======&gt;                  ]  ETA: 0:00:38Evaluating over 64 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:35Evaluating over 64 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:33Evaluating over 64 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:30Evaluating over 64 metamodels:  33%[========&gt;                ]  ETA: 0:00:28Evaluating over 64 metamodels:  34%[========&gt;                ]  ETA: 0:00:26Evaluating over 64 metamodels:  36%[========&gt;                ]  ETA: 0:00:25Evaluating over 64 metamodels:  38%[=========&gt;               ]  ETA: 0:00:23Evaluating over 64 metamodels:  39%[=========&gt;               ]  ETA: 0:00:22Evaluating over 64 metamodels:  41%[==========&gt;              ]  ETA: 0:00:20Evaluating over 64 metamodels:  42%[==========&gt;              ]  ETA: 0:00:19Evaluating over 64 metamodels:  44%[==========&gt;              ]  ETA: 0:00:18Evaluating over 64 metamodels:  45%[===========&gt;             ]  ETA: 0:00:17Evaluating over 64 metamodels:  47%[===========&gt;             ]  ETA: 0:00:16Evaluating over 64 metamodels:  48%[============&gt;            ]  ETA: 0:00:15Evaluating over 64 metamodels:  50%[============&gt;            ]  ETA: 0:00:14Evaluating over 64 metamodels:  52%[============&gt;            ]  ETA: 0:00:13Evaluating over 64 metamodels:  53%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  55%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  56%[==============&gt;          ]  ETA: 0:00:11Evaluating over 64 metamodels:  58%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  59%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  61%[===============&gt;         ]  ETA: 0:00:09Evaluating over 64 metamodels:  62%[===============&gt;         ]  ETA: 0:00:08Evaluating over 64 metamodels:  64%[================&gt;        ]  ETA: 0:00:08Evaluating over 64 metamodels:  66%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  67%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  69%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  70%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  72%[=================&gt;       ]  ETA: 0:00:05Evaluating over 64 metamodels:  73%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  75%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  77%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  78%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  80%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  81%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  83%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:03Evaluating over 64 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  88%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  89%[======================&gt;  ]  ETA: 0:00:02Evaluating over 64 metamodels:  91%[======================&gt;  ]  ETA: 0:00:01Evaluating over 64 metamodels:  92%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  94%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  95%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels: 100%[=========================] Time: 0:00:14\nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n\n\nimport MLJLinearModels âœ”\nimport MLJLIBSVMInterface âœ”\nimport NearestNeighborModels âœ”\n\n\ntrained Machine; does not cache data\n  model: ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦)\n  args: \n    1:  Source @693 â Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}\n    2:  Source @676 â AbstractVector{OrderedFactor{2}}\n\n\n\n\n3.2 predict test results\n\n\nCode\nyhat=predict_mode(mach, Xtest)|&gt;Array\n@info \"german-creditcard è¿çº¦é¢„æµ‹å‡†ç¡®ç‡\"=&gt;accuracy(ytest,yhat)|&gt;d-&gt;round(d,digits=3)\n\n\n[ Info: \"german-creditcard è¿çº¦é¢„æµ‹å‡†ç¡®ç‡\" =&gt; 0.74"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html",
    "href": "category/regression/2-ecommerce-linear-reg.html",
    "title": "2-ecommerce-linear-reg",
    "section": "",
    "text": "é€šè¿‡ä¸Šç½‘æµè§ˆæ—¶é—´é¢„æµ‹å¹´èŠ±è´¹\n\n\ndataset: kaggle ecommerce dataset\nmodel\nusing MLJLinearModels.jl ğŸ”—"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "href": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "title": "2-ecommerce-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing GLMakie, MLJ,CSV,DataFrames,StatsBase"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "href": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "title": "2-ecommerce-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\nstr=\"Ecommerce-Customers\"   \ndf=CSV.File(\"./data/Ecommerce-Customers.csv\") |&gt; DataFrame |&gt; dropmissing;\nselect!(df,4:8)\nX=df[:,1:4]|&gt;Matrix|&gt;MLJ.table\ny=Vector(df[:,5])\nfirst(df,5)\n\n\n5Ã—5 DataFrame\n\n\n\nRow\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n34.4973\n12.6557\n39.5777\n4.08262\n587.951\n\n\n2\n31.9263\n11.1095\n37.269\n2.66403\n392.205\n\n\n3\n33.0009\n11.3303\n37.1106\n4.10454\n487.548\n\n\n4\n34.3056\n13.7175\n36.7213\n3.12018\n581.852\n\n\n5\n33.3307\n12.7952\n37.5367\n4.44631\n599.406"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "title": "2-ecommerce-linear-reg",
    "section": "3. plot corrleation of variables",
    "text": "3. plot corrleation of variables\n\n\nCode\naxs = []\nlabel=names(df)|&gt;Array\ncolors = [:orange, :lightgreen, :purple,:lightblue,:red,:green]\n\nfig = Figure(resolution=(1400, 1400))\nax=Axis(fig[1,1])\n\nfunction plot_diag(i)\n\n    ax = Axis(fig[i, i])\n    push!(axs, ax)\n    density!(ax, df[:, i]; color=(colors[i], 0.5),\n            strokewidth=1.25, strokecolor=colors[i])\nend\n\n\nfunction plot_cor(i, j)\n    ax = Axis(fig[i, j])\n    scatter!(ax, df[:, i], df[:, j]; color=colors[j])\nend\n\n\nfunction plot_pair()\n    [(i == j ? plot_diag(i) : plot_cor(i, j)) for i in 1:5, j in 1:5]\nend\n\nfunction add_xy_label()\n    for i in 1:5\n        Axis(fig[5, i], xlabel=label[i],)\n        Axis(fig[i, 1], ylabel=label[i],)\n    end\nend\n\nfunction main()\n\n    plot_pair()\n    add_xy_label()\n    return fig\nend\n\nmain()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "title": "2-ecommerce-linear-reg",
    "section": "4. plot pair variablesâ€™s cov and cor matrix",
    "text": "4. plot pair variablesâ€™s cov and cor matrix\n\n\nCode\ndf_cov = df|&gt;Matrix|&gt;cov.|&gt; d -&gt; round(d, digits=3)\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=3)\n\nfunction plot_cov_cor()\n    fig = Figure(resolution=(2200, 800))\n    ax1 = Axis(fig[1, 1]; xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cov matrix\",yreversed=true)\n    ax3 = Axis(fig[1, 3], xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cor matrix\",yreversed=true)\n\n    hm = heatmap!(ax1, df_cov)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cov[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    hm2 = heatmap!(ax3, df_cor)\n    Colorbar(fig[1, 4], hm2)\n    [text!(ax3, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    fig\nend\n\nplot_cov_cor()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "href": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "title": "2-ecommerce-linear-reg",
    "section": "5. MLJ workflow",
    "text": "5. MLJ workflow\n\n5.1 load model\n\n\nCode\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fitted_params(mach)\n\n\nimport MLJLinearModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\n(coefs = [:x1 =&gt; 25.734271084705085, :x2 =&gt; 38.709153810834366, :x3 =&gt; 0.43673883559434407, :x4 =&gt; 61.57732375487839],\n intercept = -1051.5942553006273,)\n\n\n\n\n5.2 predict\n\n\nCode\n  y_hat =predict(mach, X)\n  \"rmsd\"=&gt;rmsd(y,y_hat)\n\n\n\"rmsd\" =&gt; 9.923256785022247\n\n\n\n\n5.3 plot residuals\n\n\nCode\nresid=y_hat.=y\nstem(resid)"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html",
    "href": "category/regression/5-boston-housing-mixture-regression.html",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nåˆ©ç”¨ Boston houseing å±æ€§é¢„æµ‹æˆ¿ä»·, å˜é‡å¯èƒ½ä¼šå­˜åœ¨äº¤äº’ä½œç”¨\næ‰€ä»¥è€ƒè™‘ä½¿ç”¨æ··åˆæ¨¡å‹"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing MLJ"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nX, y= @load_boston;"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "href": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nmodelType= @load GaussianMixtureRegressor pkg = \"BetaML\"\ngmr= modelType()\n\n(fitResults, cache, report) = MLJ.fit(gmr, 1, X, y);\n\n\nimport BetaML âœ”\nIter. 1:    Var. of the post  21.74887448784977       Log-likelihood -21687.09917379566\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n3.2 results\n\n\nCode\ny_res= predict(gmr, fitResults, X)\nrmse(y_res,y)\n\n\n7.9566567641159605"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html",
    "href": "category/regression/1-salary-linear-reg.html",
    "title": "1-salary-linear-reg",
    "section": "",
    "text": "explore YearsExperience and Salary relationship\n\n\ndataset: kaggle salary dataset\næ•°æ®ç±»å‹éœ€è¦åšè½¬æ¢: to_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nusing MLJLinearModels.jl ğŸ”—"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#load-package",
    "href": "category/regression/1-salary-linear-reg.html#load-package",
    "title": "1-salary-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#process-data",
    "href": "category/regression/1-salary-linear-reg.html#process-data",
    "title": "1-salary-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\nload(csv)-&gt;dataframe ==&gt;sciencetype ==&gt;MLJ table\n\n\n\n\nCode\ndf=CSV.File(\"./data/salary_dataset.csv\") |&gt; DataFrame |&gt; dropmissing;\nfirst(df,5)\n\n\n5Ã—3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\nto_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nnew_df=to_ScienceType(df)\nfirst(new_df,5)\n\n\n5Ã—3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\n X=MLJ.table(reshape(new_df[:,2],30,1))\n y=Vector(new_df[:,3])\n show(y)\n\n\n[39344.0, 46206.0, 37732.0, 43526.0, 39892.0, 56643.0, 60151.0, 54446.0, 64446.0, 57190.0, 63219.0, 55795.0, 56958.0, 57082.0, 61112.0, 67939.0, 66030.0, 83089.0, 81364.0, 93941.0, 91739.0, 98274.0, 101303.0, 113813.0, 109432.0, 105583.0, 116970.0, 112636.0, 122392.0, 121873.0]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "href": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "title": "1-salary-linear-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 load model\n\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fp=MLJ.fitted_params(mach)  #å­¦ä¹ çš„æ¨¡å‹å‚æ•°\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\nimport MLJLinearModels âœ”\n\n\n(coefs = [:x1 =&gt; 9449.962321455077],\n intercept = 24848.203966523164,)\n\n\n\n\n3.2 build linear function\n\n\nCode\n    a=fp.coefs[1,1][2]\n    b=fp.intercept\n    line_func(t)=a*t+b\n\n\nline_func (generic function with 1 method)"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#plot-results",
    "href": "category/regression/1-salary-linear-reg.html#plot-results",
    "title": "1-salary-linear-reg",
    "section": "4. plot results",
    "text": "4. plot results\n\n\nCode\nxs=range(extrema(new_df[:,2])...,200)\nfig=Figure()\nax=Axis(fig[1,1];xlabel=\"YearsExperience\",ylabel=\"Salary\")\nlines!(ax,xs,line_func.(xs);label=\"fit-line\",linewidth=3)\nscatter!(ax,new_df[:,2],new_df[:,3];label=\"data\",marker_style...)\naxislegend(ax)\nfig"
  },
  {
    "objectID": "category/regression/7-test-latexify.html",
    "href": "category/regression/7-test-latexify.html",
    "title": "7-latexify-test",
    "section": "",
    "text": "Code\nusing Latexify,Markdown,Symbolics\n\n@variables x\nexpr=latexify(3x^3 + 2x - 5)\n\nMarkdown.parse(\"\"\"$(expr)\"\"\")\n\n\n\\[\\begin{equation} -5 + 2 x + 3 x^{3} \\end{equation}\\]"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html",
    "href": "category/regression/6-compare of BetalML method.html",
    "title": "6-compare of BetalML models",
    "section": "",
    "text": "ä½¿ç”¨ BetaMLjl åº“ on german-creditcard dataset\n\nref :german-creditcard\nç±»å‹è½¬æ¢:coerce(d,autotype(d, (:few_to_finite, :discrete_to_continuous)))\nBetaMLæ˜¯juliaä¸­å¦ä¸€ä¸ªå¤§å‹çš„æœºå™¨å­¦ä¹ åº“,å‚è€ƒæ–‡æ¡£:BetaML Doc"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-package",
    "href": "category/regression/6-compare of BetalML method.html#load-package",
    "title": "6-compare of BetalML models",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,predict_mode\nimport BetaML\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-data",
    "href": "category/regression/6-compare of BetalML method.html#load-data",
    "title": "6-compare of BetalML models",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n   Xtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#define-models",
    "href": "category/regression/6-compare of BetalML method.html#define-models",
    "title": "6-compare of BetalML models",
    "section": "3. define models",
    "text": "3. define models\n\n\nCode\nfunction define_models()\n\n        modelType1= @load NeuralNetworkClassifier pkg = \"BetaML\"\n\n        layers= [BetaML.DenseLayer(19,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,2,f=BetaML.relu),BetaML.VectorFunctionLayer(2,f=BetaML.softmax)];\n        nn_model= modelType1(layers=layers,opt_alg=BetaML.ADAM())\n\n        modelType2= @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\n        dt_model= modelType2()\n\n        modelType3= @load KernelPerceptron pkg = \"BetaML\"\n        kp_model= modelType3()\n\n\n        modelType4= @load LinearPerceptron pkg = \"BetaML\"\n        lp_model= modelType4()\n\n        modelType5= @load Pegasos pkg = \"BetaML\" verbosity=0\n        peg_model=modelType5()\n\n\n        modelType6= @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\n        rf_model=modelType6()\n\n        \n        cat_model=CatBoostClassifier(iterations=5)\n\n        models=[nn_model,dt_model,kp_model,lp_model,peg_model,rf_model,cat_model]\n        models_name=[\"nn\",\"dt\",\"kp\",\"lp\",\"peg\",\"rf\",\"cat\"]\n        return models,models_name\n    end\n\n    models,models_name=define_models()\n\n\nimport BetaML âœ”\nimport BetaML âœ”\nimport BetaML âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\n(Probabilistic[NeuralNetworkClassifier(layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([0.05820738438709466 0.15359974309514773 -0.2831571785409146 0.16676850370879254 0.04929165267757313 -0.38285625364631787 0.15943637609587863 -0.27963202262072295 0.4132068837304728 0.23994588656424914 0.3331154888122921 -0.14479877282526987 0.45349354202038267 -0.36374167213500097 0.38082093079144613 0.37683827444194523 -0.46572298136617724 -0.23696388317085407 -0.08100593088111102; 0.06901683915827367 0.10885118809668531 -0.4562971657881886 0.14338110290778844 -0.11795018859442519 0.28052259638617466 -0.39114674196409016 -0.04831482621792332 0.35273693227920394 0.10480838584360691 -0.3882485252335783 -0.4056213461754389 0.017690421658710653 -0.1955787553364121 0.34792091768031325 0.09347562692019068 0.009391921693455096 -0.212095472683146 -0.20048430029910436; -0.06941467294954756 0.16129590429684365 -0.3945640354462488 0.15750470788914933 0.3885122964456477 0.3332096512447256 0.09856154384626498 -0.05614878195860046 0.33348964914921825 0.3707940793066557 -0.4479100528048172 -0.46347117014718187 -0.007039753144278238 -0.011376074492971178 -0.07072402501612518 0.08988149206751922 -0.19302224723166356 0.07734011936937807 -0.2438275089701145; 0.30256089311740614 -0.09932629957089412 -0.0077149806013037114 0.2654651769987186 0.3982010752255802 0.1289689169965758 0.297683799044506 -0.3235707608961984 -0.13947296518093505 -0.3866613481102852 0.4612340110485032 0.09589044406863684 -0.34681211749141494 0.3912379724306973 -0.274696646744903 -0.396760579894517 -0.20437013714880814 0.3495768800225088 0.28755435414271086; 0.0919437046719424 0.16931759505456506 -0.16060918296751214 0.08009622213393358 -0.3610249335742296 -0.3636441188511364 -0.0747093794790551 0.02281957204525198 0.271534490785404 -0.010953880667109894 -0.46554915248123163 -0.013605598378954087 -0.26005124669666824 -0.35861740319732727 0.30156562967838657 -0.14506943028251668 0.37773622090065445 0.0043235812838134935 -0.28832307709763183; -0.3806236147103368 -0.41421792839709537 -0.05618041769254023 0.16700437692063458 0.27102999676282086 0.35732365366629854 -0.05554105450928831 -0.2940072650901307 -0.34858976308809075 -0.08781933261412772 0.07928849524951381 -0.2921509924901116 0.18309806737871553 -0.11845842252775374 -0.06605444214841183 -0.0754080395983614 0.02466617817400185 -0.02664319143567151 0.15425330169287638; 0.13965560253588044 -0.09589353646704613 0.20626211811746914 -0.02921942808900324 0.3573230040979693 0.42935726597655327 -0.23408604902535424 -0.4215264566882938 0.4272593128320675 -0.16466109292776998 -0.33591805893963517 -0.036667714700584164 -0.34113360310055485 -0.3182231815230606 -0.11030028776196782 0.3256404048487927 0.14568923379312165 -0.019597433857755753 0.077261277931228; 0.38597940043614193 0.4319364470182649 -0.28449397939645227 0.07973192193347306 0.2694921479971733 -0.18858938084715898 -0.40054720769577173 -0.35243943303754405 0.2698207629784914 -0.04530609440807121 0.19307566106689628 -0.42992875049556945 -0.4287847726205011 -0.27969985088909566 -0.31641882142491573 -0.12743914848036442 0.31095121929552044 -0.456382107919568 -0.1648328690974179], [-0.2666745908870727, 0.13335549524629603, 0.4625666178783718, 0.3847289564398341, 0.09889588201385607, 0.2458351640238045, 0.2109170440803862, 0.27406286915212036], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([0.005075366742773646 -0.48097438646814705 0.24202419659313423 0.6039221547858566 -0.11976624202124153 -0.26134915538668896 -0.1520800429322422 -0.6079732437148756; -0.15902388506436266 0.4101717997035834 -0.5271613841794951 0.2690671662689458 -0.4230502037386945 0.11813231578901096 0.09325917845478093 0.4018585055010937; -0.2865075182395346 0.43471923470729346 0.2682249356075499 -0.5804239225619936 -0.017040839642027406 -0.3158258548173093 -0.477498223140287 0.13153104269917792; -0.5603066570243207 0.5952577897306224 0.4279333630418678 -0.35588446441781557 0.01209911108282935 -0.29111475402071807 -0.0174045265860725 0.04347310148045058; 0.5361749410563643 -0.3710729959358052 0.3529941097864132 -0.6063062419946463 -0.34004410443094263 0.4490957104375721 0.5123489900427278 0.4783393295378502; -0.11215598338594046 -0.28622835800662383 -0.5851596590693836 0.3360290209415717 -0.4882731373846543 0.3718133861167885 0.40579943105041694 -0.2718409279216593; -0.49954399310383746 0.3947207073626293 -0.5158613400091111 0.49616248887375614 0.20709696475507455 0.021484345248235504 -0.06475831562880818 -0.30896275033352183; -0.05886588471216203 0.2781006271207779 -0.3955067757966984 -0.49202779529733237 -0.49270737466597236 -0.5144344557540311 -0.5739435312843488 -0.005598791115059343], [0.12486624424729331, 0.017621262906071578, -0.5119634127517022, -0.43616584920553747, -0.5048865910191135, 0.020664280050312178, 0.5074801901222517, 0.36057442264095185], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.3362431477665603 0.5672463100472439 0.6291035495812666 0.3352432455473987 0.113728772083315 0.3165847703233825 0.403355366417247 -0.4407581299244073; -0.0742656560025654 -0.5025245602904989 0.6085454291677123 0.30955389410722545 0.40102639527542594 -0.3606842071176862 0.5068886580090403 -0.6273414555421792], [0.0529497761563511, -0.5677199029810545], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 2, 2, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], â€¦), DecisionTreeClassifier(max_depth = 0, â€¦), KernelPerceptron(kernel = radial_kernel, â€¦), LinearPerceptron(initial_coefficients = nothing, â€¦), Pegasos(initial_coefficients = nothing, â€¦), RandomForestClassifier(n_trees = 30, â€¦), CatBoostClassifier(iterations = 5, â€¦)], [\"nn\", \"dt\", \"kp\", \"lp\", \"peg\", \"rf\", \"cat\"])"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#train-model",
    "href": "category/regression/6-compare of BetalML method.html#train-model",
    "title": "6-compare of BetalML models",
    "section": "4. train model",
    "text": "4. train model\n\n\nCode\nfunction train_model()\n    for (idx,model) in enumerate(models[1:6])\n        local (fitResults, cache, report) = MLJ.fit(model, 0, Xtrain,ytrain);\n        local est_classes= predict_mode(model, fitResults, Xtest)\n        local acc=accuracy(ytest,est_classes)|&gt;d-&gt;round(d, digits=3)\n        @info \"$(models_name[idx])===&gt;$(acc)\"\n    end\nend\n\ntrain_model()\n\n\n[ Info: nn===&gt;0.695\n[ Info: dt===&gt;0.705\n[ Info: kp===&gt;0.325\n[ Info: lp===&gt;0.695\n[ Info: peg===&gt;0.695\n[ Info: rf===&gt;0.735"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html",
    "href": "category/regression/8-iris-logistics-reg.html",
    "title": "8-iris-logistics-reg",
    "section": "",
    "text": "ç®€ä»‹\n\n\n\n\nref: probml page84 figure 2.13\ndataset:iris\nplots:ä½¿ç”¨ GLMakie:contourf æ–¹æ³•"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#load-package",
    "href": "category/regression/8-iris-logistics-reg.html#load-package",
    "title": "8-iris-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#process-data",
    "href": "category/regression/8-iris-logistics-reg.html#process-data",
    "title": "8-iris-logistics-reg",
    "section": "2 process data",
    "text": "2 process data\n\n2.1 import iris datset\n\n\nCode\niris = load_iris();\n\n#selectrows(iris, 1:3)  |&gt; pretty\n\niris = DataFrames.DataFrame(iris);\nfirst(iris,5)|&gt;display\ny, X = unpack(iris, ==(:target); rng=123);\n\nX=select!(X,3:4)\n\nbyCat = iris.target\ncateg = unique(byCat)\ncolors1 = [:orange,:lightgreen,:purple];\n\n\n5Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n2.2 make desc boundary data\n\nç”Ÿæˆå†³ç­–è¾¹ç•Œå®é™…æ˜¯åˆ©ç”¨è®­ç»ƒæ¨¡å‹å¯¹åŒºé—´å†…çš„æ¯ä¸ªç‚¹éƒ½åšå‡ºé¢„æµ‹,åˆ©ç”¨ä¸¤ä¸ªå±æ€§çš„æœ€å¤§å€¼å’Œæœ€å°å€¼ ç”Ÿæˆ grid æ•°æ®,è¿™æ˜¯ testæ•°æ®\n\n\n\nCode\n# grid data\n   n1 = n2 = 200\n   tx = LinRange(0, 8, 200)\n   ty = LinRange(-1, 4, 200)\n   X_test = mapreduce(collect, hcat, Iterators.product(tx, ty))\n   X_test = MLJ.table(X_test')\n\n\nTables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "href": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "title": "8-iris-logistics-reg",
    "section": "3. Logisitcs model",
    "text": "3. Logisitcs model\n\n3.1 training model\n\n\nCode\n     LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n      \n     model = machine(LogisticClassifier(), X,y )\n     fit!(model)\n\n\nimport MLJLinearModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LogisticClassifier(lambda = 2.220446049250313e-16, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, NamedTuple{(), Tuple{}}}\nâ”‚   optim_options: Optim.Options{Float64, Nothing}\nâ””   lbfgs_options: NamedTuple{(), Tuple{}} NamedTuple()\n\n\ntrained Machine; caches model-specific representations of data\n  model: LogisticClassifier(lambda = 2.220446049250313e-16, â€¦)\n  args: \n    1:  Source @794 â Table{AbstractVector{Continuous}}\n    2:  Source @579 â AbstractVector{Multiclass{3}}\n\n\n\n\n3.2 predict\n\n\nCode\nyÌ‚ = MLJ.predict(model, X_test)\n\nres=mode.(yÌ‚)|&gt;d-&gt;reshape(d,200,200)\nfunction trans(i)\n     \n    if i==\"setosa\"\n       res=1\n    elseif  i==\"versicolor\"\n       res=2\n       \n    else\n       res=3\n    end\nend\nypred=[trans(res[i,j]) for i in 1:200, j in 1:200]\n\n\n200Ã—200 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  â€¦  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n â‹®              â‹®              â‹®        â‹±        â‹®              â‹®           \n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  â€¦  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  â€¦  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#plot-res",
    "href": "category/regression/8-iris-logistics-reg.html#plot-res",
    "title": "8-iris-logistics-reg",
    "section": "4 plot res",
    "text": "4 plot res\n\n\nCode\n   function  add_legend(axs)\n      Legend(fig[1,2], axs,\"Label\";width=100,height=200)\n   end\n\n   function desision_boundary(ax)\n      axs=[]\n      for (k, c) in enumerate(categ)\n         indc = findall(x -&gt; x == c, byCat)\n         #@show indc\n         x=scatter!(iris[:,3][indc],iris[:,4][indc];color=colors1[k],markersize=14)\n         push!(axs,x)\n      end\n      return axs\n   end\n\n   fig = Figure(resolution=(800,600))\n   ax=Axis(fig[1,1],xlabel=\"Petal length\",ylabel=\"Petal width\",title=L\"Iris Logistics classfication\")\n   contourf!(ax,tx, ty, ypred, levels=length(categ))\n   axs=desision_boundary(ax)\n   Legend(fig[1,2],[axs...],categ)\n   fig"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html",
    "href": "category/regression/3-cricket-chirp-rate.html",
    "title": "3-cricket-chirp-rate",
    "section": "",
    "text": "source1\nsource2\nsourc3\n\n\né›ªæ ‘èŸ‹èŸ€çš„é¸£å«å®é™…æ˜¯å¤§è…¿æ‘©æ“¦å‘å‡ºçš„å£°éŸ³, ç»è¿‡æ•°æ®æ”¶é›†,å‘ç°é¸£å«çš„é¢‘ç‡å’Œç¯å¢ƒæ¸©åº¦æ­£ç›¸å…³.\n\nç»è¿‡çº¿æ€§æ‹Ÿåˆå¾—åˆ°çš„å‡½æ•°ä¸º:C(t)=4.25t-157.8"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "href": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "title": "3-cricket-chirp-rate",
    "section": "1. load pacakge",
    "text": "1. load pacakge\n\n\nCode\nimport FileIO:load\nimport MLJ:fit!,match,predict,table,fitted_params\nusing GLMakie, CSV,DataFrames,MLJ,FileIO\nimg=load(\"./data/snowy-cricket.jpg\");"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#process-data",
    "href": "category/regression/3-cricket-chirp-rate.html#process-data",
    "title": "3-cricket-chirp-rate",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\ndf=CSV.File(\"./data/CricketChirps.csv\") |&gt; DataFrame |&gt; dropmissing;\nX=MLJ.table(reshape(df[:,1],7,1))\ny=Vector(df[:,2])\n\ntest_X=range(extrema(df[:,1])...,50)\ntest_X=MLJ.table(reshape(test_X,50,1))\ncols=names(df)\n\n\n2-element Vector{String}:\n \"Temperature\"\n \"Chirps\""
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "href": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "title": "3-cricket-chirp-rate",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n    mach = fit!(machine(LinearRegressor(), X, y))\n    report(mach)\n\n\nimport MLJLinearModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \nâ”Œ Warning: The number and/or types of data arguments do not match what the specified model\nâ”‚ supports. Suppress this type check by specifying `scitype_check_level=0`.\nâ”‚ \nâ”‚ Run `@doc MLJLinearModels.LinearRegressor` to learn more about your model's requirements.\nâ”‚ \nâ”‚ Commonly, but non exclusively, supervised models are constructed using the syntax\nâ”‚ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\nâ”‚ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\nâ”‚ sample or class weights.\nâ”‚ \nâ”‚ In general, data in `machine(model, data...)` is expected to satisfy\nâ”‚ \nâ”‚     scitype(data) &lt;: MLJ.fit_data_scitype(model)\nâ”‚ \nâ”‚ In the present case:\nâ”‚ \nâ”‚ scitype(data) = Tuple{Table{AbstractVector{Continuous}}, AbstractVector{Count}}\nâ”‚ \nâ”‚ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Continuous}}\nâ”” @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearRegressor(fit_intercept = true, â€¦), â€¦).\nâ”Œ Info: Solver: MLJLinearModels.Analytical\nâ”‚   iterative: Bool false\nâ””   max_inner: Int64 200\n\n\n\n\n3.2 plot fitting curve\n\n\nCode\nyhat=predict(mach,test_X).|&gt;(d-&gt;round(d,digits=2))\nfunction plot_fitting_curve(df,yhat)\n    X=df[:,1]\n    test_X=range(extrema(df[:,1])...,50)\n    cols=names(df)\n    fig=Figure()\n    ax=Axis(fig[1:3,1:3];xlabel=\"$(cols[1])\",ylabel=\"$(cols[2])\",title=\"cricket-chirp\")\n    ax2 = Axis(fig[2,4],title=\"snowy-tree-cricket\")\n    scatter!(ax, X,y,markersize=16,color=(:red,0.8))\n    lines!(ax, test_X,yhat,color=:blue)\n    image!(ax2,img)\n    hidespines!(ax2)\n    hidedecorations!(ax2)\n    fig\nend\nplot_fitting_curve(df,yhat)"
  },
  {
    "objectID": "category/materials.html",
    "href": "category/materials.html",
    "title": "dataset list",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html",
    "href": "category/classification/24-classfication-comparison.html",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#load-package",
    "href": "category/classification/24-classfication-comparison.html#load-package",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#make-data",
    "href": "category/classification/24-classfication-comparison.html#make-data",
    "title": "several classfication model comparison",
    "section": "2. make data",
    "text": "2. make data\n\n\nCode\n    function circle_data()\n    X, y = make_circles(400; noise=0.1, factor=0.3)\n    df = DataFrame(X)\n    df.y = y\n    return df\n    end\n    function moons_data()\n        X, y = make_moons(400; noise=0.1)\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    function blob_data()\n        X, y = make_blobs(400, 2; centers=2, cluster_std=[1.0, 2.0])\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    #cat=df1.y|&gt;levels|&gt;unique\n    colors=[:green, :purple]\n\n\n2-element Vector{Symbol}:\n :green\n :purple"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-function",
    "href": "category/classification/24-classfication-comparison.html#define-function",
    "title": "several classfication model comparison",
    "section": "3. define function",
    "text": "3. define function\n\n\nCode\nfunction plot_origin_data(df)\n    fig=Figure()\n    ax=Axis(fig[1,1])\n    local cat=df.y|&gt;levels|&gt;unique\n    \n    local colors=[:green, :purple]\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        #@show d\n    end\n    fig\nend\n\nnums=100\nfunction boundary_data(df,;n=nums)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = LinRange(xlow,xhigh,n1)\n    ty = LinRange(ylow,yhigh,n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    x_test=MLJ.table(x_test')\n    return tx,ty,x_test\nend\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n        tx,ty,xs,ys, xtest=boundary_data(df)\n        local ax=Axis(fig[row,i],title=\"$(names[i])\")\n\n        contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:phase)\n\n        for (i,c) in enumerate(cat)\n            d=df[y.==c,:]\n            scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        end\n        hidedecorations!(ax)\nend\n\n\nplot_desc_boudary (generic function with 1 method)"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "href": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "title": "several classfication model comparison",
    "section": "4. define machine learning models",
    "text": "4. define machine learning models\n\n\nCode\n    using CatBoost.MLJCatBoostInterface\n    SVC = @load SVC pkg=LIBSVM   \n    KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n    DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n    RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n    CatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n    BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n    Booster = @load AdaBoostStumpClassifier pkg=DecisionTree\n    \n    models=[KNNClassifier,DecisionTreeClassifier,RandomForestClassifier,CatBoostClassifier,BayesianLDA,SVC]\n    names=[\"KNN\",\"DecisionTree\",\"RandomForest\",\"CatBoost\",\"BayesianLDA\",\"SVC\"]\n   function _fit(df::DataFrame,m)\n    X,y=df[:,1:2],df[:,3]\n    _,_,xtest=boundary_data(df;n=nums)\n    local predict= m==MLJLIBSVMInterface.SVC  ? MLJ.predict : MLJ.predict_mode \n    model=m()\n   mach = machine(model, X, y)|&gt;fit!\n   yhat=predict(mach, xtest)\n   ytest=yhat|&gt;Array|&gt;d-&gt;reshape(d,nums,nums)\n   return  ytest\nend\n\n\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n    tx,ty,_=boundary_data(df)\n    local y=df.y\n    local ax=Axis(fig[row,i],title=\"$(names[i])\")\n    cat=y|&gt;levels|&gt;unique\n    contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:redsblues)\n\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n    end\n    hidedecorations!(ax)\n    \n\nend\n\nfunction plot_comparsion(testdata,df;row=1)\n    \n    for (i,data) in enumerate(testdata)\n        plot_desc_boudary(fig,data,i;df=df,row=row)\n    end\n    fig\nend\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJLIBSVMInterface âœ”\nimport NearestNeighborModels âœ”\nimport MLJDecisionTreeInterface âœ”\nimport MLJDecisionTreeInterface âœ”\nimport CatBoost âœ”\nimport MLJMultivariateStatsInterface âœ”\nimport MLJDecisionTreeInterface âœ”\n\n\nplot_comparsion (generic function with 1 method)\n\n\n\n\nCode\nfig=Figure(resolution=(2100,1000))\nfunction plot_comparsion(testdata,df,row=1)\n    \n    for i in eachindex(testdata)\n        plot_desc_boudary(fig,testdata[i],i;df=df,row=row)\n    end\n    fig\nend\n\n\n\ndf1=circle_data()\n\nytest1=[_fit(df1,m) for (i,m) in enumerate(models)]\n\ndf2=moons_data()\nytest2=[_fit(df2,m) for (i,m) in enumerate(models)]\n\ndf3=blob_data()\nytest3=[_fit(df3,m) for (i,m) in enumerate(models)]\n\ndfs=[df2,df1,df3]\nytests=[ytest2,ytest1,ytest3]\n\nfig=Figure(resolution=(2100,1000))\n\nfor (df, data,i)  in zip(dfs,ytests,[1,2,3])\n    plot_comparsion(data,df;row=i)\nend\n\nfig\n\n\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦).\n[ Info: Training machine(KNNClassifier(K = 5, â€¦), â€¦).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, â€¦), â€¦).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, â€¦), â€¦).\n[ Info: Training machine(BayesianLDA(method = gevd, â€¦), â€¦).\n[ Info: Training machine(SVC(kernel = RadialBasis, â€¦), â€¦)."
  },
  {
    "objectID": "category/schedule.html",
    "href": "category/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "category/dataset/index.html",
    "href": "category/dataset/index.html",
    "title": "dataset index list",
    "section": "",
    "text": "â€œintor of datasetâ€\n1"
  }
]