[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About MLJ",
    "section": "",
    "text": "MLJ(Machine Learning in Julia) 参见 Blaom et al. (2020), 是使用Julia语言的机器学习集成工具箱,类似Python 的 scikit-learn,其中包括 180多种原生Julia 语言模型, 同时也包括了大量的scikit-learn 模型"
  },
  {
    "objectID": "index.html#浏览mlj-model",
    "href": "index.html#浏览mlj-model",
    "title": "About MLJ",
    "section": "1. 浏览MLJ Model",
    "text": "1. 浏览MLJ Model\n由于导入的模型非常多, 所以dev版文档 提供了快速浏览模型内容,按照机器学习的类型进行划分"
  },
  {
    "objectID": "index.html#mlj-workflow",
    "href": "index.html#mlj-workflow",
    "title": "About MLJ",
    "section": "2. MLJ workflow",
    "text": "2. MLJ workflow\n\nimport data\nmodel search\nInstantiating a model\nEvaluating a model\nperformance evaluation\n\n基本上所有的 quarto note 都遵循这个流程"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html",
    "href": "category/classification/2-diabetes-svm-classficiation.html",
    "title": "2-svm-diabetes-classfication",
    "section": "",
    "text": "介绍\n\n\n\n\n参考博客文章:diagnose-diabetes-with-svm\nSVM(支持向量机)通过引入 kernelfunction,使得模型的分类灵活性大大增强,可以解决更多问题.在julia中可以通过在LIBSVM.jl 引入 kernel function 实现, 参见 文档: Support Vector Machine\nMLJ.jl 通过包装接口也提供相似功能\n响应变量需要转换类型 to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "href": "category/classification/2-diabetes-svm-classficiation.html#load-package",
    "title": "2-svm-diabetes-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ: fit!, predict\nusing CSV,DataFrames,Random\nusing MLJ\nusing Plots\nusing KernelFunctions"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "href": "category/classification/2-diabetes-svm-classficiation.html#process-data",
    "title": "2-svm-diabetes-classfication",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\n df=load_csv(\"diabetes\")\n to_ScienceType(d)=coerce(d,:Outcome=&gt; Multiclass)\n df=to_ScienceType(df)\n first(df,5)|&gt;display\n y, X =  unpack(df, ==(:Outcome), rng=123);\n (Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.7, multi=true,  rng=123)\ndisplay(schema(X))\n\n\n5×9 DataFrame\n\n\n\nRow\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nCat…\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n┌──────────────────────────┬────────────┬─────────┐\n│ names                    │ scitypes   │ types   │\n├──────────────────────────┼────────────┼─────────┤\n│ Pregnancies              │ Count      │ Int64   │\n│ Glucose                  │ Count      │ Int64   │\n│ BloodPressure            │ Count      │ Int64   │\n│ SkinThickness            │ Count      │ Int64   │\n│ Insulin                  │ Count      │ Int64   │\n│ BMI                      │ Continuous │ Float64 │\n│ DiabetesPedigreeFunction │ Continuous │ Float64 │\n│ Age                      │ Count      │ Int64   │\n└──────────────────────────┴────────────┴─────────┘"
  },
  {
    "objectID": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "href": "category/classification/2-diabetes-svm-classficiation.html#mlj-workflow",
    "title": "2-svm-diabetes-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 defin model\n\n\nCode\nSVC = @load SVC pkg=LIBSVM\n#define kernel function,evaulate  kernelfunctions methods\nkernels=[PolynomialKernel(; degree=2, c=1), \n         SqExponentialKernel(),\n         NeuralNetworkKernel(),\n         LinearKernel(;c=1.0)\n]\n\nsvc_mdls = [SVC(;kernel=k) for  k in kernels]\nsvcs = [machine(model, Xtrain, ytrain;scitype_check_level=0) for model in svc_mdls]\n[fit!(svc) for svc in svcs]\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), …), …).\n\nWARNING: reaching max number of iterations\n[ Info: Training machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), …), …).\n[ Info: Training machine(SVC(kernel = Neural Network Kernel, …), …).\n[ Info: Training machine(SVC(kernel = Linear Kernel (c = 1.0), …), …).\n\nWARNING: reaching max number of iterations\n\n\nimport MLJLIBSVMInterface ✔\n\n\n4-element Vector{Machine{MLJLIBSVMInterface.SVC, true}}:\n machine(SVC(kernel = Polynomial Kernel (c = 1, degree = 2), …), …)\n machine(SVC(kernel = Squared Exponential Kernel (metric = Distances.Euclidean(0.0)), …), …)\n machine(SVC(kernel = Neural Network Kernel, …), …)\n machine(SVC(kernel = Linear Kernel (c = 1.0), …), …)\n\n\n\n\n3.2 predict test\n\n\nCode\nfor (idx, str) in enumerate([\"Polynomial \",\"Gaussian\",\"NeuralNetwork\",\"Linear\"])\n    local yhat=predict(svcs[idx],Xtest)\n    local acc=accuracy(yhat,ytest) \n    @info \"$(str) kernel predict accuracy\"=&gt;acc   \nend\n\n\n[ Info: \"Polynomial  kernel predict accuracy\" =&gt; 0.47391304347826085\n[ Info: \"Gaussian kernel predict accuracy\" =&gt; 0.6434782608695652\n[ Info: \"NeuralNetwork kernel predict accuracy\" =&gt; 0.6478260869565218\n[ Info: \"Linear kernel predict accuracy\" =&gt; 0.7782608695652173"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html",
    "href": "category/classification/1-catboost-claffification.html",
    "title": "1-catboost-classfication",
    "section": "",
    "text": "dataset\n\n\n\ndataset 参见 clustering-exercises dataset"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-package",
    "href": "category/classification/1-catboost-claffification.html#load-package",
    "title": "1-catboost-classfication",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport Plots:scatter!,contourf\nimport MLJ:predict,predict_mode,measures\nusing Plots, MLJ, CSV, DataFrames\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#load-data",
    "href": "category/classification/1-catboost-claffification.html#load-data",
    "title": "1-catboost-classfication",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n  df=load_csv(\"basic1\")\n  cat=df[:,:color]|&gt;levels|&gt;length # 类别\n  ytrain, Xtrain =  unpack(df, ==(:color), rng=123);\n  first(df,10)\n\n\n10×3 DataFrame\n\n\n\nRow\nx\ny\ncolor\n\n\n\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n79.4083\n152.834\n0\n\n\n2\n98.0463\n186.911\n0\n\n\n3\n240.579\n48.4737\n1\n\n\n4\n109.687\n277.946\n0\n\n\n5\n249.626\n229.753\n1\n\n\n6\n100.785\n281.983\n0\n\n\n7\n235.33\n109.54\n1\n\n\n8\n262.352\n64.5746\n1\n\n\n9\n76.5589\n204.296\n0\n\n\n10\n245.558\n134.502\n1"
  },
  {
    "objectID": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "href": "category/classification/1-catboost-claffification.html#mlj-workflow",
    "title": "1-catboost-classfication",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    catboost = CatBoostClassifier(iterations=2,learning_rate=0.20)\n    mach = machine(catboost, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n    tx,ty,xtest=boundary_data(df)  # boudary data and xtest \n    ytest = predict_mode(mach, xtest)[:,1]|&gt;Array\n\n\n[ Info: Training machine(CatBoostClassifier(iterations = 2, …), …).\n\n\n40000-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n ⋮\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n3.2 plot results\n\n\nCode\ncontourf(tx,ty,ytest,levels=cat,color=cgrad(:redsblues),alpha=0.7)\np1=scatter!(df[:,:x],df[:,:y],group=df[:,:color],label=false,ms=3,alpha=0.3)"
  },
  {
    "objectID": "category/getting-started.html",
    "href": "category/getting-started.html",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#loading-package-and-data",
    "href": "category/getting-started.html#loading-package-and-data",
    "title": "getting started with MLJ",
    "section": "",
    "text": "import MLJ:evaluate\n    using MLJ,DataFrames\n    iris=load_iris()|&gt;DataFrame\n    display(first(iris,10))\n\n10×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa"
  },
  {
    "objectID": "category/getting-started.html#build-decisiontree-model",
    "href": "category/getting-started.html#build-decisiontree-model",
    "title": "getting started with MLJ",
    "section": "2. build DecisionTree model",
    "text": "2. build DecisionTree model\n\n    y, X = unpack(iris, ==(:target); rng=123);\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n                 measures=[log_loss, accuracy],\n                 verbosity=0)\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJDecisionTreeInterface ✔\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 3.12        │ 1.48    │ [2.88, 1.44, 5 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.913       │ 0.041   │ [0.92, 0.96, 0 ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html",
    "href": "category/regression/4-german-creditcard-logistics-reg.html",
    "title": "4-german-creditcard-logistics-reg",
    "section": "",
    "text": "简介\n\n\n\n\nref :german-creditcard\nscitype 转换 参考:autotype(d, :few_to_finite)方法"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#load-package",
    "title": "4-german-creditcard-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,fit!,predict_mode,range\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#data-procsssing",
    "title": "4-german-creditcard-logistics-reg",
    "section": "2. data procsssing",
    "text": "2. data procsssing\n\n\nCode\nXtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "href": "category/regression/4-german-creditcard-logistics-reg.html#mlj-workflow",
    "title": "4-german-creditcard-logistics-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\nmodel=LogisticClassifier()\nNuSVC = @load NuSVC pkg=LIBSVM\nmodel2 = NuSVC()\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel3 = KNNClassifier(weights = NearestNeighborModels.Inverse())\n\n\"定义 几个 tune 参数的区间 \"\nk1 =range(model, :gamma, lower=0.1, upper=1.2);\nk2 =range(model, :lambda, lower=0.1, upper=1.2);\nk3 =range(model, :penalty, values=([:l2, :l1,:en,:none]));\nk4 =range(model, :fit_intercept, values=([true, false]));\n\ntuning_logistic = TunedModel(model=model,\n                             resampling = CV(nfolds=4, rng=1234),\n                             tuning = Grid(resolution=8),\n                             range = [k1,k2],\n                             measure=accuracy)\nmach = machine(tuning_logistic, Xtrain, ytrain;scitype_check_level=0)|&gt;fit!\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, …), …), …).\n[ Info: Attempting to evaluate 64 models.\nEvaluating over 64 metamodels:   0%[&gt;                        ]  ETA: N/A┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\nEvaluating over 64 metamodels:   2%[&gt;                        ]  ETA: 0:13:55Evaluating over 64 metamodels:   3%[&gt;                        ]  ETA: 0:07:07Evaluating over 64 metamodels:   5%[=&gt;                       ]  ETA: 0:04:40Evaluating over 64 metamodels:   6%[=&gt;                       ]  ETA: 0:03:27Evaluating over 64 metamodels:   8%[=&gt;                       ]  ETA: 0:02:43Evaluating over 64 metamodels:   9%[==&gt;                      ]  ETA: 0:02:13Evaluating over 64 metamodels:  11%[==&gt;                      ]  ETA: 0:01:52Evaluating over 64 metamodels:  12%[===&gt;                     ]  ETA: 0:01:37Evaluating over 64 metamodels:  14%[===&gt;                     ]  ETA: 0:01:25Evaluating over 64 metamodels:  16%[===&gt;                     ]  ETA: 0:01:15Evaluating over 64 metamodels:  17%[====&gt;                    ]  ETA: 0:01:07Evaluating over 64 metamodels:  19%[====&gt;                    ]  ETA: 0:01:00Evaluating over 64 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:54Evaluating over 64 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:50Evaluating over 64 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:45Evaluating over 64 metamodels:  25%[======&gt;                  ]  ETA: 0:00:42Evaluating over 64 metamodels:  27%[======&gt;                  ]  ETA: 0:00:38Evaluating over 64 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:36Evaluating over 64 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:33Evaluating over 64 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:31Evaluating over 64 metamodels:  33%[========&gt;                ]  ETA: 0:00:29Evaluating over 64 metamodels:  34%[========&gt;                ]  ETA: 0:00:27Evaluating over 64 metamodels:  36%[========&gt;                ]  ETA: 0:00:25Evaluating over 64 metamodels:  38%[=========&gt;               ]  ETA: 0:00:23Evaluating over 64 metamodels:  39%[=========&gt;               ]  ETA: 0:00:22Evaluating over 64 metamodels:  41%[==========&gt;              ]  ETA: 0:00:20Evaluating over 64 metamodels:  42%[==========&gt;              ]  ETA: 0:00:19Evaluating over 64 metamodels:  44%[==========&gt;              ]  ETA: 0:00:18Evaluating over 64 metamodels:  45%[===========&gt;             ]  ETA: 0:00:17Evaluating over 64 metamodels:  47%[===========&gt;             ]  ETA: 0:00:16Evaluating over 64 metamodels:  48%[============&gt;            ]  ETA: 0:00:15Evaluating over 64 metamodels:  50%[============&gt;            ]  ETA: 0:00:14Evaluating over 64 metamodels:  52%[============&gt;            ]  ETA: 0:00:13Evaluating over 64 metamodels:  53%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  55%[=============&gt;           ]  ETA: 0:00:12Evaluating over 64 metamodels:  56%[==============&gt;          ]  ETA: 0:00:11Evaluating over 64 metamodels:  58%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  59%[==============&gt;          ]  ETA: 0:00:10Evaluating over 64 metamodels:  61%[===============&gt;         ]  ETA: 0:00:09Evaluating over 64 metamodels:  62%[===============&gt;         ]  ETA: 0:00:08Evaluating over 64 metamodels:  64%[================&gt;        ]  ETA: 0:00:08Evaluating over 64 metamodels:  66%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  67%[================&gt;        ]  ETA: 0:00:07Evaluating over 64 metamodels:  69%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  70%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  72%[=================&gt;       ]  ETA: 0:00:06Evaluating over 64 metamodels:  73%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  75%[==================&gt;      ]  ETA: 0:00:05Evaluating over 64 metamodels:  77%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  78%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  80%[===================&gt;     ]  ETA: 0:00:04Evaluating over 64 metamodels:  81%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  83%[====================&gt;    ]  ETA: 0:00:03Evaluating over 64 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:03Evaluating over 64 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  88%[=====================&gt;   ]  ETA: 0:00:02Evaluating over 64 metamodels:  89%[======================&gt;  ]  ETA: 0:00:02Evaluating over 64 metamodels:  91%[======================&gt;  ]  ETA: 0:00:01Evaluating over 64 metamodels:  92%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  94%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  95%[=======================&gt; ]  ETA: 0:00:01Evaluating over 64 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 64 metamodels: 100%[=========================] Time: 0:00:14\n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}, AbstractVector{OrderedFactor{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{&lt;:Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n\n\nimport MLJLinearModels ✔\nimport MLJLIBSVMInterface ✔\nimport NearestNeighborModels ✔\n\n\ntrained Machine; does not cache data\n  model: ProbabilisticTunedModel(model = LogisticClassifier(lambda = 2.220446049250313e-16, …), …)\n  args: \n    1:  Source @007 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{OrderedFactor{33}}, AbstractVector{OrderedFactor{10}}, AbstractVector{OrderedFactor{5}}, AbstractVector{OrderedFactor{53}}, AbstractVector{OrderedFactor{3}}, AbstractVector{OrderedFactor{4}}, AbstractVector{OrderedFactor{2}}}}\n    2:  Source @757 ⏎ AbstractVector{OrderedFactor{2}}\n\n\n\n\n3.2 predict test results\n\n\nCode\nyhat=predict_mode(mach, Xtest)|&gt;Array\n@info \"german-creditcard 违约预测准确率\"=&gt;accuracy(ytest,yhat)|&gt;d-&gt;round(d,digits=3)\n\n\n[ Info: \"german-creditcard 违约预测准确率\" =&gt; 0.74"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html",
    "href": "category/regression/8-iris-logistics-reg.html",
    "title": "8-iris-logistics-reg",
    "section": "",
    "text": "简介\n\n\n\n\nref: probml page84 figure 2.13\ndataset:iris\nplots:使用 GLMakie:contourf 方法"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#load-package",
    "href": "category/regression/8-iris-logistics-reg.html#load-package",
    "title": "8-iris-logistics-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#process-data",
    "href": "category/regression/8-iris-logistics-reg.html#process-data",
    "title": "8-iris-logistics-reg",
    "section": "2 process data",
    "text": "2 process data\n\n2.1 import iris datset\n\n\nCode\niris = load_iris();\n\n#selectrows(iris, 1:3)  |&gt; pretty\n\niris = DataFrames.DataFrame(iris);\nfirst(iris,5)|&gt;display\ny, X = unpack(iris, ==(:target); rng=123);\n\nX=select!(X,3:4)\n\nbyCat = iris.target\ncateg = unique(byCat)\ncolors1 = [:orange,:lightgreen,:purple];\n\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\ntarget\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n2.2 make desc boundary data\n\n生成决策边界实际是利用训练模型对区间内的每个点都做出预测,利用两个属性的最大值和最小值 生成 grid 数据,这是 test数据\n\n\n\nCode\n# grid data\n   n1 = n2 = 200\n   tx = LinRange(0, 8, 200)\n   ty = LinRange(-1, 4, 200)\n   X_test = mapreduce(collect, hcat, Iterators.product(tx, ty))\n   X_test = MLJ.table(X_test')\n\n\nTables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "href": "category/regression/8-iris-logistics-reg.html#logisitcs-model",
    "title": "8-iris-logistics-reg",
    "section": "3. Logisitcs model",
    "text": "3. Logisitcs model\n\n3.1 training model\n\n\nCode\n     LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n      \n     model = machine(LogisticClassifier(), X,y )\n     fit!(model)\n\n\nimport MLJLinearModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n┌ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, NamedTuple{(), Tuple{}}}\n│   optim_options: Optim.Options{Float64, Nothing}\n└   lbfgs_options: NamedTuple{(), Tuple{}} NamedTuple()\n\n\ntrained Machine; caches model-specific representations of data\n  model: LogisticClassifier(lambda = 2.220446049250313e-16, …)\n  args: \n    1:  Source @794 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @579 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n3.2 predict\n\n\nCode\nŷ = MLJ.predict(model, X_test)\n\nres=mode.(ŷ)|&gt;d-&gt;reshape(d,200,200)\nfunction trans(i)\n     \n    if i==\"setosa\"\n       res=1\n    elseif  i==\"versicolor\"\n       res=2\n       \n    else\n       res=3\n    end\nend\nypred=[trans(res[i,j]) for i in 1:200, j in 1:200]\n\n\n200×200 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  …  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2  …  3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3\n 2  2  2  2  2  2  2  2  2  2  2  2  2     3  3  3  3  3  3  3  3  3  3  3  3"
  },
  {
    "objectID": "category/regression/8-iris-logistics-reg.html#plot-res",
    "href": "category/regression/8-iris-logistics-reg.html#plot-res",
    "title": "8-iris-logistics-reg",
    "section": "4 plot res",
    "text": "4 plot res\n\n\nCode\n   function  add_legend(axs)\n      Legend(fig[1,2], axs,\"Label\";width=100,height=200)\n   end\n\n   function desision_boundary(ax)\n      axs=[]\n      for (k, c) in enumerate(categ)\n         indc = findall(x -&gt; x == c, byCat)\n         #@show indc\n         x=scatter!(iris[:,3][indc],iris[:,4][indc];color=colors1[k],markersize=14)\n         push!(axs,x)\n      end\n      return axs\n   end\n\n   fig = Figure(resolution=(800,600))\n   ax=Axis(fig[1,1],xlabel=\"Petal length\",ylabel=\"Petal width\",title=L\"Iris Logistics classfication\")\n   contourf!(ax,tx, ty, ypred, levels=length(categ))\n   axs=desision_boundary(ax)\n   Legend(fig[1,2],[axs...],categ)\n   fig"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html",
    "href": "category/regression/6-compare of BetalML method.html",
    "title": "6-compare of BetalML models",
    "section": "",
    "text": "简介\n\n\n\n使用 BetaMLjl 库 on german-creditcard dataset\n\nref :german-creditcard\n类型转换:coerce(d,autotype(d, (:few_to_finite, :discrete_to_continuous)))\nBetaML是julia中另一个大型的机器学习库,参考文档:BetaML Doc"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-package",
    "href": "category/regression/6-compare of BetalML method.html#load-package",
    "title": "6-compare of BetalML models",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\ninclude(\"../utils.jl\")\nimport MLJ:predict,predict_mode\nimport BetaML\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie\nusing CatBoost.MLJCatBoostInterface"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#load-data",
    "href": "category/regression/6-compare of BetalML method.html#load-data",
    "title": "6-compare of BetalML models",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\n   Xtrain, Xtest, ytrain, ytest,cat= load_german_creditcard();"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#define-models",
    "href": "category/regression/6-compare of BetalML method.html#define-models",
    "title": "6-compare of BetalML models",
    "section": "3. define models",
    "text": "3. define models\n\n\nCode\nfunction define_models()\n\n        modelType1= @load NeuralNetworkClassifier pkg = \"BetaML\"\n\n        layers= [BetaML.DenseLayer(19,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,2,f=BetaML.relu),BetaML.VectorFunctionLayer(2,f=BetaML.softmax)];\n        nn_model= modelType1(layers=layers,opt_alg=BetaML.ADAM())\n\n        modelType2= @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\n        dt_model= modelType2()\n\n        modelType3= @load KernelPerceptron pkg = \"BetaML\"\n        kp_model= modelType3()\n\n\n        modelType4= @load LinearPerceptron pkg = \"BetaML\"\n        lp_model= modelType4()\n\n        modelType5= @load Pegasos pkg = \"BetaML\" verbosity=0\n        peg_model=modelType5()\n\n\n        modelType6= @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\n        rf_model=modelType6()\n\n        \n        cat_model=CatBoostClassifier(iterations=5)\n\n        models=[nn_model,dt_model,kp_model,lp_model,peg_model,rf_model,cat_model]\n        models_name=[\"nn\",\"dt\",\"kp\",\"lp\",\"peg\",\"rf\",\"cat\"]\n        return models,models_name\n    end\n\n    models,models_name=define_models()\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport BetaML ✔\nimport BetaML ✔\nimport BetaML ✔\n\n\n(Probabilistic[NeuralNetworkClassifier(layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.36074636511061065 -0.39582846276007433 -0.19367369489258768 0.2680078206831555 0.23415797915589948 -0.35426717723464507 0.41873972936820797 0.03695708843884965 -0.1262241154740001 -0.4535341279511609 0.27083756550881694 -0.26285190070170183 -0.29113824789040094 0.3272732026253102 0.3120181348536681 0.03189504313141717 0.17859507646360423 0.24100348772661923 0.014525967665540818; 0.02585857761506677 -0.1786289777618676 0.27390992608661585 -0.00882009262935829 -0.05597939255352807 -0.45185469645137644 0.43448083345893546 -0.07933636613135336 -0.40808183079083227 -0.1512087063152568 -0.22346202440402502 0.24282973121527757 -0.4611082088959598 -0.22314239815908504 -0.2040062087828935 -0.23716874176485098 -0.4007849436405437 -0.029299013620295244 -0.2240163921875015; 0.3586660049153875 0.0990446439566613 -0.30304464083062305 -0.35711866133831294 -0.18581141830140957 0.10300116526719799 -0.3547092905827105 0.17358444569480663 0.46816569231449073 0.36725354636566204 -0.09950624589607898 -0.18348423586113166 -0.05591644911594651 0.21540326133872428 0.4397024519627983 -0.019152022051641848 0.015081037200303848 -0.09338789491179 0.2698572766549305; -0.14737750284903278 0.15390901525950657 0.11733088463889524 -0.18553420478116528 0.2773342867980156 -0.44187683747564216 -0.07128222618035901 -0.3975783832585831 -0.1797548030782582 0.45414987217334984 0.11774352278251005 0.4318637917472961 0.34114366748895236 0.20543094312431615 0.45879918119036506 0.19966031284458824 -0.4243977564030008 0.3528498049246546 -0.26449251745539387; -0.46603077560973166 0.363177679644008 0.15387610207235164 0.16878070443105436 0.22469025601685005 0.43163009427259763 0.3433671636225248 -0.04331120409999056 -0.3450286254110857 0.27013134708859105 -0.1246971621378602 0.18019183676577127 -0.22935636935801848 -0.38713683379338093 -0.36324994527472265 0.12175941762041481 0.37182252248090214 -0.2946228229043437 0.36728385779700773; -0.35965444222821236 -0.2513459765142385 -0.12760789988466997 -0.3651832232912821 0.2275393345538726 0.19344000211089812 -0.06541914124742532 0.45592078923421525 -0.3588317610866417 0.37053197119661924 -0.16897336019104503 0.006708286081282766 0.22294499850839905 0.3064633029426777 0.449684771230571 0.29878199021450175 -0.19723891228337898 0.03398494223628351 0.195930874969185; 0.16768892675786945 0.20178877728541117 -0.01920955698773008 -0.07346464258077734 -0.22641700608346715 -0.3972909747656714 -0.3926505747242142 -0.3211783657517294 -0.42299054770886885 0.4394603691078505 -0.020340987431912816 0.31152821754851673 0.275435690977708 -0.453777188734648 0.38653852340376965 0.2257564654780106 -0.39273527136599073 -0.3588904629697215 0.04673991700334451; -0.3922339208951156 0.4104731956148045 0.01849632586826505 -0.2311936494942672 0.05614872575245905 -0.10645646149582344 -0.4578158813458524 0.17582732885781888 0.007762622179870338 0.1398437234045315 -0.2026471964256374 0.3674505083332545 -0.26909992476264555 -0.20785693976235886 0.07734529876859592 0.08917618938717092 -0.22274107976966068 -0.3823399200961872 -0.45505603896994484], [-0.157465439622054, -0.21995520319116818, -0.3384789069755978, -0.15006788522938158, -0.08068739583553858, 0.17488394613124297, -0.04748199633913597, -0.3250052328690887], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.3852959591608028 -0.553059892716297 -0.015028124586988656 -0.011201024404014936 0.005100673463055649 -0.18746389838773353 -0.4465071177773833 -0.24705649867006285; 0.6026146990300689 -0.33870198929549894 0.2718518742522277 -0.3384705743525871 -0.06814271449805787 0.606002135337379 -0.10336455886078566 0.26238572386113823; 0.45018281052830456 0.18679055621241314 -0.37362480422350963 0.006078042783034587 -0.45064052026604706 -0.1425599823506864 -0.3585911486603981 -0.6063601118282695; -0.05738177001216915 -0.3564277181599287 -0.36781688568508414 -0.14252074674595333 0.2571075127666419 0.19183099681926707 -0.018662422915370347 -0.3510293358769623; -0.08061251239900558 0.1179048173797197 -0.25520661838834113 -0.02273079835591796 0.38222026437329126 0.40094107662179646 -0.5807559333068457 0.36662125095206255; 0.33297001370651136 0.33740265915614154 0.11892808620953843 -0.461833943645412 -0.44731582380216783 -0.1810395086037453 0.2447640455667498 0.18200413571170693; -0.48967054357747297 -0.4865518207276439 -0.24790297957249452 -0.45395785625464546 -0.21218635943332892 -0.11260342698590892 -0.38292860160231035 -0.5607840330610061; 0.49172091443161037 0.348025736136519 0.04600440542299833 -0.5245141418067015 -0.16209562928447985 -0.16401386904196824 -0.08199421251258743 0.043141435301436104], [0.5320229402035459, 0.08848618862963409, 0.49077131013907294, 0.40163369167512397, -0.3241193921715959, -0.4400009135782846, 0.07456092888227783, 0.36052644514730536], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.3367271772548713 0.6877326959270641 0.1575891073545942 -0.4441500273567661 0.20426279244526513 -0.08785955388176736 0.09549279041837222 -0.25434472335021696; -0.21366575215568595 0.01925661356946584 0.654401524198115 0.07698019071966455 0.006405545613967667 -0.4771138208850888 -0.7483462850040817 0.09549653596307062], [-0.604516114936473, -0.11748999487867229], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 2, 2, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], …), DecisionTreeClassifier(max_depth = 0, …), KernelPerceptron(kernel = radial_kernel, …), LinearPerceptron(initial_coefficients = nothing, …), Pegasos(initial_coefficients = nothing, …), RandomForestClassifier(n_trees = 30, …), CatBoostClassifier(iterations = 5, …)], [\"nn\", \"dt\", \"kp\", \"lp\", \"peg\", \"rf\", \"cat\"])"
  },
  {
    "objectID": "category/regression/6-compare of BetalML method.html#train-model",
    "href": "category/regression/6-compare of BetalML method.html#train-model",
    "title": "6-compare of BetalML models",
    "section": "4. train model",
    "text": "4. train model\n\n\nCode\nfunction train_model()\n    for (idx,model) in enumerate(models[1:6])\n        local (fitResults, cache, report) = MLJ.fit(model, 0, Xtrain,ytrain);\n        local est_classes= predict_mode(model, fitResults, Xtest)\n        local acc=accuracy(ytest,est_classes)|&gt;d-&gt;round(d, digits=3)\n        @info \"$(models_name[idx])===&gt;$(acc)\"\n    end\nend\n\ntrain_model()\n\n\n[ Info: nn===&gt;0.305\n[ Info: dt===&gt;0.705\n[ Info: kp===&gt;0.335\n[ Info: lp===&gt;0.45\n[ Info: peg===&gt;0.695\n[ Info: rf===&gt;0.735"
  },
  {
    "objectID": "category/regression/7-test-latexify.html",
    "href": "category/regression/7-test-latexify.html",
    "title": "7-latexify-test",
    "section": "",
    "text": "Code\nusing Latexify,Markdown,Symbolics\n\n@variables x\nexpr=latexify(3x^3 + 2x - 5)\n\nMarkdown.parse(\"\"\"$(expr)\"\"\")\n\n\n\\[\\begin{equation} -5 + 2 x + 3 x^{3} \\end{equation}\\]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html",
    "href": "category/regression/1-salary-linear-reg.html",
    "title": "1-salary-linear-reg",
    "section": "",
    "text": "简介\n\n\n\n\nexplore YearsExperience and Salary relationship\n\n\ndataset: kaggle salary dataset\n数据类型需要做转换: to_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nusing MLJLinearModels.jl 🔗"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#load-package",
    "href": "category/regression/1-salary-linear-reg.html#load-package",
    "title": "1-salary-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params\n    using GLMakie,MLJ,CSV,DataFrames"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#process-data",
    "href": "category/regression/1-salary-linear-reg.html#process-data",
    "title": "1-salary-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\nload(csv)-&gt;dataframe ==&gt;sciencetype ==&gt;MLJ table\n\n\n\n\nCode\ndf=CSV.File(\"./data/salary_dataset.csv\") |&gt; DataFrame |&gt; dropmissing;\nfirst(df,5)\n\n\n5×3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\nto_ScienceType(d)=coerce(d,:YearsExperience=&gt;Continuous,:Salary=&gt;Continuous)\nnew_df=to_ScienceType(df)\nfirst(new_df,5)\n\n\n5×3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\n\n\nCode\n X=MLJ.table(reshape(new_df[:,2],30,1))\n y=Vector(new_df[:,3])\n show(y)\n\n\n[39344.0, 46206.0, 37732.0, 43526.0, 39892.0, 56643.0, 60151.0, 54446.0, 64446.0, 57190.0, 63219.0, 55795.0, 56958.0, 57082.0, 61112.0, 67939.0, 66030.0, 83089.0, 81364.0, 93941.0, 91739.0, 98274.0, 101303.0, 113813.0, 109432.0, 105583.0, 116970.0, 112636.0, 122392.0, 121873.0]"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "href": "category/regression/1-salary-linear-reg.html#mlj-workflow",
    "title": "1-salary-linear-reg",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 load model\n\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fp=MLJ.fitted_params(mach)  #学习的模型参数\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n┌ Info: Solver: MLJLinearModels.Analytical\n│   iterative: Bool false\n└   max_inner: Int64 200\n\n\nimport MLJLinearModels ✔\n\n\n(coefs = [:x1 =&gt; 9449.962321455077],\n intercept = 24848.203966523164,)\n\n\n\n\n3.2 build linear function\n\n\nCode\n    a=fp.coefs[1,1][2]\n    b=fp.intercept\n    line_func(t)=a*t+b\n\n\nline_func (generic function with 1 method)"
  },
  {
    "objectID": "category/regression/1-salary-linear-reg.html#plot-results",
    "href": "category/regression/1-salary-linear-reg.html#plot-results",
    "title": "1-salary-linear-reg",
    "section": "4. plot results",
    "text": "4. plot results\n\n\nCode\nxs=range(extrema(new_df[:,2])...,200)\nfig=Figure()\nax=Axis(fig[1,1];xlabel=\"YearsExperience\",ylabel=\"Salary\")\nlines!(ax,xs,line_func.(xs);label=\"fit-line\",linewidth=3)\nscatter!(ax,new_df[:,2],new_df[:,3];label=\"data\",marker_style...)\naxislegend(ax)\nfig"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html",
    "href": "category/regression/5-boston-housing-mixture-regression.html",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "",
    "text": "简介\n\n\n\n\n利用 Boston houseing 属性预测房价, 变量可能会存在交互作用\n所以考虑使用混合模型"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-package",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing MLJ"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "href": "category/regression/5-boston-housing-mixture-regression.html#load-data",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nX, y= @load_boston;"
  },
  {
    "objectID": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "href": "category/regression/5-boston-housing-mixture-regression.html#mlj-workflow",
    "title": "5-bostonhousing-mixturemodel-regression",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 define model\n\n\nCode\nmodelType= @load GaussianMixtureRegressor pkg = \"BetaML\"\ngmr= modelType()\n\n(fitResults, cache, report) = MLJ.fit(gmr, 1, X, y);\n\n\nimport BetaML ✔\nIter. 1:    Var. of the post  21.74887448784977       Log-likelihood -21687.09917379566\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n3.2 results\n\n\nCode\ny_res= predict(gmr, fitResults, X)\nrmse(y_res,y)\n\n\n7.9566567641159605"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html",
    "href": "category/regression/2-ecommerce-linear-reg.html",
    "title": "2-ecommerce-linear-reg",
    "section": "",
    "text": "简介\n\n\n\n\n通过上网浏览时间预测年花费\n\n\ndataset: kaggle ecommerce dataset\nmodel\nusing MLJLinearModels.jl 🔗"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "href": "category/regression/2-ecommerce-linear-reg.html#load-package",
    "title": "2-ecommerce-linear-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:predict\nusing GLMakie, MLJ,CSV,DataFrames,StatsBase\n\n\nWARNING: using StatsBase.predict in module Main conflicts with an existing identifier."
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "href": "category/regression/2-ecommerce-linear-reg.html#process-data",
    "title": "2-ecommerce-linear-reg",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\nstr=\"Ecommerce-Customers\"   \ndf=CSV.File(\"./data/Ecommerce-Customers.csv\") |&gt; DataFrame |&gt; dropmissing;\nselect!(df,4:8)\nX=df[:,1:4]|&gt;Matrix|&gt;MLJ.table\ny=Vector(df[:,5])\nfirst(df,5)\n\n\n5×5 DataFrame\n\n\n\nRow\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n34.4973\n12.6557\n39.5777\n4.08262\n587.951\n\n\n2\n31.9263\n11.1095\n37.269\n2.66403\n392.205\n\n\n3\n33.0009\n11.3303\n37.1106\n4.10454\n487.548\n\n\n4\n34.3056\n13.7175\n36.7213\n3.12018\n581.852\n\n\n5\n33.3307\n12.7952\n37.5367\n4.44631\n599.406"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-corrleation-of-variables",
    "title": "2-ecommerce-linear-reg",
    "section": "3. plot corrleation of variables",
    "text": "3. plot corrleation of variables\n\n\nCode\naxs = []\nlabel=names(df)|&gt;Array\ncolors = [:orange, :lightgreen, :purple,:lightblue,:red,:green]\n\nfig = Figure(resolution=(1400, 1400))\nax=Axis(fig[1,1])\n\nfunction plot_diag(i)\n\n    ax = Axis(fig[i, i])\n    push!(axs, ax)\n    density!(ax, df[:, i]; color=(colors[i], 0.5),\n            strokewidth=1.25, strokecolor=colors[i])\nend\n\n\nfunction plot_cor(i, j)\n    ax = Axis(fig[i, j])\n    scatter!(ax, df[:, i], df[:, j]; color=colors[j])\nend\n\n\nfunction plot_pair()\n    [(i == j ? plot_diag(i) : plot_cor(i, j)) for i in 1:5, j in 1:5]\nend\n\nfunction add_xy_label()\n    for i in 1:5\n        Axis(fig[5, i], xlabel=label[i],)\n        Axis(fig[i, 1], ylabel=label[i],)\n    end\nend\n\nfunction main()\n\n    plot_pair()\n    add_xy_label()\n    return fig\nend\n\nmain()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "href": "category/regression/2-ecommerce-linear-reg.html#plot-pair-variabless-cov-and-cor-matrix",
    "title": "2-ecommerce-linear-reg",
    "section": "4. plot pair variables’s cov and cor matrix",
    "text": "4. plot pair variables’s cov and cor matrix\n\n\nCode\ndf_cov = df|&gt;Matrix|&gt;cov.|&gt; d -&gt; round(d, digits=3)\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=3)\n\nfunction plot_cov_cor()\n    fig = Figure(resolution=(2200, 800))\n    ax1 = Axis(fig[1, 1]; xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cov matrix\",yreversed=true)\n    ax3 = Axis(fig[1, 3], xticks=(1:5, label), yticks=(1:5, label), title=\"ecommerce cor matrix\",yreversed=true)\n\n    hm = heatmap!(ax1, df_cov)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cov[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    hm2 = heatmap!(ax3, df_cor)\n    Colorbar(fig[1, 4], hm2)\n    [text!(ax3, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:5, y in 1:5]\n\n    fig\nend\n\nplot_cov_cor()"
  },
  {
    "objectID": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "href": "category/regression/2-ecommerce-linear-reg.html#mlj-workflow",
    "title": "2-ecommerce-linear-reg",
    "section": "5. MLJ workflow",
    "text": "5. MLJ workflow\n\n5.1 load model\n\n\nCode\n  LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n  model=LinearRegressor()\n  mach = MLJ.fit!(machine(model,X,y))\n  fitted_params(mach)\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n┌ Info: Solver: MLJLinearModels.Analytical\n│   iterative: Bool false\n└   max_inner: Int64 200\n\n\nimport MLJLinearModels ✔\n\n\n(coefs = [:x1 =&gt; 25.734271084705085, :x2 =&gt; 38.709153810834366, :x3 =&gt; 0.43673883559434407, :x4 =&gt; 61.57732375487839],\n intercept = -1051.5942553006273,)\n\n\n\n\n5.2 predict\n\n\nCode\n  y_hat =predict(mach, X)\n  \"rmsd\"=&gt;rmsd(y,y_hat)\n\n\n\"rmsd\" =&gt; 9.923256785022247\n\n\n\n\n5.3 plot residuals\n\n\nCode\nresid=y_hat.=y\nstem(resid)"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html",
    "href": "category/regression/9-poisson-reg.html",
    "title": "9-poisson-reg",
    "section": "",
    "text": "简介\n\n\n\n泊松回归(Poisson Regression) 是一类特殊的回归模型,相应变量是计数数据(离散正整数) 响应变量的分布遵循泊松分布\ndataset data\n数据集有两个变量, 预测变量为数学成绩(Math Score),响应变量为奖学金等级(0-6)"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html#load-package",
    "href": "category/regression/9-poisson-reg.html#load-package",
    "title": "9-poisson-reg",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\n    include(\"../utils.jl\")\n    import MLJ:fit!,fitted_params,coerce\n    using GLMakie,MLJ,CSV,DataFrames,ScientificTypes"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html#load-data",
    "href": "category/regression/9-poisson-reg.html#load-data",
    "title": "9-poisson-reg",
    "section": "2. load data",
    "text": "2. load data\n\n\nCode\nto_ScienceType(d)=coerce(d,:Awards=&gt; Multiclass,:MathScore=&gt;Continuous)\ndf=CSV.File(\"./data/competition_awards_data.csv\") |&gt; DataFrame|&gt;dropmissing\n \n X=MLJ.table(reshape(df[:,2],200,1))\n y=Vector(df[:,1])\n (Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\n first(df,10)\n\n\n10×2 DataFrame\n\n\n\nRow\nAwards\nMathScore\n\n\n\nInt64\nInt64\n\n\n\n\n1\n0\n43\n\n\n2\n0\n38\n\n\n3\n0\n41\n\n\n4\n0\n33\n\n\n5\n0\n39\n\n\n6\n0\n43\n\n\n7\n0\n35\n\n\n8\n0\n41\n\n\n9\n0\n36\n\n\n10\n0\n38"
  },
  {
    "objectID": "category/regression/9-poisson-reg.html#mlj-workflow",
    "href": "category/regression/9-poisson-reg.html#mlj-workflow",
    "title": "9-poisson-reg",
    "section": "3. MLJ Workflow",
    "text": "3. MLJ Workflow\n\n3.1 load model\n\n\nCode\n    CountRegressor = @load LinearCountRegressor pkg=GLM\n    model = CountRegressor(fit_intercept=false)\n    mach = machine(model, Xtrain, ytrain)\n    fit!(mach)\n\n\nimport MLJGLMInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc GLM.LinearCountRegressor` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{AbstractVector{Count}}, AbstractVector{Count}}\n│ \n│ fit_data_scitype(model) = Union{Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Count}}, Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Count}, AbstractVector{&lt;:Union{Continuous, Count}}}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearCountRegressor(fit_intercept = false, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: LinearCountRegressor(fit_intercept = false, …)\n  args: \n    1:  Source @955 ⏎ Table{AbstractVector{Count}}\n    2:  Source @555 ⏎ AbstractVector{Count}\n\n\n\n\n3.2 predict model results\n\n\nCode\n yhat=predict_mode(mach, Xtest)|&gt;Array\n @info \"rms\"=&gt;rms(yhat,ytest)\n\n report(mach)\n\n\n[ Info: \"rms\" =&gt; 0.9486832980505138\n\n\n(stderror = [0.0013746169531615926],\n dof_residual = 160.0,\n vcov = [1.88957176791926e-6;;],\n deviance = 254.53389416397937,\n coef_table = ───────────────────────────────────────────────────────────────────\n         Coef.  Std. Error     z  Pr(&gt;|z|)    Lower 95%   Upper 95%\n───────────────────────────────────────────────────────────────────\nx1  0.00104856  0.00137462  0.76    0.4456  -0.00164564  0.00374276\n───────────────────────────────────────────────────────────────────,)"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html",
    "href": "category/regression/3-cricket-chirp-rate.html",
    "title": "3-cricket-chirp-rate",
    "section": "",
    "text": "简介\n\n\n\n\nsource1\nsource2\nsourc3\n\n\n雪树蟋蟀的鸣叫实际是大腿摩擦发出的声音, 经过数据收集,发现鸣叫的频率和环境温度正相关.\n\n经过线性拟合得到的函数为:C(t)=4.25t-157.8"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "href": "category/regression/3-cricket-chirp-rate.html#load-pacakge",
    "title": "3-cricket-chirp-rate",
    "section": "1. load pacakge",
    "text": "1. load pacakge\n\n\nCode\nimport FileIO:load\nimport MLJ:fit!,match,predict,table,fitted_params\nusing GLMakie, CSV,DataFrames,MLJ,FileIO\nimg=load(\"./data/snowy-cricket.jpg\");"
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#process-data",
    "href": "category/regression/3-cricket-chirp-rate.html#process-data",
    "title": "3-cricket-chirp-rate",
    "section": "2. process data",
    "text": "2. process data\n\n\nCode\ndf=CSV.File(\"./data/CricketChirps.csv\") |&gt; DataFrame |&gt; dropmissing;\nX=MLJ.table(reshape(df[:,1],7,1))\ny=Vector(df[:,2])\n\ntest_X=range(extrema(df[:,1])...,50)\ntest_X=MLJ.table(reshape(test_X,50,1))\ncols=names(df)\n\n\n2-element Vector{String}:\n \"Temperature\"\n \"Chirps\""
  },
  {
    "objectID": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "href": "category/regression/3-cricket-chirp-rate.html#mlj-workflow",
    "title": "3-cricket-chirp-rate",
    "section": "3. MLJ workflow",
    "text": "3. MLJ workflow\n\n3.1 fitting model\n\n\nCode\n    LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n    mach = fit!(machine(LinearRegressor(), X, y))\n    report(mach)\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LinearRegressor` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{AbstractVector{Continuous}}, AbstractVector{Count}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}, AbstractVector{Continuous}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n┌ Info: Solver: MLJLinearModels.Analytical\n│   iterative: Bool false\n└   max_inner: Int64 200\n\n\nimport MLJLinearModels ✔\n\n\n\n\n3.2 plot fitting curve\n\n\nCode\nyhat=predict(mach,test_X).|&gt;(d-&gt;round(d,digits=2))\nfunction plot_fitting_curve(df,yhat)\n    X=df[:,1]\n    test_X=range(extrema(df[:,1])...,50)\n    cols=names(df)\n    fig=Figure()\n    ax=Axis(fig[1:3,1:3];xlabel=\"$(cols[1])\",ylabel=\"$(cols[2])\",title=\"cricket-chirp\")\n    ax2 = Axis(fig[2,4],title=\"snowy-tree-cricket\")\n    scatter!(ax, X,y,markersize=16,color=(:red,0.8))\n    lines!(ax, test_X,yhat,color=:blue)\n    image!(ax2,img)\n    hidespines!(ax2)\n    hidedecorations!(ax2)\n    fig\nend\nplot_fitting_curve(df,yhat)"
  },
  {
    "objectID": "category/materials.html",
    "href": "category/materials.html",
    "title": "dataset list",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html",
    "href": "category/classification/24-classfication-comparison.html",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#load-package",
    "href": "category/classification/24-classfication-comparison.html#load-package",
    "title": "several classfication model comparison",
    "section": "",
    "text": "Code\n    import MLJ:predict,predict_mode\n    using  MLJ,GLMakie,DataFrames,Random\n    Random.seed!(1222)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#make-data",
    "href": "category/classification/24-classfication-comparison.html#make-data",
    "title": "several classfication model comparison",
    "section": "2. make data",
    "text": "2. make data\n\n\nCode\n    function circle_data()\n    X, y = make_circles(400; noise=0.1, factor=0.3)\n    df = DataFrame(X)\n    df.y = y\n    return df\n    end\n    function moons_data()\n        X, y = make_moons(400; noise=0.1)\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    function blob_data()\n        X, y = make_blobs(400, 2; centers=2, cluster_std=[1.0, 2.0])\n        df = DataFrame(X)\n        df.y = y\n        return df\n    end\n    #cat=df1.y|&gt;levels|&gt;unique\n    colors=[:green, :purple]\n\n\n2-element Vector{Symbol}:\n :green\n :purple"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-function",
    "href": "category/classification/24-classfication-comparison.html#define-function",
    "title": "several classfication model comparison",
    "section": "3. define function",
    "text": "3. define function\n\n\nCode\nfunction plot_origin_data(df)\n    fig=Figure()\n    ax=Axis(fig[1,1])\n    local cat=df.y|&gt;levels|&gt;unique\n    \n    local colors=[:green, :purple]\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        #@show d\n    end\n    fig\nend\n\nnums=100\nfunction boundary_data(df,;n=nums)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = LinRange(xlow,xhigh,n1)\n    ty = LinRange(ylow,yhigh,n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    x_test=MLJ.table(x_test')\n    return tx,ty,x_test\nend\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n        tx,ty,xs,ys, xtest=boundary_data(df)\n        local ax=Axis(fig[row,i],title=\"$(names[i])\")\n\n        contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:phase)\n\n        for (i,c) in enumerate(cat)\n            d=df[y.==c,:]\n            scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n        end\n        hidedecorations!(ax)\nend\n\n\nplot_desc_boudary (generic function with 1 method)"
  },
  {
    "objectID": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "href": "category/classification/24-classfication-comparison.html#define-machine-learning-models",
    "title": "several classfication model comparison",
    "section": "4. define machine learning models",
    "text": "4. define machine learning models\n\n\nCode\n    using CatBoost.MLJCatBoostInterface\n    SVC = @load SVC pkg=LIBSVM   \n    KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n    DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n    RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n    CatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n    BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n    Booster = @load AdaBoostStumpClassifier pkg=DecisionTree\n    \n    models=[KNNClassifier,DecisionTreeClassifier,RandomForestClassifier,CatBoostClassifier,BayesianLDA,SVC]\n    names=[\"KNN\",\"DecisionTree\",\"RandomForest\",\"CatBoost\",\"BayesianLDA\",\"SVC\"]\n   function _fit(df::DataFrame,m)\n    X,y=df[:,1:2],df[:,3]\n    _,_,xtest=boundary_data(df;n=nums)\n    local predict= m==MLJLIBSVMInterface.SVC  ? MLJ.predict : MLJ.predict_mode \n    model=m()\n   mach = machine(model, X, y)|&gt;fit!\n   yhat=predict(mach, xtest)\n   ytest=yhat|&gt;Array|&gt;d-&gt;reshape(d,nums,nums)\n   return  ytest\nend\n\n\n\nfunction plot_desc_boudary(fig,ytest,i;df=df1,row=1)\n    tx,ty,_=boundary_data(df)\n    local y=df.y\n    local ax=Axis(fig[row,i],title=\"$(names[i])\")\n    cat=y|&gt;levels|&gt;unique\n    contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:redsblues)\n\n    for (i,c) in enumerate(cat)\n        d=df[y.==c,:]\n        scatter!(ax, d[:,1],d[:,2],color=(colors[i],0.6))\n    end\n    hidedecorations!(ax)\n    \n\nend\n\nfunction plot_comparsion(testdata,df;row=1)\n    \n    for (i,data) in enumerate(testdata)\n        plot_desc_boudary(fig,data,i;df=df,row=row)\n    end\n    fig\nend\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nimport MLJLIBSVMInterface ✔\nimport NearestNeighborModels ✔\nimport MLJDecisionTreeInterface ✔\nimport MLJDecisionTreeInterface ✔\nimport CatBoost ✔\nimport MLJMultivariateStatsInterface ✔\nimport MLJDecisionTreeInterface ✔\n\n\nplot_comparsion (generic function with 1 method)\n\n\n\n\nCode\nfig=Figure(resolution=(2100,1000))\nfunction plot_comparsion(testdata,df,row=1)\n    \n    for i in eachindex(testdata)\n        plot_desc_boudary(fig,testdata[i],i;df=df,row=row)\n    end\n    fig\nend\n\n\n\ndf1=circle_data()\n\nytest1=[_fit(df1,m) for (i,m) in enumerate(models)]\n\ndf2=moons_data()\nytest2=[_fit(df2,m) for (i,m) in enumerate(models)]\n\ndf3=blob_data()\nytest3=[_fit(df3,m) for (i,m) in enumerate(models)]\n\ndfs=[df2,df1,df3]\nytests=[ytest2,ytest1,ytest3]\n\nfig=Figure(resolution=(2100,1000))\n\nfor (df, data,i)  in zip(dfs,ytests,[1,2,3])\n    plot_comparsion(data,df;row=i)\nend\n\nfig\n\n\n[ Info: Training machine(KNNClassifier(K = 5, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, …), …).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, …), …).\n[ Info: Training machine(BayesianLDA(method = gevd, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n[ Info: Training machine(KNNClassifier(K = 5, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, …), …).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, …), …).\n[ Info: Training machine(BayesianLDA(method = gevd, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n[ Info: Training machine(KNNClassifier(K = 5, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n[ Info: Training machine(RandomForestClassifier(max_depth = -1, …), …).\n[ Info: Training machine(CatBoostClassifier(iterations = 1000, …), …).\n[ Info: Training machine(BayesianLDA(method = gevd, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …)."
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html",
    "title": "1-nci60-pca-clustering-svm",
    "section": "",
    "text": "简介\n\n\n\n参见 [ISLR-nci60]:An Introduction to Statistical Learning.pdf page 18\n流程为:pca-&gt;clustering-&gt;svm 半监督学习方法,首先对数据降维, 然后聚类, 最后使用 SVM 进行分类学习"
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html#load-package",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html#load-package",
    "title": "1-nci60-pca-clustering-svm",
    "section": "1. load package",
    "text": "1. load package\n\n\nCode\nimport MLJ:transform,predict\nusing DataFrames,MLJ,CSV,MLJModelInterface,GLMakie,Random\nRandom.seed!(45454)\n\n\nTaskLocalRNG()"
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html#import-data",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html#import-data",
    "title": "1-nci60-pca-clustering-svm",
    "section": "2. import data",
    "text": "2. import data\n\n\nCode\n    df= CSV.File(\"./data/NCI60.csv\") |&gt; DataFrame |&gt; dropmissing\n    Xtr = df[:,2:end]\n    Xtr_labels = Vector(df[:,1])\n    # # split other half to testing set\n    Xte=df[1:3:end,2:end]\n    Xte_labels = Vector(df[1:3:end,1])\n    first(df,10)\n\n\n10×6831 DataFrame6731 columns omitted\n\n\n\nRow\nColumn1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n⋯\n\n\n\nString3\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n⋯\n\n\n\n\n1\nV1\n0.3\n1.18\n0.55\n1.14\n-0.265\n-0.07\n0.35\n-0.315\n-0.45\n-0.65498\n-0.65\n-0.94\n0.31\n0.0150098\n-0.08\n-2.37\n-0.54\n-0.615\n0.0\n-0.51999\n-0.37\n-0.29\n-0.17499\n0.07\n-0.04\n0.025\n-0.74\n-0.47999\n-0.45\n-0.93\n0.16\n-0.55\n-0.55001\n0.055\n-0.37\n-0.165\n0.21\n0.47\n0.0\n-2.60208e-18\n0.139981\n-0.215\n-0.065\n-0.225\n-0.35\n-1.335\n0.0\n0.2175\n0.25\n0.13\n-0.48\n-0.42\n-0.7\n-0.275\n-0.34499\n-0.16\n-0.35\n0.555\n0.29\n-0.27\n-0.339981\n0.305\n-0.005\n0.7\n0.45002\n0.21\n0.29\n0.0849902\n-0.45501\n0.12\n-0.66\n0.1\n0.1\n-0.099961\n-0.399981\n-0.195\n0.28\n2.36\n0.47\n0.18\n-0.64499\n1.3\n0.0\n-0.48\n0.595\n-0.0599805\n0.055\n0.0975\n0.4\n0.28\n0.76\n1.425\n-0.51\n0.94\n0.94\n0.68\n-0.21\n-1.19\n0.0\n⋯\n\n\n2\nV2\n0.679961\n1.28996\n0.169961\n0.379961\n0.464961\n0.579961\n0.699961\n0.724961\n-0.040039\n-0.285019\n-0.310039\n-0.720039\n-0.010039\n0.0\n-0.570039\n0.0\n-0.470039\n-0.355039\n0.00498051\n-0.480029\n-0.140039\n-0.090039\n0.00497074\n-0.220039\n-0.370039\n0.0\n-0.320039\n0.159971\n0.179961\n-0.320039\n-0.440039\n0.349961\n0.449951\n0.104961\n0.489961\n0.204961\n-0.050039\n-0.010039\n0.269961\n0.019961\n0.0499415\n-0.315039\n-0.325039\n-0.055039\n-0.280039\n-0.255039\n0.229961\n-0.342539\n-0.560039\n-0.900039\n-0.060039\n-0.200039\n-0.670039\n0.324961\n0.134971\n0.539961\n0.229961\n0.084961\n-0.080039\n0.949961\n0.93998\n0.194961\n1.90496\n0.499961\n0.349981\n0.899961\n1.21996\n0.0\n0.374951\n0.279961\n-3.89862e-5\n-0.090039\n-0.050039\n0.0\n0.90998\n0.274961\n-0.040039\n0.869961\n-0.100039\n1.40996\n1.00497\n0.779961\n-0.110039\n-0.350039\n-0.215039\n-0.0600195\n0.324961\n0.267461\n0.129961\n0.229961\n0.079961\n0.514961\n-0.420039\n-0.350039\n-0.790039\n-0.290039\n-0.010039\n-1.05004\n-2.04004\n⋯\n\n\n3\nV3\n0.94\n-0.04\n-0.17\n-0.04\n-0.605\n0.0\n0.09\n0.645\n0.43\n0.475019\n0.41\n0.13\n-0.35\n0.0\n0.0\n0.0\n-0.8\n0.0\n-0.00498051\n0.0\n-0.14\n0.05\n-0.0649903\n-0.06\n0.29\n0.715\n-0.07\n-0.0899903\n-0.31\n0.58\n-0.48\n0.23\n-0.0400098\n-0.935\n-0.75\n-0.385\n-0.34\n0.12\n-0.47\n0.17\n-0.86002\n-0.175\n-0.715\n-0.965\n-0.54\n-0.005\n-0.06\n-0.7225\n-0.92\n0.47\n0.7\n0.67\n-0.9\n-0.265\n-0.42499\n-0.24\n-0.03\n0.215\n0.29\n0.07\n0.12002\n0.515\n0.545\n-0.03\n0.19002\n-0.07\n0.4\n0.0\n-0.0950098\n0.27\n0.08\n0.0\n0.0\n-0.319961\n0.0900195\n0.105\n-0.28\n1.99\n0.0\n0.87\n0.0\n0.74\n0.0\n-1.2\n-0.335\n0.630019\n0.345\n0.6975\n0.27\n-0.02\n0.0\n-0.115\n-0.44\n0.54\n0.49\n0.64\n0.66\n0.0\n0.0\n⋯\n\n\n4\nV4\n0.28\n-0.31\n0.68\n-0.81\n0.625\n-1.38778e-17\n0.17\n0.245\n0.02\n0.0950195\n-0.01\n-0.12\n-0.21\n0.0\n0.61\n-1.02\n-0.47\n0.0\n-0.76498\n0.0\n-0.31\n-0.62\n-0.28499\n-0.54\n-0.52\n-0.135\n-0.89\n-0.26999\n-0.84\n-0.23\n0.32\n0.0\n0.10999\n0.455\n-0.34\n-0.895\n-1.08\n-0.43\n-0.03\n-0.13\n-0.540019\n-1.225\n-1.265\n-1.415\n-0.27\n-0.705\n-0.22\n-0.5025\n-0.04\n-0.15\n-0.16\n-0.29\n-0.18\n-0.665\n-0.77499\n0.21\n-0.77\n-0.605\n-0.19\n0.17\n-0.20998\n-0.615\n0.165\n-0.19\n0.0\n-0.06\n-0.01\n-0.50501\n-0.77501\n-0.29\n-0.71\n-0.45\n-0.61\n-0.809961\n-1.53998\n-1.035\n0.0\n3.6\n0.0\n0.85\n-0.19499\n-1.21\n1.02\n0.66\n0.775\n-0.279981\n-0.245\n-0.5025\n-0.8\n-0.75\n0.06\n-1.075\n0.54\n0.16\n0.0\n0.23\n-0.74\n0.0\n-2.5\n⋯\n\n\n5\nV5\n0.485\n-0.465\n0.395\n0.905\n0.2\n-0.005\n0.085\n0.11\n0.235\n1.49002\n0.685\n0.605\n0.355\n1.22001\n2.425\n0.0\n-0.315\n0.31\n-0.519981\n-0.0749902\n-0.865\n-0.455\n-0.49999\n-0.245\n-0.235\n-0.33\n0.0\n0.0150097\n-0.105\n-0.225\n-0.105\n-0.275\n-0.57501\n-0.45\n-0.465\n-0.39\n-0.995\n-0.355\n0.0\n-0.475\n-0.38502\n-0.77\n-0.96\n-0.97\n-0.895\n-0.63\n-0.535\n-0.8875\n-0.945\n-0.535\n0.005\n0.185\n-0.105\n-0.34\n-0.19999\n-0.665\n-0.225\n-0.41\n-0.215\n-0.175\n-0.784981\n-0.33\n-0.39\n-0.075\n-0.20498\n-0.325\n-0.485\n-0.35001\n-0.23001\n-0.155\n-0.575\n-0.615\n-0.695\n-0.634961\n-1.09498\n-1.0\n-0.335\n-1.385\n0.345\n0.815\n0.56001\n-0.155\n0.0\n-1.195\n-0.16\n-0.10498\n0.0\n-0.0075\n-0.945\n-0.965\n-0.225\n-0.46\n0.045\n0.795\n1.305\n0.705\n0.055\n0.715\n0.925\n⋯\n\n\n6\nV6\n0.31\n-0.03\n-0.1\n-0.46\n-0.205\n-0.54\n-0.64\n-0.585\n-0.77\n-0.24498\n-0.12\n0.0\n-0.69\n-0.73499\n-0.67\n-0.05\n0.09\n-0.805\n0.59502\n-0.42999\n-0.85\n-0.09\n-0.0149903\n0.0\n0.15\n0.805\n-0.7\n0.36001\n-0.16\n0.04\n-0.17\n0.09\n0.0599902\n-0.635\n-0.51\n-0.585\n0.72\n-0.17\n-0.55\n0.21\n-0.13002\n0.125\n-0.415\n-0.475\n-0.02\n-0.405\n-0.4\n0.0475\n0.22\n-1.4\n-0.65\n-0.65\n-0.15\n-0.475\n-0.51499\n0.0\n-0.35\n-0.755\n-0.54\n-1.16\n-1.70998\n-0.415\n-1.275\n-0.89\n-0.269981\n-1.18\n-1.89\n-1.93501\n-2.56501\n-2.5\n-1.01\n-0.75\n-0.85\n-0.589961\n0.0\n-0.085\n0.18\n1.37\n-0.15\n-2.84\n0.0\n-1.74\n-1.38\n-2.62\n-0.715\n-0.819981\n-0.545\n-0.5525\n-0.76\n-1.02\n-1.71\n-0.495\n-0.76\n-0.28\n-0.39\n-0.5\n-0.53\n-0.85\n-0.38\n⋯\n\n\n7\nV7\n-0.83\n0.0\n0.13\n-1.63\n0.075\n-0.36\n0.1\n0.155\n-0.29\n-0.0849805\n-0.01\n-0.24\n-0.44\n0.0\n0.0\n0.0\n-0.38\n0.495\n0.0\n0.43001\n0.31\n0.09\n-0.30499\n-0.24\n-0.58\n0.445\n-0.28\n0.37001\n0.0\n-0.27\n-0.12\n0.05\n0.00999023\n-0.195\n-0.86\n-0.405\n-0.6\n-0.27\n-0.54\n0.19\n-0.0100195\n-0.465\n-1.375\n-0.945\n-0.31\n-0.435\n-0.1\n-0.8725\n-0.74\n-2.66\n-2.02\n-1.52\n-1.33\n-0.555\n-0.84499\n-0.76\n-0.84\n-0.735\n-0.37\n-0.95\n-1.14998\n-1.185\n-1.485\n-0.58\n-0.0299805\n-1.26\n-1.25\n-0.0450098\n-1.08501\n-0.08\n-0.44\n-0.1\n-0.28\n-0.269961\n0.0900195\n-0.335\n-0.17\n-0.48\n0.05\n-1.12\n-0.58499\n-1.93\n0.0\n-1.29\n-0.875\n-1.62998\n-0.755\n-0.8325\n-0.73\n-0.47\n-0.69\n-1.145\n-1.6\n-0.58\n-1.39\n-0.62\n-0.15\n-0.79\n-0.83\n⋯\n\n\n8\nV8\n-0.19\n-0.87\n-0.45\n0.08\n0.005\n0.35\n-0.04\n-0.265\n-0.31\n-0.24498\n-0.91\n-1.23\n-0.52\n-0.26499\n-0.48\n-1.4\n-0.19\n0.125\n-0.51498\n-0.83999\n-0.72\n-0.32\n-0.59499\n-0.34\n-0.82\n0.0\n-1.18\n-0.32999\n-0.69\n-1.67\n1.3\n0.0\n-0.43001\n-0.685\n-1.55\n-0.955\n-0.74\n-0.2\n-0.58\n0.01\n-0.44002\n-0.055\n-0.565\n-0.795\n-0.8\n-0.345\n-0.37\n-0.2725\n-0.71\n-0.21\n-0.32\n-0.3\n-0.92\n-0.115\n0.0150098\n0.13\n0.13\n0.475\n-0.57\n-0.07\n-0.389981\n-0.185\n-1.055\n0.15\n-0.18998\n0.84\n0.84\n0.73499\n0.18499\n0.21\n-0.48\n-0.28\n0.12\n0.100039\n-0.389981\n-0.715\n0.14\n-0.82\n-0.47\n-1.64\n0.0\n0.0\n0.0\n-0.4\n-0.335\n0.11002\n-0.125\n0.0675\n-0.77\n-0.65\n-0.43\n1.215\n0.4\n-0.7\n-1.79\n-1.13\n-0.07\n0.53\n0.21\n⋯\n\n\n9\nV9\n0.46\n0.0\n1.15\n-1.4\n-0.005\n-0.7\n-0.92\n-0.515\n-0.28\n-0.11498\n-0.17\n-0.4\n-0.52\n-0.23499\n2.21\n-2.44\n0.26\n0.155\n0.135019\n-0.90999\n-0.13\n-0.4\n-0.29499\n0.16\n-0.23\n-0.315\n0.51\n0.20001\n-0.19\n0.09\n-0.85\n-0.78\n-1.05001\n0.005\n-1.06\n-0.425\n-0.76\n-0.27\n-0.96\n-1.26\n-1.05002\n-0.835\n-1.705\n-1.195\n-1.24\n-1.065\n0.04\n-0.0225\n-0.48\n-0.46\n0.22\n0.46\n0.56\n0.335\n0.35501\n0.39\n0.18\n-0.055\n-0.11\n0.46\n0.46002\n-0.005\n0.445\n-0.52\n-0.339981\n-0.01\n-0.55\n0.17499\n-0.41501\n-0.18\n0.05\n3.44234e-18\n0.0\n-0.249961\n0.360019\n0.195\n-0.3\n-0.77\n-0.37\n-0.34\n-0.45499\n-0.82\n0.0\n-0.7\n0.145\n0.0200195\n-0.155\n-0.1825\n0.74\n0.52\n0.05\n-0.105\n-0.54\n0.0\n0.53\n0.21\n0.0\n0.0\n1.36\n⋯\n\n\n10\nV10\n0.76\n1.49\n0.28\n0.1\n-0.525\n0.36\n0.6\n0.175\n0.58\n1.14502\n1.75\n1.71\n0.51\n0.66501\n-0.1\n-0.24\n-0.69\n-0.115\n0.235019\n0.89001\n0.3\n0.08\n0.12501\n0.32\n1.28\n0.895\n1.35\n0.42001\n0.37\n0.45\n-0.25\n-0.01\n-0.27001\n-0.455\n0.61\n0.615\n0.41\n0.2\n0.13\n0.7\n-0.12002\n-0.135\n-0.235\n0.475\n0.04\n-0.295\n1.05\n1.1275\n0.74\n-0.26\n-0.09\n0.15\n-0.09\n-0.265\n0.15501\n-0.23\n0.25\n0.095\n0.36\n-0.08\n-0.12998\n0.205\n0.985\n0.42\n0.32002\n0.25\n0.19\n0.21499\n0.0449902\n-0.2\n-0.31\n0.0\n-0.41\n-0.00996101\n0.19002\n0.095\n0.0\n1.94\n0.14\n-0.18\n-0.58499\n-0.42\n-0.38\n-1.52\n-0.745\n-0.55998\n-0.285\n-0.2025\n-0.05\n0.35\n0.03\n0.755\n0.8\n1.52\n2.11\n0.91\n-0.25\n-0.56\n-0.77\n⋯"
  },
  {
    "objectID": "category/dimension-reduction/1-nci60-pca-svm.html#mlj-workflow",
    "href": "category/dimension-reduction/1-nci60-pca-svm.html#mlj-workflow",
    "title": "1-nci60-pca-clustering-svm",
    "section": "2. MLJ WorkFlow",
    "text": "2. MLJ WorkFlow\n\n2.1 define models\n需要定义三个模型:\n\npca model\nclustering model\nclassficiation model\n\n\n\nCode\n PCA = @load PCA pkg=MultivariateStats\n KMeans = @load KMeans pkg=Clustering\n SVC = @load SVC pkg=LIBSVM\n\n model=PCA(maxoutdim=2) # pca model\n model2 = KMeans(k=3)   # clustering model\n model3 = SVC()        # svm dodel\n\n\nimport MLJMultivariateStatsInterface ✔\nimport MLJClusteringInterface ✔\nimport MLJLIBSVMInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nSVC(\n  kernel = LIBSVM.Kernel.RadialBasis, \n  gamma = 0.0, \n  cost = 1.0, \n  cachesize = 200.0, \n  degree = 3, \n  coef0 = 0.0, \n  tolerance = 0.001, \n  shrinking = true)\n\n\n\n\n2.2 PCA\n在 PCA 流程中要完成两步:\n\nPCA 模型训练(如果为了便于可视化, 维度为 2或者 3)\n将原始数据映射到降维的空间上\n\n\n\nCode\nmach = machine(model, Xtr) |&gt; fit!\nXproj =transform(mach, Xtr)\nfirst(Xproj,10)\n\n\n[ Info: Training machine(PCA(maxoutdim = 2, …), …).\n\n\n10×2 DataFrame\n\n\n\nRow\nx1\nx2\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n-19.7958\n0.115269\n\n\n2\n-21.5461\n-1.45735\n\n\n3\n-25.0566\n1.52609\n\n\n4\n-37.4095\n-11.3895\n\n\n5\n-50.2186\n-1.34617\n\n\n6\n-26.4352\n0.462982\n\n\n7\n-27.3393\n2.65031\n\n\n8\n-21.4897\n4.95414\n\n\n9\n-20.8525\n10.1631\n\n\n10\n-26.9529\n21.4733\n\n\n\n\n\n\n\n\n2.3 生成决策边界测试数据集\n\n\nCode\nfunction boundary_data(df,;n=200)\n    n1=n2=n\n    xlow,xhigh=extrema(df[:,:x1])\n    ylow,yhigh=extrema(df[:,:x2])\n    tx = range(xlow,xhigh; length=n1)\n    ty = range(ylow,yhigh; length=n2)\n    x_test = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    xtest=MLJ.table(x_test')\n    return tx,ty, xtest\nend\ntx,ty, xtest=boundary_data(Xproj)  #xtest  生成决策边界的数据\n\n\n(-50.21864152783379:0.5383986285981281:56.9226855631937, -51.362711708066826:0.3660101806028475:21.473314231899835, Tables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64)\n\n\n\n\n2.5 Clustering and SVM training\n\n\nCode\n mach2= machine(model2, Xproj) |&gt; fit!\n yhat = predict(mach2, Xproj)  # 聚类结果\n cat=yhat|&gt;Array|&gt;levels\n\n mach3 = machine(model3, Xproj, yhat)|&gt;fit!\n ypred=predict(mach3, xtest)|&gt;Array|&gt;d-&gt;reshape(d,200,200) #SVM 结果\n\n\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n\n\n200×200 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3  …  3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3  …  3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n\n\n\n\n2.6 plot results\n\n\nCode\n    function plot_model()\n        fig = Figure()\n        ax = Axis(fig[1, 1],title=\"NCI60 Machine Learning\",subtitle=\"pca-&gt;clustering-&gt;svm\")\n\n        colors = [:red, :orange, :blue]\n        contourf!(ax, tx,ty,ypred)\n        for (i, c) in enumerate(Array(yhat))\n            data = Xproj[i, :]\n            \n            scatter!(ax, data.x1, data.x2;marker=:circle,markersize=12,color=(colors[c],0.3),strokewidth=1,strokecolor=:black)\n            text!(ax,data.x1, data.x2;text=\"v$(i)\")\n        end\n\n        fig\n        #save(\"NCI60 Machine Learning:pca-&gt;clustering-&gt;svm-with-tag.png\",fig)\n    end\n\n    plot_model()"
  },
  {
    "objectID": "category/schedule.html",
    "href": "category/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "category/dataset/index.html",
    "href": "category/dataset/index.html",
    "title": "dataset index list",
    "section": "",
    "text": "“intor of dataset”\n1"
  }
]